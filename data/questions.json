{
  "foundations-01": {
    "title": "Foundations of AI - Probability Theory",
    "questions": [
      {
        "id": 1,
        "topic": "Sources of Uncertainty",
        "difficulty": "recall",
        "question": "In the context of Data Science, which of the following is identified as a primary reason why models may be uncertain even if the underlying data acquisition is perfect?",
        "options": [
          "The use of frequentist instead of Bayesian priors",
          "Insufficient data volume to represent complex patterns",
          "The deterministic nature of human labels",
          "The sample space being larger than the event space"
        ],
        "correct_index": 1,
        "rationale": "As per the slides, models can be uncertain due to insufficient data, even if the data itself is captured accurately.",
        "distractor_analysis": {
          "0": "Priors are a choice of framework, not an inherent source of model uncertainty caused by data limits.",
          "1": "Correct. Limited data prevents the model from converging on a stable representation.",
          "2": "The slides state labels are stochastic and often ambiguous, not deterministic.",
          "3": "The event space is typically the power set of the sample space, making it larger or equal, not the other way around."
        }
      },
      {
        "id": 2,
        "topic": "Probability Space",
        "difficulty": "analysis",
        "question": "If an event space E is defined as the power set of a finite sample space Ω (E = 2^Ω), what is the implication for the probability measure P?",
        "options": [
          "P must map every possible subset of Ω to a value between 0 and 1",
          "P only assigns probabilities to elementary events, not their combinations",
          "The sum of probabilities of all elements in E must equal 1",
          "P is restricted to mapping events to either 0 or 1 exclusively"
        ],
        "correct_index": 0,
        "rationale": "By definition, P: E -> [0, 1] maps every event in the event space (which are subsets of Ω) to a probability value.",
        "distractor_analysis": {
          "0": "Correct. The probability measure must be defined for all sets in the event space.",
          "1": "P maps events (sets), which include combinations of elementary outcomes.",
          "2": "The sum of probabilities of elementary outcomes in Ω equals 1, not the sum of the whole power set.",
          "3": "This describes a deterministic assignment, whereas probability allows any value in the interval [0,1]."
        }
      },
      {
        "id": 3,
        "topic": "Probability Properties",
        "difficulty": "application",
        "question": "In a scenario where events A and B overlap, why is P(A ∩ B) subtracted in the formula for P(A ∪ B)?",
        "options": [
          "To account for the conditional dependence of B on A",
          "Because the intersection represents the 'impossible event' in a disjoint set",
          "To avoid double-counting the outcomes that belong to both sets",
          "To normalize the total probability so it does not exceed the value of the median"
        ],
        "correct_index": 2,
        "rationale": "The slides explicitly state that the common space is counted twice when adding P(A) and P(B), so it must be removed once.",
        "distractor_analysis": {
          "0": "The formula P(A ∪ B) = P(A) + P(B) - P(A ∩ B) applies regardless of dependence.",
          "1": "If they overlap, the intersection is not an impossible event (probability zero).",
          "2": "Correct. It corrects for the overlap in the Venn diagram logic.",
          "3": "Normalization ensures it doesn't exceed 1, which is the value of P(Ω), not the median."
        }
      },
      {
        "id": 4,
        "topic": "Bayes' Theorem",
        "difficulty": "analysis",
        "question": "When applying Bayes' Theorem, what happens to the posterior probability if the 'Evidence' P(A) increases while the likelihood and prior remain constant?",
        "options": [
          "The posterior probability increases proportionally",
          "The posterior probability decreases",
          "The posterior probability remains unchanged as it only depends on the likelihood",
          "The relationship is non-monotonic and depends on the sample space size"
        ],
        "correct_index": 1,
        "rationale": "In P(B|A) = [P(A|B)P(B)] / P(A), the Evidence P(A) is in the denominator. Increasing the denominator decreases the quotient.",
        "distractor_analysis": {
          "0": "Since P(A) is the denominator, its increase leads to a decrease in the result.",
          "1": "Correct. Higher evidence for a general occurrence makes a specific cause B less likely for that specific event.",
          "2": "The posterior is defined by the full ratio including the Evidence.",
          "3": "The formula dictates a clear inverse relationship with the denominator."
        }
      },
      {
        "id": 5,
        "topic": "Conditional Independence",
        "difficulty": "analysis",
        "question": "Two events A and B are independent. If they are now conditioned on a third event C, what can be definitively said about their relationship?",
        "options": [
          "They are guaranteed to remain independent given C",
          "They will always become dependent given C",
          "Independence does not automatically imply conditional independence",
          "Conditional independence only exists if C is the empty set"
        ],
        "correct_index": 2,
        "rationale": "The slides pose the question 'If A and B are independent, are they also independent given C?'. In probability theory, independence and conditional independence are distinct properties.",
        "distractor_analysis": {
          "0": "This is a common misconception; conditioning can introduce dependencies (e.g., explaining-away effect).",
          "1": "They might remain independent, but it is not guaranteed.",
          "2": "Correct. One does not strictly follow from the other.",
          "3": "Empty sets have probability 0, which makes conditional probability undefined."
        }
      },
      {
        "id": 6,
        "topic": "Simpson's Paradox",
        "difficulty": "application",
        "question": "In the provided Simpson’s Paradox example regarding drug efficacy, why does Drug X appear better when looking at subgroups, but Drug Y appears better in the aggregate?",
        "options": [
          "Drug Y has a higher success rate for both men and women individually",
          "The male subgroup has a significantly higher recovery rate and a strong bias toward one drug",
          "The sample size for women was too small to be statistically significant",
          "Drug X was only tested on patients who were already recovering"
        ],
        "correct_index": 1,
        "rationale": "The slides note that being male is a strong cause for both drug usage (Drug Y) and recovery, skewing the weighted average.",
        "distractor_analysis": {
          "0": "In the table, Drug X has higher rates in both subgroups (0.10 vs 0.05 and 0.91 vs 0.50).",
          "1": "Correct. The imbalance in how drugs were assigned across genders with different baseline recovery rates causes the paradox.",
          "2": "The paradox persists regardless of the 'significance' if the weightings are imbalanced.",
          "3": "While true that recovery rates differed, the key is the 'strong cause' of the gender variable on both recovery and drug choice."
        }
      },
      {
        "id": 7,
        "topic": "Random Variables",
        "difficulty": "recall",
        "question": "What is the primary condition that distinguishes a discrete random variable from a continuous one?",
        "options": [
          "The range of the variable must be a subset of Integers",
          "The mapping space M must be countable",
          "The probability of any single outcome must be exactly 0.5",
          "The variable must be independent and identically distributed (i.i.d.)"
        ],
        "correct_index": 1,
        "rationale": "Slide 11 states: 'If M is countable, X is called discrete, otherwise continuous.'",
        "distractor_analysis": {
          "0": "Discrete variables can be non-integers as long as they are countable.",
          "1": "Correct. Countability is the defining mathematical property here.",
          "2": "This is a specific property of a fair Bernoulli trial, not a general definition.",
          "3": "i.i.d. describes a relationship between multiple variables, not the type of a single variable."
        }
      },
      {
        "id": 8,
        "topic": "Probability Density Functions (PDF)",
        "difficulty": "analysis",
        "question": "For a continuous random variable X, why is the value of the PDF f(x) not equal to the probability P(X = x)?",
        "options": [
          "Because the sum of probabilities for continuous variables must be infinite",
          "The probability of any specific point in a continuous distribution is zero",
          "The PDF represents the cumulative area from negative infinity to x",
          "Continuous variables do not have outcomes, only ranges"
        ],
        "correct_index": 1,
        "rationale": "In continuous distributions, probability is defined over intervals. The probability at a single point is the integral from x to x, which is zero.",
        "distractor_analysis": {
          "0": "The total probability (integral) must equal 1.",
          "1": "Correct. The PDF represents density; probability is the area under the curve.",
          "2": "That is the definition of the CDF (Cumulative Distribution Function).",
          "3": "They have outcomes, but the outcomes exist on a continuum where P(X=x) vanishes."
        }
      },
      {
        "id": 9,
        "topic": "Quantile Function",
        "difficulty": "recall",
        "question": "Which specific value of the quantile function F^-1(q) corresponds to the median of a distribution?",
        "options": [
          "q = 0",
          "q = 1",
          "q = 0.5",
          "q = Mean / Variance"
        ],
        "correct_index": 2,
        "rationale": "Slide 13 explicitly defines the median as F^-1(q) for q = 0.5.",
        "distractor_analysis": {
          "0": "This would be the minimum of the support (if it exists).",
          "1": "This would be the maximum of the support.",
          "2": "Correct. The 0.5 quantile is the definition of the median.",
          "3": "This is an arbitrary ratio with no standard relation to the median."
        }
      },
      {
        "id": 10,
        "topic": "Bernoulli Distribution",
        "difficulty": "application",
        "question": "A Bernoulli distribution is used to model a single coin toss. If we extend this to model the total number of heads in 'm' independent tosses, which distribution do we transition to?",
        "options": [
          "Geometric Distribution",
          "Poisson Distribution",
          "Binomial Distribution",
          "Multinomial Distribution"
        ],
        "correct_index": 2,
        "rationale": "The Binomial distribution models the number of successes (heads) in m independent Bernoulli trials.",
        "distractor_analysis": {
          "0": "Geometric models the number of trials until the first success occurs.",
          "1": "Poisson models the number of events in a fixed interval of time/space.",
          "2": "Correct. A Binomial is the sum of i.i.d. Bernoulli variables.",
          "3": "Multinomial is for outcomes with more than two categories (e.g., dice)."
        }
      },
      {
        "id": 11,
        "topic": "Geometric Distribution",
        "difficulty": "analysis",
        "question": "In a Geometric distribution with parameter 'p', what does a very small 'p' value imply about the expected number of trials 'k' until success?",
        "options": [
          "Success is likely to happen in the very first trial",
          "The number of trials 'k' is likely to be very large",
          "The probability distribution becomes a Uniform distribution",
          "The variance of the distribution will approach zero"
        ],
        "correct_index": 1,
        "rationale": "If the probability of success per trial is low (small p), you expect to wait longer (more trials k) for the first success.",
        "distractor_analysis": {
          "0": "Small p means success is rare, making early success unlikely.",
          "1": "Correct. Lower p shifts the probability mass toward larger values of k.",
          "2": "A Geometric distribution is always decaying; it never becomes Uniform.",
          "3": "Lower probability of success generally increases the spread/uncertainty of when it will occur."
        }
      },
      {
        "id": 12,
        "topic": "Poisson Process",
        "difficulty": "application",
        "question": "You are modeling the number of phone calls received per hour. If the expected number of calls (λ) increases, how does the shape of the Poisson distribution change?",
        "options": [
          "It becomes more skewed toward zero",
          "It stays centered at zero but becomes taller",
          "It becomes more symmetric and bell-shaped",
          "It transforms into a Bernoulli distribution"
        ],
        "correct_index": 2,
        "rationale": "Slide 17 shows that as λ increases (from 0.5 to 10), the distribution shifts right and becomes more symmetric (approaching a Gaussian shape).",
        "distractor_analysis": {
          "0": "Smaller λ values are more skewed toward zero.",
          "1": "The peak (mode) shifts with λ, it doesn't stay at zero.",
          "2": "Correct. This is a property of the Poisson distribution as λ grows.",
          "3": "A Poisson distribution is for counts (0 to infinity), while Bernoulli is only 0 or 1."
        }
      },
      {
        "id": 13,
        "topic": "Exponential Distribution",
        "difficulty": "analysis",
        "question": "The 'memoryless' property of the Exponential distribution implies that:",
        "options": [
          "The probability of failure increases the longer a device has been running",
          "The remaining waiting time is independent of how much time has already passed",
          "The distribution has no variance, making it easy to predict",
          "Past events determine the exact timing of the next event"
        ],
        "correct_index": 1,
        "rationale": "Slide 18 states P(X > a + b | X > a) = P(X > b), meaning the probability of waiting 'b' more units is the same regardless of the 'a' units already waited.",
        "distractor_analysis": {
          "0": "This would describe a distribution with 'wear-out', which the Exponential specifically lacks.",
          "1": "Correct. This is the definition of memorylessness.",
          "2": "Exponential distributions have high variance (equal to 1/λ^2).",
          "3": "This contradicts the 'independent' part of the Poisson/Exponential process."
        }
      },
      {
        "id": 14,
        "topic": "Pareto Distribution",
        "difficulty": "recall",
        "question": "The Pareto distribution is often used to model which type of phenomenon?",
        "options": [
          "Events that occur at a constant average rate over time",
          "Power-law distributions where a small percentage of causes lead to most effects",
          "Symmetric outcomes like height or weight in a population",
          "Binary outcomes where success and failure are equally likely"
        ],
        "correct_index": 1,
        "rationale": "Slide 19 links the Pareto distribution to the '80/20' principle and power-law distributions (wealth, populations, followers).",
        "distractor_analysis": {
          "0": "This describes a Poisson or Exponential process.",
          "1": "Correct. It is the mathematical model for heavy-tailed phenomena.",
          "2": "Symmetric outcomes are modeled by Gaussian distributions.",
          "3": "This is a Bernoulli trial with p = 0.5."
        }
      },
      {
        "id": 15,
        "topic": "Logistic Distribution",
        "difficulty": "application",
        "question": "In which field is the Logistic cumulative distribution function most commonly applied as an 'activation' or inference tool?",
        "options": [
          "Modeling radioactive decay",
          "Neural Networks and Classification",
          "Determining the median of a Pareto distribution",
          "Calculating the number of heads in 100 coin tosses"
        ],
        "correct_index": 1,
        "rationale": "Slide 20 lists 'Inference in neural networks' and 'Classification' as primary applications.",
        "distractor_analysis": {
          "0": "Radioactive decay is modeled by the Exponential distribution.",
          "1": "Correct. The sigmoid shape is fundamental to logistic regression and ML.",
          "2": "These are different distribution types with distinct functional forms.",
          "3": "This is modeled by the Binomial distribution."
        }
      },
      {
        "id": 16,
        "topic": "Normal (Gaussian) Distribution",
        "difficulty": "analysis",
        "question": "How does increasing the standard deviation (σ) affect the Probability Density Function (PDF) of a Normal distribution?",
        "options": [
          "The peak shifts to the right on the x-axis",
          "The peak becomes taller and narrower",
          "The curve becomes flatter and wider, but the area remains 1",
          "The distribution becomes skewed to the left"
        ],
        "correct_index": 2,
        "rationale": "Standard deviation measures spread. A higher σ spreads the probability mass over a wider range, lowering the peak to maintain a total area of 1.",
        "distractor_analysis": {
          "0": "The mean (μ) shifts the peak, not σ.",
          "1": "This happens when σ decreases.",
          "2": "Correct. This represents increased uncertainty or variance.",
          "3": "The Normal distribution is always perfectly symmetric around its mean."
        }
      },
      {
        "id": 17,
        "topic": "Multivariate Distributions",
        "difficulty": "recall",
        "question": "What is the result of marginalizing a Joint PDF of several random variables?",
        "options": [
          "A single probability value (scalar)",
          "A distribution that depends only on a subset of the original variables",
          "A conditional distribution given the excluded variables",
          "The covariance matrix of the variables"
        ],
        "correct_index": 1,
        "rationale": "Slide 22 shows that marginalization (summing or integrating out variables) results in a distribution for the remaining variables (e.g., f_Xi(xi)).",
        "distractor_analysis": {
          "0": "Marginalization results in a function (PDF), not a single number, unless all variables are integrated out.",
          "1": "Correct. It effectively 'ignores' the variation of the other variables.",
          "2": "Conditional distributions are found by dividing the joint by the marginal, not just by marginalizing.",
          "3": "The covariance matrix is a summary statistic, not the marginalized distribution itself."
        }
      },
      {
        "id": 18,
        "topic": "Multivariate Gaussian",
        "difficulty": "analysis",
        "question": "In a Multivariate Gaussian distribution, what do the off-diagonal elements of the Σ (Sigma) matrix represent?",
        "options": [
          "The means of each individual variable",
          "The variance of each individual variable",
          "The covariance between pairs of variables",
          "The weights of the Gaussian mixture"
        ],
        "correct_index": 2,
        "rationale": "Slide 23 defines Σ_ij as Cov(Xi, Xj). The diagonal contains variances (i=j), and off-diagonals contain covariances (i≠j).",
        "distractor_analysis": {
          "0": "Means are stored in the μ vector.",
          "1": "Variances are on the diagonal of Σ.",
          "2": "Correct. They indicate how much two variables change together.",
          "3": "Weights are used in Mixture Models (GMM), not a single Multivariate Gaussian."
        }
      },
      {
        "id": 19,
        "topic": "Multinomial Distribution",
        "difficulty": "application",
        "question": "Which scenario would be best modeled using a Multinomial distribution?",
        "options": [
          "Predicting the outcome of a single fair coin toss",
          "Counting the number of '6's rolled in 10 tosses of a standard die",
          "Counting how many times each face (1 through 6) appears after rolling a die 50 times",
          "Determining the time between arrivals of customers at a bank"
        ],
        "correct_index": 2,
        "rationale": "The Multinomial distribution is for n trials with more than two possible outcomes per trial (e.g., rolling n m-sided dice).",
        "distractor_analysis": {
          "0": "This is a Bernoulli distribution.",
          "1": "This is a Binomial distribution (success '6' vs failure 'not 6').",
          "2": "Correct. It handles the counts for all possible faces simultaneously.",
          "3": "This is an Exponential distribution."
        }
      },
      {
        "id": 20,
        "topic": "Expectation Properties",
        "difficulty": "analysis",
        "question": "If two random variables X and Y are uncorrelated, which property of Expectation is guaranteed to hold?",
        "options": [
          "E(X + Y) = E(X) + E(Y)",
          "E(XY) = E(X)E(Y)",
          "E(X / Y) = E(X) / E(Y)",
          "E(X^2) = E(X)^2"
        ],
        "correct_index": 1,
        "rationale": "Slide 24 notes that E(XY) = E(X)E(Y) for independent variables, and variables for which this holds are called uncorrelated.",
        "distractor_analysis": {
          "0": "This property (linearity) holds for all random variables, regardless of correlation.",
          "1": "Correct. This is the definition/implication of being uncorrelated.",
          "2": "Expectation does not distribute over division.",
          "3": "This only holds if the variance is zero (X is a constant)."
        }
      },
      {
        "id": 21,
        "topic": "Variance Properties",
        "difficulty": "application",
        "question": "What is the variance of the sum of two variables X and Y [Var(X + Y)] if they are perfectly positively correlated?",
        "options": [
          "Var(X) + Var(Y)",
          "Var(X) + Var(Y) + 2Cov(X, Y)",
          "Var(X) - Var(Y)",
          "0"
        ],
        "correct_index": 1,
        "rationale": "Slide 25 gives the general formula: Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y).",
        "distractor_analysis": {
          "0": "This only holds if they are uncorrelated (Cov = 0).",
          "1": "Correct. This is the general expansion including the interaction term.",
          "2": "This is not a standard variance expansion formula.",
          "3": "Variance is only zero for constant values, not for correlated variables."
        }
      },
      {
        "id": 22,
        "topic": "Frequentist View",
        "difficulty": "recall",
        "question": "What is the primary shortcoming of the Frequentist view of probability as identified in the slides?",
        "options": [
          "It relies too heavily on subjective prior beliefs",
          "It can only be applied to events that are frequently repeatable",
          "It fails when the number of trials approaches infinity",
          "It assumes that all outcomes are equally likely in every scenario"
        ],
        "correct_index": 1,
        "rationale": "Slide 26 states a shortcoming: 'Can be only applied to frequently repeatable events.'",
        "distractor_analysis": {
          "0": "This is a characteristic (often viewed as a drawback) of the Bayesian view.",
          "1": "Correct. It cannot handle 'one-off' events where frequency can't be measured.",
          "2": "Frequentist estimates actually become more meaningful as n -> infinity.",
          "3": "This describes the 'classical' definition, not the frequentist 'relative frequency' approach."
        }
      },
      {
        "id": 23,
        "topic": "Bayesian View",
        "difficulty": "analysis",
        "question": "In the Bayesian view, what happens to the influence of the 'Prior' as the number of new observations increases?",
        "options": [
          "The prior becomes more relevant as more data justifies it",
          "The prior remains equally influential regardless of data size",
          "The prior becomes less and less relevant",
          "The prior is discarded and replaced by the evidence"
        ],
        "correct_index": 2,
        "rationale": "Slide 27 states: 'With increasing number of observations, prior beliefs become less and less relevant (i.e., uncertainty is reduced).'",
        "distractor_analysis": {
          "0": "The likelihood begins to dominate the posterior calculation with more data.",
          "1": "The weight of evidence overcomes the initial belief.",
          "2": "Correct. Data 'washes out' the influence of the starting prior.",
          "3": "It is not discarded; its weight in the product P(likelihood)*P(prior) simply diminishes relative to the likelihood."
        }
      },
      {
        "id": 24,
        "topic": "Cox’s Theorem",
        "difficulty": "recall",
        "question": "According to Cox’s Theorem, if a belief system is 'consistent', what must be true?",
        "options": [
          "Beliefs must be updated using only MLE",
          "If a belief can be derived in many ways, all results must be equal",
          "Beliefs can only be represented by integers",
          "The belief must not change with new observations"
        ],
        "correct_index": 1,
        "rationale": "Slide 28 explicitly defines consistency as: 'If the belief... can be derived in many ways, all the results must be equal.'",
        "distractor_analysis": {
          "0": "Cox's theorem provides a foundation for probability theory, not specifically MLE.",
          "1": "Correct. Consistency implies paths to the same conclusion must yield the same value.",
          "2": "Cox's theorem states beliefs should be represented by real numbers (Numerical Comparability).",
          "3": "This would violate 'Common Sense', which says beliefs should change sensibly with observations."
        }
      },
      {
        "id": 25,
        "topic": "Bayesian Inference",
        "difficulty": "analysis",
        "question": "Which component of Bayesian Inference is typically ignored when finding the most likely parameters (Maximum A Posteriori) because it acts only as a normalization constant?",
        "options": [
          "Prior",
          "Likelihood",
          "Evidence",
          "Posterior"
        ],
        "correct_index": 2,
        "rationale": "Slide 29 shows the proportionality: Posterior ∝ Likelihood * Prior. The Evidence P(x) is the denominator that normalizes the distribution.",
        "distractor_analysis": {
          "0": "The prior is essential to the Bayesian calculation.",
          "1": "The likelihood represents the data's contribution and is essential.",
          "2": "Correct. Since P(x) does not depend on the parameters θ, it is ignored during maximization.",
          "3": "The posterior is the result we are trying to find/maximize."
        }
      },
      {
        "id": 26,
        "topic": "Conjugate Priors",
        "difficulty": "recall",
        "question": "What is the primary 'algebraic convenience' provided by using a conjugate prior?",
        "options": [
          "It ensures that the likelihood is always Gaussian",
          "The posterior distribution will be in the same family as the prior",
          "It eliminates the need for any likelihood function",
          "It guarantees that the MLE and Bayesian estimate will be identical"
        ],
        "correct_index": 1,
        "rationale": "Slide 30 defines a conjugate prior as one where the posterior is in the same pdf family as the prior.",
        "distractor_analysis": {
          "0": "Conjugacy exists for many families (Beta, Gamma, etc.), not just Gaussian.",
          "1": "Correct. This allows for easy iterative updates as more data arrives.",
          "2": "You still need a likelihood; conjugacy is a property of the prior with respect to a likelihood.",
          "3": "They are usually different because the prior shifts the Bayesian estimate."
        }
      },
      {
        "id": 27,
        "topic": "Maximum Likelihood Estimation (MLE)",
        "difficulty": "recall",
        "question": "What is the core objective of Maximum Likelihood Estimation?",
        "options": [
          "To find the parameters that make the observed data most probable",
          "To calculate the prior probability of the parameters",
          "To minimize the variance of the sample space",
          "To ensure the model is unbiased and perfectly accurate"
        ],
        "correct_index": 0,
        "rationale": "MLE seeks parameters θ that maximize L(θ) = P(Data | θ).",
        "distractor_analysis": {
          "0": "Correct. It picks the θ that 'explains' the data best.",
          "1": "MLE is a frequentist approach and typically does not use priors.",
          "2": "MLE maximizes a likelihood function; it doesn't inherently minimize sample variance.",
          "3": "MLE is not always unbiased (e.g., the MLE for variance is biased)."
        }
      },
      {
        "id": 28,
        "topic": "Log-Likelihood",
        "difficulty": "analysis",
        "question": "Why is it common practice to maximize the log-likelihood rather than the likelihood itself?",
        "options": [
          "Logarithms change the location of the maximum to a more stable point",
          "It turns products of probabilities into sums, simplifying differentiation",
          "The log-likelihood is always convex, whereas the likelihood is not",
          "Logarithms remove the influence of outliers in the data"
        ],
        "correct_index": 1,
        "rationale": "Likelihoods for i.i.d. data involve large products. Logarithms convert these to sums, which are mathematically easier to handle when taking derivatives.",
        "distractor_analysis": {
          "0": "The log function is monotonic; it preserves the location of the maximum.",
          "1": "Correct. This is a fundamental computational trick in statistics/ML.",
          "2": "Not necessarily. If the likelihood is non-convex, the log-likelihood is likely also non-convex.",
          "3": "Logarithms do not change the data; they just change the scale of the optimization function."
        }
      },
      {
        "id": 29,
        "topic": "MLE for Gaussian",
        "difficulty": "application",
        "question": "If you have a set of i.i.d. samples from a Gaussian distribution, what is the MLE for the mean (μ)?",
        "options": [
          "The median of the samples",
          "The arithmetic average of the samples",
          "The square root of the variance",
          "The value of the first observation x1"
        ],
        "correct_index": 1,
        "rationale": "Slide 33 derives the MLE for μ as (1/n) * Σ(xi), which is the sample mean (arithmetic average).",
        "distractor_analysis": {
          "0": "The median is a robust estimator but not the MLE for a standard Gaussian mean.",
          "1": "Correct. It is the most 'likely' center point given the data.",
          "2": "This is related to the standard deviation, not the mean.",
          "3": "The first observation ignores the rest of the evidence in the dataset."
        }
      },
      {
        "id": 30,
        "topic": "MLE Generalization",
        "difficulty": "analysis",
        "question": "When is MLE considered 'analytically intractable', necessitating iterative methods like Expectation-Maximization?",
        "options": [
          "When the sample size n is less than 30",
          "When the derivative of the log-likelihood cannot be solved for zero in closed form",
          "Whenever a prior distribution is included in the model",
          "If the data is perfectly clean and has no noise"
        ],
        "correct_index": 1,
        "rationale": "Slide 34 states: 'If ∂ log L / ∂ p is analytically intractable, use iterative numerical methods...'",
        "distractor_analysis": {
          "0": "Intractability refers to the mathematical form, not the sample size.",
          "1": "Correct. If you can't solve it with a formula, you must search for it numerically.",
          "2": "MLE doesn't include priors; that would lead to MAP (Maximum A Posteriori).",
          "3": "Clean data usually makes MLE easier, not harder."
        }
      },
      {
        "id": 31,
        "topic": "Gaussian Mixture Models (GMM)",
        "difficulty": "recall",
        "question": "What defines a Gaussian Mixture Model compared to a standard Gaussian distribution?",
        "options": [
          "It uses a single mean but multiple variance parameters",
          "It represents the data as being generated from a weighted sum of several Gaussians",
          "It is only used for discrete random variables",
          "It is a Bayesian method that requires a Beta prior"
        ],
        "correct_index": 1,
        "rationale": "Slide 35 describes a mixture of Gaussians with weights (pA, pB) and individual parameters (μ, σ).",
        "distractor_analysis": {
          "0": "Each component in a mixture has its own mean and its own variance.",
          "1": "Correct. It allows modeling complex, multi-modal distributions.",
          "2": "GMMs are continuous mixture models.",
          "3": "GMM parameters can be estimated using frequentist methods like MLE/EM."
        }
      },
      {
        "id": 32,
        "topic": "EM for GMM",
        "difficulty": "analysis",
        "question": "Why is standard MLE usually impossible for finding the global maximum of a Gaussian Mixture Model likelihood?",
        "options": [
          "The likelihood function is always linear",
          "The likelihood function is non-convex and may have multiple local maxima",
          "GMMs do not have a likelihood function",
          "The sample size is always too small for GMMs"
        ],
        "correct_index": 1,
        "rationale": "Slide 36 states: 'likelihood function for GMMs is highly non-convex with multiple local maxima.'",
        "distractor_analysis": {
          "0": "Mixture models involve sums inside logs (after expanding), making them highly non-linear.",
          "1": "Correct. This non-convexity is why we need iterative algorithms like EM.",
          "2": "They have a well-defined likelihood; it's just hard to optimize.",
          "3": "GMMs can be used with any sample size, though results improve with more data."
        }
      },
      {
        "id": 33,
        "topic": "Expectation Step (E-step)",
        "difficulty": "application",
        "question": "In the E-step of the EM algorithm for mixture models, what is being estimated for each data point xi?",
        "options": [
          "The final parameters μ and σ",
          "The total probability of the entire dataset",
          "The 'membership weight' (the probability xi belongs to a specific cluster)",
          "The derivative of the likelihood with respect to the weights"
        ],
        "correct_index": 2,
        "rationale": "Slide 37/38: The E-step computes 'expected membership values' P(A|xi) and P(B|xi).",
        "distractor_analysis": {
          "0": "Parameters are updated in the M-step, not the E-step.",
          "1": "The likelihood is evaluated to check for convergence, but not the goal of the E-step.",
          "2": "Correct. This step 'assigns' data points to distributions based on current guesses.",
          "3": "Derivatives are part of the optimization logic in the M-step."
        }
      },
      {
        "id": 34,
        "topic": "Maximization Step (M-step)",
        "difficulty": "application",
        "question": "During the M-step of the EM algorithm, how are the parameters (μ, σ, p) updated?",
        "options": [
          "They are randomly perturbed to avoid local maxima",
          "They are recalculated using the membership weights found in the E-step",
          "They are held constant while the data labels are changed",
          "They are set to the values of the first observation x1"
        ],
        "correct_index": 1,
        "rationale": "Slide 38 shows that new parameters are 'weighted' averages of the data points, using weights (wAi) calculated in the E-step.",
        "distractor_analysis": {
          "0": "EM is a deterministic iterative algorithm; it doesn't use random perturbation.",
          "1": "Correct. The E-step provides the 'soft' assignments, and the M-step optimizes based on them.",
          "2": "The parameters are the things being updated in this step.",
          "3": "This would be a very poor estimator that ignores most data."
        }
      },
      {
        "id": 35,
        "topic": "EM Convergence",
        "difficulty": "analysis",
        "question": "Which statement best describes the convergence behavior of the EM algorithm?",
        "options": [
          "It is guaranteed to find the global maximum in a single iteration",
          "It monotonically approaches a local maximum of the likelihood",
          "It oscillates indefinitely and never truly converges",
          "It converges only if the initial parameters are perfectly accurate"
        ],
        "correct_index": 1,
        "rationale": "Slide 39: 'Note: EM monotonically approaches local maximum.'",
        "distractor_analysis": {
          "0": "It is iterative and, for non-convex functions, rarely finds the global maximum.",
          "1": "Correct. Each step is guaranteed not to decrease the likelihood.",
          "2": "Because it is monotonic, it cannot oscillate; it must level off.",
          "3": "It converges from almost any starting point, though the specific local maximum reached depends on the start."
        }
      },
      {
        "id": 36,
        "topic": "EM Generalization",
        "difficulty": "recall",
        "question": "In the general formulation of EM, what do the values 'z1, ..., zm' represent?",
        "options": [
          "Observed data points",
          "Hidden (latent) values",
          "Model parameters to be optimized",
          "Hyper-parameters for the Beta prior"
        ],
        "correct_index": 1,
        "rationale": "Slide 39 distinguishes between 'observed data points xi' and 'hidden values zj'.",
        "distractor_analysis": {
          "0": "Observed data is denoted as x.",
          "1": "Correct. EM is designed precisely to handle these unobserved variables.",
          "2": "Parameters are denoted as θ.",
          "3": "EM is usually described in a frequentist MLE context, not Bayesian hyper-parameters."
        }
      },
      {
        "id": 37,
        "topic": "Probability measure",
        "difficulty": "recall",
        "question": "A probability measure P maps an event to a number. What is the defined range of this number?",
        "options": [
          "(-∞, +∞)",
          "[0, 1]",
          "Any integer value",
          "[-1, 1]"
        ],
        "correct_index": 1,
        "rationale": "Slide 5 defines P: E -> [0, 1].",
        "distractor_analysis": {
          "0": "Probabilities cannot be negative or infinitely large.",
          "1": "Correct. This is a fundamental axiom of probability.",
          "2": "Probabilities are real numbers, not just integers.",
          "3": "Probabilities are never negative."
        }
      },
      {
        "id": 38,
        "topic": "Binomial vs Bernoulli",
        "difficulty": "analysis",
        "question": "If you set the parameter 'm = 1' in a Binomial distribution, what does it simplify to?",
        "options": [
          "A Poisson distribution",
          "A Bernoulli distribution",
          "A Uniform distribution",
          "A Gaussian distribution"
        ],
        "correct_index": 1,
        "rationale": "A Binomial distribution is the sum of m trials. With only 1 trial, it is by definition a Bernoulli trial.",
        "distractor_analysis": {
          "0": "Poisson is for an infinite number of possible outcomes (counts).",
          "1": "Correct. A single trial with two outcomes is Bernoulli.",
          "2": "Uniform requires all outcomes in the sample space to have equal probability, which is only a special case of Bernoulli (p=0.5).",
          "3": "Gaussian is continuous; Binomial is discrete."
        }
      },
      {
        "id": 39,
        "topic": "Uniform Distribution",
        "difficulty": "application",
        "question": "When rolling a fair 6-sided die, what is the probability of a single face according to the Discrete Uniform Distribution formula?",
        "options": [
          "1/2",
          "1/6",
          "1/m^2",
          "p^x * (1-p)^(1-x)"
        ],
        "correct_index": 1,
        "rationale": "Slide 14: Uniform distribution over {1...m} has P(X=k) = 1/m. For a die, m=6.",
        "distractor_analysis": {
          "0": "This would be for a 2-sided coin.",
          "1": "Correct. 1 divided by the total number of outcomes.",
          "2": "This is not a standard probability mass function formula.",
          "3": "This is the formula for a Bernoulli distribution."
        }
      },
      {
        "id": 40,
        "topic": "Joint Probability",
        "difficulty": "application",
        "question": "If P(A, B) = P(A) * P(B), what can we conclude about events A and B?",
        "options": [
          "They are mutually exclusive",
          "They are independent",
          "A is a subset of B",
          "P(A|B) must be zero"
        ],
        "correct_index": 1,
        "rationale": "The definition of independence for two events is that their joint probability is the product of their marginal probabilities (Slide 9).",
        "distractor_analysis": {
          "0": "Mutually exclusive events have P(A, B) = 0.",
          "1": "Correct. This is the mathematical definition of independence.",
          "2": "A being a subset would mean P(A, B) = P(A).",
          "3": "If they are independent, P(A|B) = P(A), not zero."
        }
      },
      {
        "id": 41,
        "topic": "Marginal Probability",
        "difficulty": "analysis",
        "question": "In the grid example (Slide 12), if you sum all joint probabilities P(X=xi, Y=yj) across all possible values of j, what do you obtain?",
        "options": [
          "The conditional probability P(Y|X)",
          "The marginal probability P(X=xi)",
          "The total probability of the entire sample space (1.0)",
          "The joint probability P(X, Y)"
        ],
        "correct_index": 1,
        "rationale": "The 'Sum Rule' (Slide 12) states that summing the joint probability over all Y yields the marginal for X.",
        "distractor_analysis": {
          "0": "Conditional probability requires dividing the joint by the marginal.",
          "1": "Correct. This is the definition of marginalization.",
          "2": "You would only get 1.0 if you summed over both i and j.",
          "3": "You are starting with the joint; the sum simplifies it."
        }
      },
      {
        "id": 42,
        "topic": "Conjugate Prior Examples",
        "difficulty": "recall",
        "question": "Which of the following is the correct conjugate prior for a Poisson likelihood?",
        "options": [
          "Beta",
          "Gamma",
          "Dirichlet",
          "Gaussian"
        ],
        "correct_index": 1,
        "rationale": "Slide 30 lists the Poisson likelihood's conjugate prior as Gamma.",
        "distractor_analysis": {
          "0": "Beta is for Bernoulli/Binomial.",
          "1": "Correct. Poisson and Gamma form a conjugate pair.",
          "2": "Dirichlet is for Multinomial.",
          "3": "Gaussian is its own conjugate prior."
        }
      },
      {
        "id": 43,
        "topic": "Uncorrelated vs Independent",
        "difficulty": "analysis",
        "question": "Which statement correctly describes the relationship between independence and correlation?",
        "options": [
          "Independent variables are always uncorrelated",
          "Uncorrelated variables are always independent",
          "Correlation and independence are exactly the same concept",
          "Only Gaussian variables can be independent"
        ],
        "correct_index": 0,
        "rationale": "Independence is a much stronger condition than zero correlation. If X and Y are independent, they are necessarily uncorrelated, but the reverse is not always true.",
        "distractor_analysis": {
          "0": "Correct. Independence implies no linear (or non-linear) relationship.",
          "1": "Variables can be uncorrelated (no linear relationship) but still have a non-linear dependency.",
          "2": "Correlation only measures linear dependency; independence measures all dependency.",
          "3": "Any type of random variable can be independent."
        }
      },
      {
        "id": 44,
        "topic": "Maximum Likelihood Logic",
        "difficulty": "analysis",
        "question": "If you toss a coin 'n' times and see 'k' heads, the MLE for the probability of heads 'p' is k/n. Why is this logically intuitive?",
        "options": [
          "It assumes the coin is always fair (0.5)",
          "It matches the observed relative frequency of the event",
          "It is the only value that makes the likelihood exactly 1.0",
          "It minimizes the chance of seeing any tails"
        ],
        "correct_index": 1,
        "rationale": "Slide 32 derives p = k/n. This matches the frequentist intuition that the best estimate of a probability is its occurrence rate in the sample.",
        "distractor_analysis": {
          "0": "MLE doesn't assume p=0.5; it estimates p from data.",
          "1": "Correct. It aligns the model's 'expected' rate with the 'observed' rate.",
          "2": "The likelihood at k/n is typically much less than 1.0; it's just the maximum possible value.",
          "3": "It doesn't minimize tails; it finds the balance that best fits the observed count of both heads and tails."
        }
      },
      {
        "id": 45,
        "topic": "Mixture Model Applications",
        "difficulty": "application",
        "question": "In a medical study of height and weight distribution (Slide 23/35), why might a Mixture Model be more appropriate than a single Multivariate Gaussian?",
        "options": [
          "Because height and weight are always independent",
          "Because the population may contain distinct subgroups (e.g., men and women) with different centers",
          "Because MLE can only be used with one variable at a time",
          "Because the sample size is likely too large for a single Gaussian"
        ],
        "correct_index": 1,
        "rationale": "Slide 35 explicitly mentions 'weights of women and men' as an example for mixture models, as they likely form two overlapping clusters.",
        "distractor_analysis": {
          "0": "Height and weight are usually highly correlated, not independent.",
          "1": "Correct. Mixtures account for sub-populations within the data.",
          "2": "MLE works for any number of variables (Multivariate MLE).",
          "3": "Gaussians handle large samples well; the issue is 'multi-modality' (multiple peaks)."
        }
      },
      {
        "id": 46,
        "topic": "EM Loop",
        "difficulty": "recall",
        "question": "What is the correct iterative loop for the EM algorithm for mixture models?",
        "options": [
          "E-step -> M-step -> Iterate until convergence",
          "M-step -> E-step -> Finalize parameters",
          "Prior -> Likelihood -> Evidence",
          "Sample -> Evaluate -> Discard"
        ],
        "correct_index": 0,
        "rationale": "Slide 37 defines the steps: 1. E-step, 2. M-step, 3. Iterate until convergence.",
        "distractor_analysis": {
          "0": "Correct. You need the E-step (assignments) to perform the M-step (updates).",
          "1": "You cannot perform a meaningful M-step without the E-step's membership weights.",
          "2": "This describes the components of Bayes' theorem, not an iterative algorithm loop.",
          "3": "This is not a standard description of any statistical algorithm in the slides."
        }
      },
      {
        "id": 47,
        "topic": "Cumulative Distribution Function (CDF)",
        "difficulty": "analysis",
        "question": "As x approaches positive infinity, what value must the Cumulative Distribution Function F(x) approach?",
        "options": [
          "0",
          "0.5",
          "1",
          "Infinity"
        ],
        "correct_index": 2,
        "rationale": "The CDF represents the total probability that X is less than or equal to x. As x covers the entire support, the probability must sum to 1.",
        "distractor_analysis": {
          "0": "The CDF approaches 0 as x goes to negative infinity.",
          "1": "This is only true at the median.",
          "2": "Correct. All probability mass is accounted for.",
          "3": "Probability values are capped at 1."
        }
      },
      {
        "id": 48,
        "topic": "Pareto Principle",
        "difficulty": "application",
        "question": "If a social network's follower distribution follows a Pareto distribution, what would you expect to observe?",
        "options": [
          "Most users have almost exactly the same number of followers",
          "A very small number of 'super-users' have the vast majority of all followers",
          "The number of followers is distributed as a bell curve centered at 500",
          "Users are either 'celebrities' (1M followers) or 'bots' (0 followers) with nothing in between"
        ],
        "correct_index": 1,
        "rationale": "Slide 19 explains the Pareto principle: 80% of effects (followers) come from 20% of causes (users). This is a heavy-tailed distribution.",
        "distractor_analysis": {
          "0": "This would be a distribution with very low variance (not Pareto).",
          "1": "Correct. This is the characteristic 'winner-take-all' nature of power-law distributions.",
          "2": "This would be a Gaussian distribution.",
          "3": "A Pareto distribution is continuous and covers all values, though higher values are rarer."
        }
      },
      {
        "id": 49,
        "topic": "MLE bias",
        "difficulty": "analysis",
        "question": "For a Gaussian distribution, the MLE for the variance (σ^2) is Σ(xi - μ)^2 / n. Why is this sometimes criticized in statistics compared to the 'sample variance' (which divides by n-1)?",
        "options": [
          "The MLE is always larger than the true variance",
          "The MLE is a biased estimator that tends to underestimate the true variance in small samples",
          "The MLE cannot be calculated if the mean is unknown",
          "The MLE requires the data to be independent"
        ],
        "correct_index": 1,
        "rationale": "While not explicitly in the slides, the formula on Slide 33 shows division by 'n'. In statistical theory, dividing by 'n' rather than 'n-1' makes the MLE biased for variance.",
        "distractor_analysis": {
          "0": "Dividing by a larger number (n) makes the result smaller, not larger.",
          "1": "Correct. This is a classic result in statistical theory regarding MLE bias.",
          "2": "MLE uses the estimated mean (μ-hat) to calculate variance.",
          "3": "The likelihood derivation itself assumes i.i.d. data."
        }
      },
      {
        "id": 50,
        "topic": "EM and Local Maxima",
        "difficulty": "analysis",
        "question": "If the EM algorithm is run multiple times on the same GMM problem but with different random initializations, what is a likely outcome?",
        "options": [
          "It will always converge to the exact same parameter values",
          "It may converge to different local maxima with different likelihood values",
          "It will fail to converge on all but one of the runs",
          "It will prove that the likelihood function is actually convex"
        ],
        "correct_index": 1,
        "rationale": "Because the GMM likelihood is non-convex (Slide 36), the algorithm's end point (local maximum) depends heavily on the starting point.",
        "distractor_analysis": {
          "0": "This only happens if there is only one peak (convexity).",
          "1": "Correct. This is why practitioners often use multiple restarts for EM.",
          "2": "EM is very robust in terms of convergence; it will converge to something almost every time.",
          "3": "Running an algorithm doesn't change the underlying mathematical shape of the function."
        }
      }
    ]
  },
  "foundations-02": {
    "title": "Foundations of AI - Statistics",
    "questions": [
      {
        "id": 1,
        "topic": "Sampling",
        "difficulty": "analysis",
        "question": "A researcher wants to study the average study time of students at a massive university. They decide to survey only students present in the main library on a Friday night. Which statement best describes the statistical implications of this sampling method?",
        "options": [
          "It represents a stratified sample because the library is a specific subpopulation.",
          "It is a convenience sample that likely introduces a positive bias toward study hours.",
          "The sample is unbiased because every student has the potential to be in the library.",
          "The law of large numbers will eventually correct the sampling bias as more students are surveyed in the library."
        ],
        "correct_index": 1,
        "rationale": "Sampling only those easily accessible (convenience sampling) often leads to a sample that does not represent the broader population. In this case, students in a library on a Friday night are likely to study more than the average student, leading to a biased estimate.",
        "distractor_analysis": {
          "0": "Stratified sampling requires deliberate partitioning of the entire population, not just picking one group.",
          "1": "Correct. This identifies the method (convenience) and the likely direction of bias.",
          "2": "Unbiasedness requires every element in the total population to have an equal (or known) probability of selection, which isn't true here.",
          "3": "The Law of Large Numbers ensures convergence to the *sample* mean's expected value, not the *population* mean if the sampling process is inherently biased."
        }
      },
      {
        "id": 2,
        "topic": "Sampling",
        "difficulty": "application",
        "question": "In a scenario where a population has highly distinct subgroups (e.g., different income levels) and you want to ensure each group is represented proportionally to its size, which sampling strategy is most appropriate?",
        "options": [
          "Systematic sampling",
          "Convenience sampling",
          "Stratified sampling",
          "Random sampling"
        ],
        "correct_index": 2,
        "rationale": "Stratified sampling involves dividing the population into subpopulations (strata) and sampling from each to ensure representation, especially when there is high variance between groups.",
        "distractor_analysis": {
          "0": "Systematic sampling follows a fixed interval (every k-th element) and doesn't account for group proportions.",
          "1": "Convenience sampling is based on ease of access and lacks representativeness.",
          "2": "Correct. This explicitly addresses the need to reflect subpopulation proportions.",
          "3": "Simple random sampling might miss smaller subgroups by chance, whereas stratified sampling guarantees their inclusion."
        }
      },
      {
        "id": 3,
        "topic": "Estimators",
        "difficulty": "recall",
        "question": "What is the primary conceptual difference between 'Empirical Variance' and 'Sample Variance' as presented in the slides?",
        "options": [
          "Empirical variance uses the true population mean, while sample variance uses the empirical mean.",
          "Sample variance is an unbiased estimator because it divides by (n-1), whereas empirical variance is biased.",
          "Empirical variance is used for continuous data, while sample variance is used for discrete data.",
          "There is no difference; they are different names for the same mathematical function."
        ],
        "correct_index": 1,
        "rationale": "Slide 12 shows that the empirical variance (dividing by n) is a biased estimator of the true variance. Using (n-1) in the sample variance formula corrects this bias.",
        "distractor_analysis": {
          "0": "Both typically use the empirical (sample) mean in practice.",
          "1": "Correct. The (n-1) factor (Bessel's correction) is the key to achieving unbiasedness.",
          "2": "The distinction is about estimator bias, not the nature of the data (discrete vs. continuous).",
          "3": "The slides explicitly list them as different formulas with different expected values."
        }
      },
      {
        "id": 4,
        "topic": "Bias/Unbiased",
        "difficulty": "analysis",
        "question": "If an estimator is 'consistent,' what happens as the sample size (n) approaches infinity?",
        "options": [
          "The estimator's bias remains constant but its variance reduces to zero.",
          "The estimator will always equal the true parameter for any n > 30.",
          "The probability of the estimator deviating from the true parameter by any margin epsilon goes to zero.",
          "The estimator becomes normally distributed regardless of its original bias."
        ],
        "correct_index": 2,
        "rationale": "Slide 11 defines consistency: an estimator is consistent if the probability that the estimate differs from the true value by more than epsilon vanishes as n goes to infinity.",
        "distractor_analysis": {
          "0": "If an estimator is consistent, the bias must also vanish as n approaches infinity.",
          "1": "Consistency is a probabilistic limit, not a guarantee of exactness for a specific finite n.",
          "2": "Correct. This is the formal definition of convergence in probability for a consistent estimator.",
          "3": "This describes the Central Limit Theorem's effect on the distribution of the mean, not the general definition of consistency."
        }
      },
      {
        "id": 5,
        "topic": "Mean/Variance Sensitivity",
        "difficulty": "application",
        "question": "You are analyzing a dataset of house prices where one entry was accidentally entered as $100,000,000 instead of $1,000,000. How will this outlier most likely affect your estimators?",
        "options": [
          "The median will increase significantly, but the mean will remain stable.",
          "The mean and variance will increase significantly, while the median remains relatively stable.",
          "The correlation between house size and price will become perfectly linear.",
          "The empirical distribution function will become a smooth bell curve."
        ],
        "correct_index": 1,
        "rationale": "The mean and variance are highly sensitive to outliers because they incorporate every value's magnitude (and the square of the magnitude for variance). The median, being a rank-based measure, is robust to extreme outliers.",
        "distractor_analysis": {
          "0": "This is the reverse of reality; the mean is sensitive, the median is robust.",
          "1": "Correct. Outliers 'pull' the mean and drastically inflate variance.",
          "2": "One outlier usually degrades correlation or creates a 'leverage point' rather than making it perfectly linear.",
          "3": "Outliers create heavy tails or 'jumps' in the EDF, not a smooth normal distribution."
        }
      },
      {
        "id": 6,
        "topic": "Correlation",
        "difficulty": "analysis",
        "question": "If the correlation between two random variables X and Y is zero, what can we definitively conclude about their relationship?",
        "options": [
          "X and Y are independent.",
          "There is no linear relationship between X and Y.",
          "X and Y have no relationship of any kind.",
          "The covariance between X and Y must be positive."
        ],
        "correct_index": 1,
        "rationale": "Slide 8 notes that correlation measures linear dependency. A correlation of zero only implies the absence of a linear relationship; the variables could still be non-linearly dependent.",
        "distractor_analysis": {
          "0": "Independence implies zero correlation, but zero correlation does not necessarily imply independence.",
          "1": "Correct. Correlation is specifically a measure of linear association.",
          "2": "They could have a strong non-linear relationship (e.g., Y = X^2).",
          "3": "If correlation is zero, covariance must also be zero (Correlation = Cov / (Sx*Sy))."
        }
      },
      {
        "id": 7,
        "topic": "Law of Large Numbers",
        "difficulty": "analysis",
        "question": "How does the Law of Large Numbers (LLN) justify the use of larger datasets in Data Science?",
        "options": [
          "It guarantees that larger datasets will have a smaller standard deviation than smaller ones.",
          "It ensures that the sample average will converge to the true population expected value as the dataset grows.",
          "It proves that as n increases, the data will naturally follow a Normal distribution.",
          "It states that the error of an estimator will be zero once the sample size exceeds 1,000."
        ],
        "correct_index": 1,
        "rationale": "As per Slide 13, the LLN (both Weak and Strong) describes how the sample average approaches the population mean as the number of trials or observations increases.",
        "distractor_analysis": {
          "0": "LLN is about the mean converging, not the population standard deviation shrinking.",
          "1": "Correct. This is the foundational logic for why 'more data is better' for estimation.",
          "2": "This is the Central Limit Theorem, not the Law of Large Numbers.",
          "3": "LLN describes a limit as n goes to infinity; error doesn't hit zero at a specific finite number like 1,000."
        }
      },
      {
        "id": 8,
        "topic": "Skewness",
        "difficulty": "application",
        "question": "In a 'Positive Skew' (Right Skew) distribution, such as the yearly income example on Slide 16, what is the typical relationship between the mean and the median?",
        "options": [
          "The mean is significantly lower than the median.",
          "The mean and median are identical.",
          "The mean is significantly higher than the median.",
          "The relationship between mean and median cannot be determined by skewness."
        ],
        "correct_index": 2,
        "rationale": "In a right-skewed distribution, the long tail on the right side (high values) pulls the mean upward more than it affects the median.",
        "distractor_analysis": {
          "0": "This describes a negative (left) skew.",
          "1": "This describes a perfectly symmetric distribution, like a Normal distribution.",
          "2": "Correct. High outliers in the tail increase the mean while the median remains at the 50th percentile.",
          "3": "Skewness is defined by the relative positions of the mean, median, and mode."
        }
      },
      {
        "id": 9,
        "topic": "Central Limit Theorem",
        "difficulty": "recall",
        "question": "According to the Central Limit Theorem (CLT), what specific entity becomes 'approximately normally distributed' as n becomes large?",
        "options": [
          "The underlying population data itself.",
          "The distribution of the individual samples.",
          "The distribution of the sample mean (or sum).",
          "The variance of the sample."
        ],
        "correct_index": 2,
        "rationale": "Slide 19 states that the cdf of the sum (Z) or the mean of i.i.d. variables converges to the cdf of a normal distribution. The theorem applies to the estimator (mean), not the raw data.",
        "distractor_analysis": {
          "0": "The population distribution remains whatever it originally was (e.g., uniform, exponential).",
          "1": "Individual samples follow the population distribution.",
          "2": "Correct. The *sampling distribution of the mean* is what becomes normal.",
          "3": "The sample variance does not become normally distributed through the CLT (it follows a Chi-square related distribution)."
        }
      },
      {
        "id": 10,
        "topic": "Hypothesis Testing",
        "difficulty": "analysis",
        "question": "When we 'retain' the null hypothesis (H0), what have we actually concluded?",
        "options": [
          "We have proven that H0 is definitively true.",
          "The counter-hypothesis H1 has been proven false.",
          "There is insufficient statistical evidence to reject H0 at the chosen significance level.",
          "The p-value was exactly zero."
        ],
        "correct_index": 2,
        "rationale": "Slide 22 notes the goal is 'falsification'. If we fail to reject (retain) H0, it doesn't mean H0 is true; it just means the data didn't provide enough evidence to suggest otherwise.",
        "distractor_analysis": {
          "0": "Statistics cannot 'prove' a null hypothesis; we only fail to reject it.",
          "1": "Failure to reject H0 does not equate to a proof of H1's falsehood.",
          "2": "Correct. Retaining H0 is a statement about the strength of evidence, not the absolute truth of the hypothesis.",
          "3": "A p-value of zero would lead to a strong rejection of H0, not retention."
        }
      },
      {
        "id": 11,
        "topic": "p-value",
        "difficulty": "recall",
        "question": "Based on Slide 26, how is the p-value defined in the context of hypothesis testing?",
        "options": [
          "The probability that the null hypothesis is true.",
          "The probability that the counter-hypothesis is true.",
          "The minimal significance level at which the null hypothesis can be rejected.",
          "The confidence level (1 - alpha) of the test."
        ],
        "correct_index": 2,
        "rationale": "The slide explicitly defines the p-value as the 'minimal significance level at which H0 can be rejected' based on the observed data.",
        "distractor_analysis": {
          "0": "This is a common misconception; the p-value is a conditional probability of the data, not the hypothesis.",
          "1": "p-values are calculated assuming the null is true; they don't measure the truth of the alternative.",
          "2": "Correct. This is the technical definition provided in the slide deck.",
          "3": "The p-value is compared against the significance level (alpha), but it is not the same as the confidence level."
        }
      },
      {
        "id": 12,
        "topic": "One/Two sided tests",
        "difficulty": "application",
        "question": "You are testing a new drug and you only want to reject the null hypothesis if the drug is *better* than the current standard. Which type of test should you use?",
        "options": [
          "A two-sided test",
          "A one-sided test",
          "A Chi-Square independence test",
          "A Wald test for variance"
        ],
        "correct_index": 1,
        "rationale": "Slide 25 explains that a test where H1 specifies a direction (e.g., mean > theta_0) is a one-sided test. Since we only care if it is 'better', not just 'different', the one-sided test is appropriate.",
        "distractor_analysis": {
          "0": "A two-sided test looks for any difference (better or worse).",
          "1": "Correct. Directional hypotheses require one-sided tests.",
          "2": "Chi-Square tests are typically for goodness-of-fit or independence, not comparing means for directional improvement.",
          "3": "We are testing the mean (efficacy), not the variance."
        }
      },
      {
        "id": 13,
        "topic": "Mean Squared Error",
        "difficulty": "analysis",
        "question": "According to the MSE decomposition (Slide 15), if an estimator is unbiased, what constitutes its Mean Squared Error?",
        "options": [
          "The MSE is equal to the Bias squared.",
          "The MSE is equal to the Variance of the estimator.",
          "The MSE is equal to the sum of the Sample Mean and the Sample Variance.",
          "The MSE is zero."
        ],
        "correct_index": 1,
        "rationale": "Slide 15 shows: MSE = Var + Bias^2. If the estimator is unbiased, Bias = 0, therefore MSE = Var.",
        "distractor_analysis": {
          "0": "This would only be true if Variance was zero.",
          "1": "Correct. For unbiased estimators, the error is purely due to variance.",
          "2": "The decomposition involves Var and Bias^2, not the raw mean and variance values.",
          "3": "MSE is only zero if both bias and variance are zero, which is almost never true for a random sample."
        }
      },
      {
        "id": 14,
        "topic": "Normal Distribution",
        "difficulty": "recall",
        "question": "For a standard normal distribution N(0,1), what percentage of the data roughly falls within one standard deviation ([-1, 1]) of the mean?",
        "options": [
          "50%",
          "95%",
          "68.2%",
          "99.7%"
        ],
        "correct_index": 2,
        "rationale": "Slide 18 shows the bell curve. The area between -1 sigma and +1 sigma is 34.1% + 34.1% = 68.2%.",
        "distractor_analysis": {
          "0": "This is the area to one side of the mean.",
          "1": "This is the area within roughly 2 standard deviations.",
          "2": "Correct. This is a fundamental property of the Normal distribution shown in the graphic.",
          "3": "This is the area within 3 standard deviations."
        }
      },
      {
        "id": 15,
        "topic": "Central Limit Theorem",
        "difficulty": "analysis",
        "question": "What happens to the 'bell shape' of the sample mean distribution as the sample size n increases from 2 to 100?",
        "options": [
          "It becomes wider and flatter.",
          "It becomes increasingly narrower and more 'peaked' around the true mean.",
          "It stays exactly the same shape regardless of n.",
          "It shifts further to the right on the x-axis."
        ],
        "correct_index": 1,
        "rationale": "As n increases, the standard error (sigma/sqrt(n)) decreases. This means the distribution of the mean becomes more concentrated (narrower) around the population mean.",
        "distractor_analysis": {
          "0": "This would happen if n decreased or variance increased.",
          "1": "Correct. Larger n leads to higher precision (lower variance of the mean).",
          "2": "The CLT says it becomes more *Normal* in shape, but the *Corollary* on Slide 19 shows the variance is sigma^2/n, which changes with n.",
          "3": "The distribution is centered at the population mean 'mu', which does not change with n."
        }
      },
      {
        "id": 16,
        "topic": "Covariance",
        "difficulty": "application",
        "question": "If you calculate a 'Sample Covariance' and get a large positive value, what does this tell you qualitatively about the two variables?",
        "options": [
          "As one variable increases, the other tends to increase.",
          "The two variables are identical.",
          "One variable causes the other to change.",
          "The variables have a very low standard deviation."
        ],
        "correct_index": 0,
        "rationale": "Positive covariance indicates that the variables move in the same direction. It does not imply identity or causation.",
        "distractor_analysis": {
          "0": "Correct. This is the qualitative definition of positive covariance.",
          "1": "Identity would imply a specific correlation and variance relationship, not just positive covariance.",
          "2": "Covariance measures association, not causation.",
          "3": "A large covariance often implies large standard deviations in the underlying variables."
        }
      },
      {
        "id": 17,
        "topic": "Confidence Intervals",
        "difficulty": "analysis",
        "question": "If you decrease the significance level (alpha) from 0.05 to 0.01, what happens to the width of the resulting confidence interval?",
        "options": [
          "The interval becomes narrower.",
          "The interval becomes wider.",
          "The interval stays the same.",
          "The interval disappears."
        ],
        "correct_index": 1,
        "rationale": "Decreasing alpha means increasing the confidence level (1-alpha) from 95% to 99%. To be more confident that the interval contains the true parameter, the interval must be wider.",
        "distractor_analysis": {
          "0": "Narrower intervals correspond to *higher* alpha (less confidence).",
          "1": "Correct. More certainty (lower alpha) requires a larger 'net' (wider interval).",
          "2": "The critical value Z or t increases as alpha decreases, changing the width.",
          "3": "The interval only disappears if the sample size were infinite and variance zero."
        }
      },
      {
        "id": 18,
        "topic": "Empirical Median",
        "difficulty": "application",
        "question": "Why is the empirical median preferred over the empirical mean when reporting 'Typical Household Income' in a country with high wealth inequality?",
        "options": [
          "The median is easier to calculate mathematically.",
          "The mean is biased by a few very high earners, making the 'typical' person seem richer than they are.",
          "The median is always higher than the mean in income distributions.",
          "The Law of Large Numbers only applies to the median."
        ],
        "correct_index": 1,
        "rationale": "Slide 16 explains this exact scenario. High outliers (millionaires) pull the mean up significantly, while the median remains a better representation of the middle of the population.",
        "distractor_analysis": {
          "0": "Ease of calculation is not the reason; statistical representativeness is.",
          "1": "Correct. This is the 'usefulness' argument from the slides.",
          "2": "In right-skewed distributions like income, the mean is actually higher than the median.",
          "3": "The LLN primarily concerns the convergence of the sample average (mean)."
        }
      },
      {
        "id": 19,
        "topic": "Law of Large Numbers",
        "difficulty": "analysis",
        "question": "In the coin flip example on Slide 13, why does flipping a coin 2000 times give a 'better' mean than flipping it 5 times?",
        "options": [
          "Because 2000 is a lucky number in statistics.",
          "The probability of heads changes as you flip more often.",
          "The variance of the sample average decreases as n increases, making the estimate more stable.",
          "The Strong Law of Large Numbers only activates after 1000 trials."
        ],
        "correct_index": 2,
        "rationale": "According to the LLN, the sample average converges to the expected value. The variance of this average is sigma^2/n, so a larger n (2000 vs 5) significantly reduces the fluctuation around the true mean.",
        "distractor_analysis": {
          "0": "Statistics relies on mathematical proof, not luck.",
          "1": "The underlying probability 'p' is constant for i.i.d. trials.",
          "2": "Correct. Increased sample size leads to a more 'stable' and accurate average.",
          "3": "LLN applies to any n, but the convergence is 'closer' as n grows; there is no 'activation' threshold."
        }
      },
      {
        "id": 20,
        "topic": "Hypothesis Testing",
        "difficulty": "recall",
        "question": "What is the 'null hypothesis' (H0) generally formulated to be?",
        "options": [
          "The most complex explanation for the data.",
          "The claim that the researcher is trying to prove is true.",
          "A statement of 'no effect' or the simplest possible assumption.",
          "A statement that the sample variance is zero."
        ],
        "correct_index": 2,
        "rationale": "Slide 27 describes the 'General recipe': formulate the null hypothesis as the 'simplest hypothesis'. It usually represents the status quo or 'no difference'.",
        "distractor_analysis": {
          "0": "We aim for parsimony; the null is the simplest state.",
          "1": "The researcher usually wants to *reject* the null in favor of the alternative (H1).",
          "2": "Correct. This is the standard statistical convention.",
          "3": "Variance is almost never zero; H0 is about population parameters, not making variance disappear."
        }
      },
      {
        "id": 21,
        "topic": "Bias/Unbiased",
        "difficulty": "analysis",
        "question": "If you use the 'Empirical Variance' formula (dividing by n) on a small sample of 5 people's heights, how will your estimate likely relate to the true population variance?",
        "options": [
          "It will be exactly correct.",
          "It will tend to underestimate the true variance.",
          "It will tend to overestimate the true variance.",
          "It will be unbiased but inconsistent."
        ],
        "correct_index": 1,
        "rationale": "Slide 12 shows E(S_em^2) = [(n-1)/n] * Var(X). Since (n-1)/n is less than 1, the empirical variance systematically underestimates the true variance (it is biased downward).",
        "distractor_analysis": {
          "0": "Estimators on small samples are rarely 'exactly' correct due to sampling error and bias.",
          "1": "Correct. This is why we use (n-1) to 'inflate' the estimate slightly to reach unbiasedness.",
          "2": "Dividing by n makes the denominator larger, which makes the result smaller (underestimation).",
          "3": "It is both biased and consistent (as n goes to infinity, (n-1)/n goes to 1)."
        }
      },
      {
        "id": 22,
        "topic": "Estimators",
        "difficulty": "recall",
        "question": "What does it mean for an estimator to be 'Asymptotically Normal' (as mentioned for MLE on Slide 17)?",
        "options": [
          "The estimator is only accurate if the original data was normal.",
          "As the sample size increases, the distribution of the estimator approaches a normal distribution.",
          "The estimator's value eventually becomes zero.",
          "The estimator can only be used on data from Wikipedia."
        ],
        "correct_index": 1,
        "rationale": "Asymptotic normality means that for large n, the sampling distribution of the estimator (like the Maximum Likelihood Estimator) behaves like a normal distribution, regardless of the population distribution.",
        "distractor_analysis": {
          "0": "Asymptotic normality is a property of the estimator's distribution, not the data's.",
          "1": "Correct. This property allows us to use Z-tables for testing even with non-normal data if n is large.",
          "2": "Normality refers to the shape, not a value of zero.",
          "3": "Wikipedia was just the source for the images, not a constraint on the math."
        }
      },
      {
        "id": 23,
        "topic": "p-value",
        "difficulty": "application",
        "question": "A p-value of 0.03 is calculated for a test. If your significance level (alpha) was set at 0.05, what is your conclusion?",
        "options": [
          "Reject the null hypothesis; the result is statistically significant.",
          "Retain the null hypothesis; the result is not significant.",
          "Increase the sample size until the p-value reaches 0.01.",
          "The test is invalid because the p-value must be lower than 0.01."
        ],
        "correct_index": 0,
        "rationale": "If p-value < alpha, we reject the null hypothesis. Since 0.03 < 0.05, the result is significant.",
        "distractor_analysis": {
          "0": "Correct. The evidence is strong enough to reject H0 at the 5% level.",
          "1": "We only retain if p-value > alpha.",
          "2": "'P-hacking' (increasing n just to change the p-value) is an unethical/incorrect practice.",
          "3": "The threshold (alpha) is chosen by the researcher; 0.05 is the most common standard."
        }
      },
      {
        "id": 24,
        "topic": "Skewness",
        "difficulty": "analysis",
        "question": "Looking at the 'Empirical Skew' formula on Slide 17, what is the effect of the cubed term (xi - mean)^3 in the numerator?",
        "options": [
          "It ensures the skewness value is always positive.",
          "It preserves the sign of the deviations, allowing us to see if the tail is on the left or right.",
          "It eliminates the effect of outliers.",
          "It makes the formula identical to the variance formula."
        ],
        "correct_index": 1,
        "rationale": "Cubing a negative number stays negative; cubing a positive stays positive. This allows the formula to indicate direction (negative for left-skew, positive for right-skew).",
        "distractor_analysis": {
          "0": "Squaring (like in variance) makes values positive; cubing does not.",
          "1": "Correct. This is why the third moment is used for skewness.",
          "2": "Cubing actually *magnifies* the effect of outliers.",
          "3": "Variance uses a squared term (second moment), not a cubed term."
        }
      },
      {
        "id": 25,
        "topic": "Central Limit Theorem",
        "difficulty": "application",
        "question": "You have a Galton Machine (Slide 21). Each ball's path is a sequence of i.i.d. Bernoulli choices. Why do the balls form a bell curve at the bottom?",
        "options": [
          "Because the machine is tilted.",
          "The sum of many independent random movements converges to a normal distribution.",
          "The balls are attracted to the center by gravity.",
          "The Law of Large Numbers forces every ball to land in the center bin."
        ],
        "correct_index": 1,
        "rationale": "The Galton Machine is a physical demonstration of the CLT. The final position of a ball is the sum of many independent 'left/right' steps, leading to a normal distribution of final positions.",
        "distractor_analysis": {
          "0": "Tilting would just shift the mean, not create the bell shape.",
          "1": "Correct. This is the CLT in action.",
          "2": "Gravity pulls them down, but the horizontal 'spread' is defined by probability, not gravitational attraction to a point.",
          "3": "LLN says the *average* of all balls will be at the center; CLT explains the *shape* of the entire pile."
        }
      },
      {
        "id": 26,
        "topic": "t-Test",
        "difficulty": "analysis",
        "question": "Why would a researcher use a t-test instead of a Z-test (Slide 33)?",
        "options": [
          "The sample size is extremely large (n > 10,000).",
          "The population variance is unknown and must be estimated from the sample.",
          "The data is categorical rather than numerical.",
          "The t-test is only used for one-sided hypotheses."
        ],
        "correct_index": 1,
        "rationale": "Slide 33 states the t-test is for 'unknown mean and unknown variance'. When we estimate variance from the sample (s^2), the distribution of the mean follows a Student's t-distribution rather than a normal Z-distribution.",
        "distractor_analysis": {
          "0": "For very large n, the t-distribution becomes nearly identical to the Z-distribution.",
          "1": "Correct. Estimating variance adds uncertainty, which the t-distribution accounts for with its 'heavier tails'.",
          "2": "Both tests are for numerical means.",
          "3": "Both Z and t tests can be one or two-sided."
        }
      },
      {
        "id": 27,
        "topic": "Correlation",
        "difficulty": "application",
        "question": "Slide 8 mentions that correlation (r) is 1 if Y = aX + b and a > 0. What does r = -1 imply?",
        "options": [
          "There is no relationship between X and Y.",
          "There is a perfect linear relationship where Y decreases as X increases.",
          "The variables have different units.",
          "The variance of Y is negative."
        ],
        "correct_index": 1,
        "rationale": "A correlation of -1 indicates a perfect negative linear relationship. As one variable goes up, the other goes down in a perfectly straight line.",
        "distractor_analysis": {
          "0": "That would be r = 0.",
          "1": "Correct. The sign of the correlation indicates the direction of the slope.",
          "2": "Correlation is dimensionless; units don't affect its value.",
          "3": "Variance is always non-negative by definition."
        }
      },
      {
        "id": 28,
        "topic": "Bias/Unbiased",
        "difficulty": "analysis",
        "question": "Which of the following is considered the 'Best Estimator' for the true mean of many useful distributions (Slide 14)?",
        "options": [
          "The Empirical Median",
          "The Sample Mean",
          "The Maximum value in the sample",
          "The Sample Variance"
        ],
        "correct_index": 1,
        "rationale": "Slide 14 explicitly notes that the 'sample mean is the best estimator of the true mean for many useful distributions' because it is unbiased and has the lowest variance among unbiased estimators (in many cases).",
        "distractor_analysis": {
          "0": "Median is robust but often has higher variance than the mean for normal-like distributions.",
          "1": "Correct. It is the Minimum Variance Unbiased Estimator (MVUE) for many cases.",
          "2": "The maximum is a highly biased estimator of the mean.",
          "3": "Variance is an estimator of spread, not the mean (location)."
        }
      },
      {
        "id": 29,
        "topic": "Chi-Square",
        "difficulty": "recall",
        "question": "What is the primary purpose of a Chi-Square Goodness-of-Fit test (Slide 35)?",
        "options": [
          "To see if a sample follows a proposed discrete distribution.",
          "To determine the correlation between two continuous variables.",
          "To estimate the mean of a normal distribution with unknown variance.",
          "To prove that two variables are independent."
        ],
        "correct_index": 0,
        "rationale": "As per Slide 35, the test checks if the observed frequencies 'follow a proposed discrete distribution'.",
        "distractor_analysis": {
          "0": "Correct. It compares observed vs. expected frequencies.",
          "1": "Correlation is measured by 'r' (Pearson), not Chi-square.",
          "2": "That is the role of the t-test.",
          "3": "Chi-Square can be used to *test* independence (Slide 37), but you can't 'prove' it, only fail to reject the null of independence."
        }
      },
      {
        "id": 30,
        "topic": "Sampling",
        "difficulty": "analysis",
        "question": "Why is 'Systematic Sampling' (Slide 6) marked as 'biased' in some contexts?",
        "options": [
          "It requires a computer to pick the elements.",
          "If the underlying list has a periodic pattern that matches the sampling interval 'k', it can produce a non-representative sample.",
          "It always results in the same mean as convenience sampling.",
          "It only works for populations smaller than 100."
        ],
        "correct_index": 1,
        "rationale": "If a population list is ordered in a way that repeats every k elements (e.g., every 10th person is a manager), picking every 10th person will result in a sample of only managers, which is highly biased.",
        "distractor_analysis": {
          "0": "Using a computer doesn't cause bias.",
          "1": "Correct. Periodicity in the data is the main risk for systematic sampling.",
          "2": "Systematic is usually much better than convenience, but less robust than pure random sampling.",
          "3": "Sampling methods work for any population size; the constraints are about logic, not size."
        }
      },
      {
        "id": 31,
        "topic": "Hypothesis Testing",
        "difficulty": "application",
        "question": "If your test statistic falls inside the 'Critical Region' (Slide 24), what is the appropriate action?",
        "options": [
          "Retain the null hypothesis.",
          "Reject the null hypothesis.",
          "Change the significance level until it falls outside.",
          "Discard the data as an outlier."
        ],
        "correct_index": 1,
        "rationale": "The critical region is the set of values for which the null hypothesis is rejected. If the statistic is in this region, it means the result is unlikely to have occurred by chance under H0.",
        "distractor_analysis": {
          "0": "We retain if it is *outside* the critical region.",
          "1": "Correct. This is the definition of the critical region.",
          "2": "This is unethical and invalidates the statistical process.",
          "3": "The entire point of the test is to see where the data falls; discarding it defeats the purpose."
        }
      },
      {
        "id": 32,
        "topic": "p-value",
        "difficulty": "analysis",
        "question": "Which of the following would lead to a *smaller* p-value, assuming all else remains constant?",
        "options": [
          "A smaller sample size.",
          "A larger difference between the observed sample mean and the null hypothesis mean.",
          "A larger sample variance.",
          "A higher significance level alpha."
        ],
        "correct_index": 1,
        "rationale": "The p-value measures how 'extreme' the data is relative to the null. A larger gap between the observation and the null expectation is more extreme, leading to a smaller p-value.",
        "distractor_analysis": {
          "0": "Smaller samples have more uncertainty, leading to *larger* p-values (harder to reject).",
          "1": "Correct. More 'extreme' data = lower p-value.",
          "2": "More noise (variance) makes the result less certain, leading to a larger p-value.",
          "3": "Alpha is a threshold we choose; it doesn't change the p-value itself (which is calculated from the data)."
        }
      },
      {
        "id": 33,
        "topic": "Estimators",
        "difficulty": "recall",
        "question": "According to Slide 11, what is 'Bias' in the context of an estimator theta-hat?",
        "options": [
          "The difference between the expected value of the estimator and the true parameter.",
          "The spread of the estimator's values.",
          "The mistake made by the researcher in collecting data.",
          "The ratio of the sample mean to the population mean."
        ],
        "correct_index": 0,
        "rationale": "Slide 11 defines an unbiased estimator as one where E(theta-hat) = theta. Bias is the difference between these two values.",
        "distractor_analysis": {
          "0": "Correct. It measures if the estimator is 'on target' on average.",
          "1": "The spread is the 'Variance', not the Bias.",
          "2": "Bias is a statistical property of the formula, not just human error.",
          "3": "Bias is an additive difference, not a ratio."
        }
      },
      {
        "id": 34,
        "topic": "One/Two sided tests",
        "difficulty": "analysis",
        "question": "What is a disadvantage of using a two-sided test when you have a specific directional hypothesis?",
        "options": [
          "It is more likely to give a false positive (Type I error).",
          "It requires more data to calculate.",
          "It is 'harder' to reject the null because the significance level alpha is split between two tails.",
          "It cannot be used with the t-distribution."
        ],
        "correct_index": 2,
        "rationale": "In a two-sided test, alpha is divided (e.g., 0.025 in each tail). This means you need a more extreme result in one specific direction to reject the null compared to a one-sided test where the full 0.05 is in one tail.",
        "distractor_analysis": {
          "0": "A two-sided test is actually more conservative (less likely to have a false positive in one specific direction).",
          "1": "Data requirements are the same; the difference is in the critical values used.",
          "2": "Correct. You sacrifice 'power' in one direction to check for both.",
          "3": "t-distributions are used for both one and two-sided tests."
        }
      },
      {
        "id": 35,
        "topic": "Standard Error",
        "difficulty": "recall",
        "question": "How is 'Standard Error' related to 'Standard Deviation' (Slide 30)?",
        "options": [
          "They are the same thing.",
          "Standard error is the standard deviation of the population.",
          "Standard error is the standard deviation of an estimator (like the mean).",
          "Standard error is the square of the standard deviation."
        ],
        "correct_index": 2,
        "rationale": "Slide 30 defines s = sqrt(Var(theta-hat)) as the 'standard error'. It represents the typical distance an estimate falls from the true parameter.",
        "distractor_analysis": {
          "0": "Standard deviation usually refers to raw data; standard error refers to the estimate.",
          "1": "No, that is 'sigma'.",
          "2": "Correct. It is the 'SD' of the sampling distribution.",
          "3": "The square of SD is Variance."
        }
      },
      {
        "id": 36,
        "topic": "Hypothesis Testing",
        "difficulty": "application",
        "question": "You conduct an IQ test on students (Slide 23). H0 is mean=100. Your sample mean is 115. If the p-value is 0.001, what is the best qualitative interpretation?",
        "options": [
          "The students are definitely geniuses.",
          "A sample mean of 115 is extremely unlikely to occur if the true population mean were really 100.",
          "The test was biased because 115 is too far from 100.",
          "The students' IQs are exactly 115 on average."
        ],
        "correct_index": 1,
        "rationale": "A very low p-value (0.001) means that the observed data is highly inconsistent with the null hypothesis. It suggests we should reject the idea that the true mean is 100.",
        "distractor_analysis": {
          "0": "Statistics suggests a difference, but 'genius' is a subjective label not proven by one test.",
          "1": "Correct. This is the definition of a small p-value: the data is rare under the null.",
          "2": "A large difference isn't bias; it's 'evidence' for a real effect.",
          "3": "115 is the *sample* mean; the population mean could be 112, 118, etc."
        }
      },
      {
        "id": 37,
        "topic": "Consistency",
        "difficulty": "analysis",
        "question": "Slide 12 states that Empirical Variance is a 'biased consistent' estimator. Why is it still useful despite the bias?",
        "options": [
          "Because the bias is zero when n=1.",
          "Because as the dataset becomes very large (n -> infinity), the bias disappears.",
          "Because biased estimators have lower variance than unbiased ones.",
          "Because 'biased' in statistics means it is more fair."
        ],
        "correct_index": 1,
        "rationale": "Consistency means the estimator converges to the true value as n grows. Even if it is biased for small n, the bias (n-1)/n becomes negligible as n increases (e.g., 999,999/1,000,000).",
        "distractor_analysis": {
          "0": "The bias is actually infinite or undefined for n=1 (division by zero in sample variance).",
          "1": "Correct. In 'Big Data' contexts, the difference between n and n-1 is irrelevant.",
          "2": "While some biased estimators have lower variance (Bias-Variance trade-off), that is not the definition of why a *consistent* estimator is useful.",
          "3": "In statistics, 'bias' is a mathematical term for 'off-center', not a social term for 'fairness'."
        }
      },
      {
        "id": 38,
        "topic": "Central Limit Theorem",
        "difficulty": "analysis",
        "question": "Why is the Central Limit Theorem considered a 'bridge' between different types of data distributions and the Normal distribution?",
        "options": [
          "It says that all data eventually becomes Normal if you wait long enough.",
          "It allows us to use Normal-based tests (like Z-tests) on the mean of *any* distribution, provided the sample size is large.",
          "It turns discrete data into continuous data.",
          "It proves that the population mean is always zero."
        ],
        "correct_index": 1,
        "rationale": "The CLT's power is that it doesn't matter what the 'underlying' distribution is (Uniform, Binomial, etc.); the *average* will behave normally. This is why we can use the same tests for income, height, or error rates.",
        "distractor_analysis": {
          "0": "The data doesn't change; only the *average* of the data follows the bell curve.",
          "1": "Correct. This is why the Normal distribution is 'Universal' in statistics.",
          "2": "The mean of discrete data is often a continuous-looking distribution, but that's not the 'bridge' CLT provides.",
          "3": "The mean is 'mu', which can be any finite value."
        }
      },
      {
        "id": 39,
        "topic": "Correlation",
        "difficulty": "recall",
        "question": "According to Slide 8, what happens if you turn on a car where the engine is broken and the fuel tank is empty?",
        "options": [
          "They become independent.",
          "They gain a conditional dependence.",
          "The correlation becomes exactly 1.",
          "The variance of the fuel levels increases."
        ],
        "correct_index": 1,
        "rationale": "The slide uses this as an example of conditional dependence: variables can be independent normally but become dependent given a third variable (the car's failure to start).",
        "distractor_analysis": {
          "0": "The example states they were independent *before* the third variable was introduced.",
          "1": "Correct. This highlights that independence can change based on context/evidence.",
          "2": "Dependency doesn't mean *perfect linear* dependency (correlation 1).",
          "3": "Fuel level variance is irrelevant to the logical point of dependency."
        }
      },
      {
        "id": 40,
        "topic": "Mean/Variance Sensitivity",
        "difficulty": "analysis",
        "question": "You have a sample [10, 12, 11, 13, 11]. You add a new data point: 500. Which estimator will change the most in proportion to its original value?",
        "options": [
          "The Empirical Median",
          "The Empirical Mean",
          "The Sample Variance",
          "The Sample Skewness"
        ],
        "correct_index": 2,
        "rationale": "The variance involves the *square* of the distance from the mean. Since the new point is very far away, its contribution to the variance ( (500-mean)^2 ) will be enormous compared to its contribution to the mean.",
        "distractor_analysis": {
          "0": "The median will barely change (it might shift from 11 to 11.5 or 12).",
          "1": "The mean will increase, but not as drastically as variance which squares the outlier's distance.",
          "2": "Correct. Variance is extremely sensitive to large outliers due to the squaring term.",
          "3": "Skewness would also change significantly, but variance is the fundamental measure of spread that 'explodes' with such an outlier."
        }
      },
      {
        "id": 41,
        "topic": "Law of Large Numbers",
        "difficulty": "application",
        "question": "A casino relies on the Law of Large Numbers to ensure profit. How does this work?",
        "options": [
          "They ensure every individual player wins at least once.",
          "Over thousands of bets, the average payout to players will converge to the expected value, which is slightly less than the bet amount.",
          "They use a Z-test to see if players are cheating.",
          "They only allow 30 people to play at a time to keep n small."
        ],
        "correct_index": 1,
        "rationale": "The 'House Edge' is a small difference in expected value. LLN ensures that over a large 'n' (many bets), the actual average outcome for the casino will be very close to that theoretical expected profit.",
        "distractor_analysis": {
          "0": "LLN doesn't guarantee individual outcomes.",
          "1": "Correct. This is the 'stability' that LLN provides for large datasets.",
          "2": "Cheating detection is a different application; the core business model is based on LLN.",
          "3": "Small n would increase the casino's risk (variance); they want n as large as possible."
        }
      },
      {
        "id": 42,
        "topic": "Normal Distribution",
        "difficulty": "analysis",
        "question": "In the 'Standard Normal Distribution' formula (Slide 18), what is the effect of the term 'sigma' in the denominator of the exponent?",
        "options": [
          "It shifts the distribution left or right.",
          "It determines the height of the curve at the mean.",
          "It scales the horizontal 'spread' of the distribution.",
          "It ensures the area under the curve is always zero."
        ],
        "correct_index": 2,
        "rationale": "The sigma (standard deviation) acts as a scaling factor. A larger sigma spreads the curve out horizontally, while a smaller sigma squeezes it toward the mean.",
        "distractor_analysis": {
          "0": "That is 'mu' (the mean).",
          "1": "The height is determined by 1/(sigma*sqrt(2pi)), but the primary 'shape' role of sigma is spread.",
          "2": "Correct. Sigma measures dispersion.",
          "3": "The area under a probability distribution must always be 1, not 0."
        }
      },
      {
        "id": 43,
        "topic": "Hypothesis Testing",
        "difficulty": "analysis",
        "question": "If you reject a null hypothesis at alpha=0.05, will you also necessarily reject it at alpha=0.01?",
        "options": [
          "Yes, because 0.01 is more strict.",
          "No, because the p-value might be between 0.01 and 0.05.",
          "Yes, because the p-value doesn't change.",
          "It depends on whether the test was one-sided or two-sided."
        ],
        "correct_index": 1,
        "rationale": "Rejecting at 0.05 means p < 0.05. However, if the p-value is 0.03, it is smaller than 0.05 (reject) but larger than 0.01 (retain).",
        "distractor_analysis": {
          "0": "Being more strict means it's *harder* to reject; therefore, you might not.",
          "1": "Correct. A result can be 'significant' at one level but not a stricter one.",
          "2": "The p-value doesn't change, but the conclusion depends on the alpha it is compared against.",
          "3": "While test type affects p-value, the logic of alpha thresholds applies the same way to any p-value."
        }
      },
      {
        "id": 44,
        "topic": "Chi-Square",
        "difficulty": "application",
        "question": "You want to know if there is a relationship between 'Gender' and 'Preference for Coffee vs Tea'. Which test from the Appendix should you use?",
        "options": [
          "Wald Test",
          "t-Test for unknown variance",
          "Chi-Square independence test",
          "Empirical Skewness test"
        ],
        "correct_index": 2,
        "rationale": "Gender and Coffee/Tea preference are categorical variables. Slide 37 shows the 'Chi-Square independence test' is used for testing relationships between features in a contingency table.",
        "distractor_analysis": {
          "0": "Wald test is generally for comparing a parameter to a fixed value.",
          "1": "t-tests are for numerical means, not categorical associations.",
          "2": "Correct. This is the standard test for 'association' between categories.",
          "3": "Skewness is for the shape of a single numerical distribution."
        }
      },
      {
        "id": 45,
        "topic": "Estimators",
        "difficulty": "recall",
        "question": "What is 'Maximum Likelihood Estimation' (MLE) primarily trying to do?",
        "options": [
          "Find the parameter values that make the observed data most probable.",
          "Minimize the variance of the sample.",
          "Ensure the data follows a Normal distribution.",
          "Calculate the median of a large dataset."
        ],
        "correct_index": 0,
        "rationale": "MLE (Slide 17) is an estimator that 'maximizes the likelihood' (probability) of the observed sample given the parameters.",
        "distractor_analysis": {
          "0": "Correct. It is a fundamental method for parameter estimation.",
          "1": "MLE doesn't minimize sample variance; it finds parameters for a distribution.",
          "2": "MLE can be used for any distribution (Poisson, Binomial, etc.), not just Normal.",
          "3": "MLE is a general technique, whereas median is a specific estimator."
        }
      },
      {
        "id": 46,
        "topic": "p-value",
        "difficulty": "analysis",
        "question": "A researcher states, 'My p-value is 0.04, so there is a 4% chance the null hypothesis is true.' What is wrong with this statement?",
        "options": [
          "The p-value should be multiplied by 100.",
          "The p-value measures the probability of the data given the null, not the probability of the null itself.",
          "A p-value of 0.04 is not significant enough to make such a claim.",
          "The null hypothesis is never a percentage."
        ],
        "correct_index": 1,
        "rationale": "This is a major misconception in statistics. The p-value is P(Data | H0), not P(H0 | Data). We cannot state the probability that the hypothesis is true using frequentist p-values.",
        "distractor_analysis": {
          "0": "0.04 is 4%, so the math isn't the issue; the logic is.",
          "1": "Correct. It's a conditional probability of observing such data if the null were true.",
          "2": "It is significant (usually), but the interpretation of 'probability of H0' is always wrong regardless of the number.",
          "3": "Hypotheses aren't percentages, but probabilities are; the error is in the 'direction' of the logic."
        }
      },
      {
        "id": 47,
        "topic": "Normal Distribution",
        "difficulty": "application",
        "question": "You have a normal distribution N(100, 16). What is the mean and the standard deviation?",
        "options": [
          "Mean = 100, SD = 16",
          "Mean = 10, SD = 4",
          "Mean = 100, SD = 4",
          "Mean = 16, SD = 100"
        ],
        "correct_index": 2,
        "rationale": "The notation N(mu, sigma^2) is standard (Slide 18). Here, mu = 100 and sigma^2 = 16, so sigma = sqrt(16) = 4.",
        "distractor_analysis": {
          "0": "This mistakes the variance for the standard deviation.",
          "1": "This takes the square root of the mean, which is incorrect.",
          "2": "Correct. 100 is the mean, and the square root of 16 is the SD.",
          "3": "This reverses the parameters."
        }
      },
      {
        "id": 48,
        "topic": "Estimators",
        "difficulty": "analysis",
        "question": "Which property of the 'Empirical Distribution Function' (EDF) is highlighted on Slide 12?",
        "options": [
          "It is only useful for small datasets.",
          "It is an unbiased and consistent estimator of the true Cumulative Distribution Function (CDF).",
          "It always follows a bell curve.",
          "It can only be used if we know the true mean."
        ],
        "correct_index": 1,
        "rationale": "Slide 12 explicitly lists the empirical distribution function as an 'unbiased consistent estimator of the true cumulative distribution F_x'.",
        "distractor_analysis": {
          "0": "EDF actually becomes more powerful and accurate as n increases.",
          "1": "Correct. It 'mimics' the true CDF perfectly in the limit.",
          "2": "The EDF's shape depends on the data (it's a step function); it doesn't force a bell curve.",
          "3": "EDF is calculated directly from the sample data points without needing parameters."
        }
      },
      {
        "id": 49,
        "topic": "Hypothesis Testing",
        "difficulty": "recall",
        "question": "In the Wald Test (Slide 30), what is 'W' approximately distributed as for large samples?",
        "options": [
          "A t-distribution with n-1 degrees of freedom.",
          "A Normal distribution N(0,1).",
          "A Chi-Square distribution.",
          "A Uniform distribution."
        ],
        "correct_index": 1,
        "rationale": "Slide 30 states: 'The test variable W := ... is approximately N(0,1)-distributed'.",
        "distractor_analysis": {
          "0": "This is for the t-test when variance is unknown and n is small.",
          "1": "Correct. Wald tests typically use the Z-table for critical values.",
          "2": "Chi-Square is for different types of tests (variance or categorical).",
          "3": "Statistical test variables are designed to follow specific shapes like Normal or t, never Uniform."
        }
      },
      {
        "id": 50,
        "topic": "Comparative Analysis",
        "difficulty": "analysis",
        "question": "Comparing the LLN and CLT: Which one explains *where* the average goes, and which one explains the *shape* of the average's distribution?",
        "options": [
          "LLN explains the shape; CLT explains the location.",
          "CLT explains the location; LLN explains the shape.",
          "LLN explains the location (convergence to the mean); CLT explains the shape (Normal distribution).",
          "Both explain the same thing; they are synonyms."
        ],
        "correct_index": 2,
        "rationale": "The Law of Large Numbers dictates that the sample mean converges to a specific point (the population mean). The Central Limit Theorem describes the fluctuating 'shape' of that mean's distribution around that point as it converges.",
        "distractor_analysis": {
          "0": "This is the opposite of their roles.",
          "1": "This is also swapped.",
          "2": "Correct. Location/Convergence = LLN. Shape/Distribution = CLT.",
          "3": "They are distinct mathematical theorems with different (though related) conclusions."
        }
      }
    ]
  },
  "foundations-03": {
    "title": "Foundations of AI - Information Theory",
    "questions": [
      {
        "id": 1,
        "topic": "Information Content",
        "difficulty": "recall",
        "question": "In information theory, how is the relationship between the probability of an event and its information content (surprise) best described?",
        "options": [
          "Information content increases as the probability of the event decreases.",
          "Information content is independent of the probability of the event.",
          "Information content increases linearly with the probability of the event.",
          "Information content only exists for events with a probability of 0.5."
        ],
        "correct_index": 0,
        "rationale": "Information content, or self-information, is defined as h(x) = -log2 P(x). This inverse relationship means rare (low probability) events carry more surprise and thus more information than common (high probability) events.",
        "distractor_analysis": {
          "0": "Correct. Low probability equals high surprise.",
          "1": "Incorrect; information is a direct function of probability in this framework.",
          "2": "Incorrect; the relationship is logarithmic and inverse, not linear.",
          "3": "Incorrect; information content is defined for all probabilities in the range (0, 1]."
        }
      },
      {
        "id": 2,
        "topic": "Entropy",
        "difficulty": "application",
        "question": "If a discrete random variable has only one possible state that occurs with probability 1.0, what is the entropy of this variable?",
        "options": [
          "Exactly 0 bits",
          "Exactly 1 bit",
          "Positive infinity",
          "It cannot be calculated because log(0) is undefined."
        ],
        "correct_index": 0,
        "rationale": "Entropy measures uncertainty. If an outcome is certain (probability = 1), there is no uncertainty and no 'average surprise,' resulting in an entropy of 0.",
        "distractor_analysis": {
          "0": "Correct. Certainty means zero entropy.",
          "1": "Incorrect; 1 bit implies two equally likely states (like a fair coin flip).",
          "2": "Incorrect; infinity would imply infinite uncertainty, the opposite of certainty.",
          "3": "Incorrect; while log(0) is a limit, P*log(P) for P=0 is conventionally 0, and log(1) is 0."
        }
      },
      {
        "id": 3,
        "topic": "Entropy",
        "difficulty": "analysis",
        "question": "A system is moved from a state of uniform probability across all outcomes to a state where one outcome is highly likely and others are rare. What is the impact on the system's entropy?",
        "options": [
          "The entropy decreases because the system becomes more predictable.",
          "The entropy increases because the system becomes more ordered.",
          "The entropy remains constant because the number of possible states has not changed.",
          "The entropy increases because the surprise of the rare events is higher."
        ],
        "correct_index": 0,
        "rationale": "Entropy is maximized when all outcomes are equally likely (maximum uncertainty). Moving away from a uniform distribution toward a skewed one increases predictability and thus decreases entropy.",
        "distractor_analysis": {
          "0": "Correct. Predictability reduces the average surprise (entropy).",
          "1": "Incorrect; increasing order corresponds to a decrease in entropy.",
          "2": "Incorrect; entropy depends on the probability distribution, not just the count of possible states.",
          "3": "Incorrect; while rare events have high individual surprise, their low probability means they contribute less to the 'average' surprise (entropy)."
        }
      },
      {
        "id": 4,
        "topic": "Minimum Description Length (MDL)",
        "difficulty": "application",
        "question": "When applying the MDL principle to select between two models, which model should be chosen?",
        "options": [
          "The model that minimizes the sum of the model's description length and the data's description length given the model.",
          "The model with the highest number of parameters to ensure the lowest data loss.",
          "The model that ignores the data and focuses only on having the shortest code length for the model itself.",
          "The model that perfectly fits the training data regardless of its own complexity."
        ],
        "correct_index": 0,
        "rationale": "The MDL principle is a formalization of Occam's Razor, balancing the complexity of the model (L(M)) against its ability to compress the data (L(D|M)).",
        "distractor_analysis": {
          "0": "Correct. This is the core definition of the MDL principle.",
          "1": "Incorrect; high parameters increase model complexity, which MDL penalizes to prevent overfitting.",
          "2": "Incorrect; a model that ignores data is useless, as L(D|M) would be very high.",
          "3": "Incorrect; perfect fitting often leads to over-complex models (overfitting), which MDL avoids."
        }
      },
      {
        "id": 5,
        "topic": "Kolmogorov Complexity",
        "difficulty": "analysis",
        "question": "Two strings of the same length are compared: String A is '010101...01' and String B is a sequence of random coin flips. Which has higher Kolmogorov complexity?",
        "options": [
          "String B, because it cannot be significantly compressed by a shorter program.",
          "String A, because the alternating pattern requires complex logic to generate.",
          "Both have the same complexity because they have the same length.",
          "String A, because it contains more information content for a human observer."
        ],
        "correct_index": 0,
        "rationale": "Kolmogorov complexity is the size of the shortest program that generates a string. String A can be generated by a very short loop (print '01' n times), while String B has no pattern and requires a program almost as long as the string itself.",
        "distractor_analysis": {
          "0": "Correct. Randomness implies lack of compressible patterns.",
          "1": "Incorrect; simple patterns have low Kolmogorov complexity.",
          "2": "Incorrect; length is not the determinant; the structure/compressibility is.",
          "3": "Incorrect; 'information content' in this context is lower for patterns that are highly predictable."
        }
      },
      {
        "id": 6,
        "topic": "Kullback-Leibler (KL) Divergence",
        "difficulty": "recall",
        "question": "Why is the Kullback-Leibler (KL) Divergence referred to as an asymmetric measure?",
        "options": [
          "Because the divergence from P to Q is not necessarily equal to the divergence from Q to P.",
          "Because it can return negative values depending on which distribution is the reference.",
          "Because it only works for discrete distributions and not continuous ones.",
          "Because it measures the difference in mean but not the difference in variance."
        ],
        "correct_index": 0,
        "rationale": "KL(P||Q) measures the extra bits required to encode data from distribution P using a code optimized for Q. Swapping P and Q changes the reference, resulting in a different value; hence, it is not a true 'distance' metric.",
        "distractor_analysis": {
          "0": "Correct. This is the definition of asymmetry in this context.",
          "1": "Incorrect; according to Gibbs' inequality, KL divergence is always non-negative.",
          "2": "Incorrect; KL divergence is defined for both discrete and continuous variables.",
          "3": "Incorrect; KL divergence takes the entire distribution into account, not just specific moments."
        }
      },
      {
        "id": 7,
        "topic": "Mutual Information",
        "difficulty": "analysis",
        "question": "If the mutual information I(X; Y) between two random variables is 0, what can be definitively concluded about X and Y?",
        "options": [
          "X and Y are independent.",
          "X and Y have the same entropy.",
          "X and Y are perfectly correlated.",
          "Y provides some information about X, but X provides no information about Y."
        ],
        "correct_index": 0,
        "rationale": "Mutual information measures how much knowing one variable reduces the uncertainty about the other. If I(X; Y) = 0, no reduction in uncertainty occurs, which is the definition of independence.",
        "distractor_analysis": {
          "0": "Correct. Zero mutual information is synonymous with statistical independence.",
          "1": "Incorrect; variables can be independent even if they have different entropies.",
          "2": "Incorrect; perfect correlation would result in the maximum possible mutual information.",
          "3": "Incorrect; mutual information is symmetric: I(X; Y) = I(Y; X)."
        }
      },
      {
        "id": 8,
        "topic": "Huffman Compression",
        "difficulty": "recall",
        "question": "What is the significance of the 'prefix property' in Huffman coding?",
        "options": [
          "No codeword is a prefix of another, allowing for unique decoding without delimiters.",
          "All codewords must begin with the same bit to signal the start of a sequence.",
          "The most frequent symbol always has the longest prefix to ensure data integrity.",
          "It ensures that the compression is lossy so that more space can be saved."
        ],
        "correct_index": 0,
        "rationale": "The prefix property (or prefix-free property) ensures that as soon as a bit sequence matches a codeword, it can be decoded immediately because no longer valid codeword starts with that same sequence.",
        "distractor_analysis": {
          "0": "Correct. This enables 'instantaneous' and unique decoding.",
          "1": "Incorrect; this would make codewords indistinguishable and waste bits.",
          "2": "Incorrect; the most frequent symbol receives the *shortest* codeword.",
          "3": "Incorrect; Huffman coding is a lossless compression technique."
        }
      },
      {
        "id": 9,
        "topic": "Noiseless Coding Theorem",
        "difficulty": "analysis",
        "question": "According to Shannon's Noiseless Coding Theorem, what is the absolute lower bound on the average number of bits needed to represent a symbol from a source X?",
        "options": [
          "The entropy H(X) of the source.",
          "The log of the number of possible states in X.",
          "The reciprocal of the probability of the most frequent symbol.",
          "The length of the shortest codeword in a Huffman tree."
        ],
        "correct_index": 0,
        "rationale": "Shannon proved that no lossless compression can represent a source with fewer average bits than its entropy H(X).",
        "distractor_analysis": {
          "0": "Correct. Entropy is the fundamental limit of compression.",
          "1": "Incorrect; this is the length for a fixed-length code, which is only optimal if all states are equally likely.",
          "2": "Incorrect; this is related to individual surprise, not the average for the whole source.",
          "3": "Incorrect; the shortest codeword is just one part of the average; the entropy is the overall bound."
        }
      },
      {
        "id": 10,
        "topic": "Entropy",
        "difficulty": "application",
        "question": "In a binary system where P(X=1) = p, at what value of p is the entropy H(X) maximized?",
        "options": [
          "p = 0.5",
          "p = 0.0",
          "p = 1.0",
          "The entropy is constant regardless of p as long as the system is binary."
        ],
        "correct_index": 0,
        "rationale": "For a binary variable, uncertainty is highest when both outcomes are equally likely (a fair coin). This occurs at p = 0.5, where H(X) = 1 bit.",
        "distractor_analysis": {
          "0": "Correct. This is the peak of the binary entropy function.",
          "1": "Incorrect; at p=0, the outcome is certain (X=0), so entropy is 0.",
          "2": "Incorrect; at p=1, the outcome is certain (X=1), so entropy is 0.",
          "3": "Incorrect; entropy varies significantly with the probability distribution p."
        }
      },
      {
        "id": 11,
        "topic": "Information Content",
        "difficulty": "analysis",
        "question": "If two events A and B are independent, how does the information content of both events occurring simultaneously h(A ∩ B) relate to their individual information contents?",
        "options": [
          "h(A ∩ B) = h(A) + h(B)",
          "h(A ∩ B) = h(A) * h(B)",
          "h(A ∩ B) = max(h(A), h(B))",
          "h(A ∩ B) = log(h(A) + h(B))"
        ],
        "correct_index": 0,
        "rationale": "Information content is additive for independent events. Because P(A ∩ B) = P(A) * P(B) for independent events, taking the negative log converts the product into a sum.",
        "distractor_analysis": {
          "0": "Correct. This is the additivity property of information.",
          "1": "Incorrect; probabilities multiply, but information contents (logs) add.",
          "2": "Incorrect; information is not limited to the maximum of the parts.",
          "3": "Incorrect; the log is already part of the definition of h(x)."
        }
      },
      {
        "id": 12,
        "topic": "Kolmogorov Complexity",
        "difficulty": "application",
        "question": "Why is the Minimum Description Length (MDL) considered equivalent to Kolmogorov Complexity in a practical sense?",
        "options": [
          "Both seek the shortest possible representation of data to find the underlying regularities.",
          "Both require infinite computing power to calculate for any given string.",
          "Both are only applicable to binary strings and not natural language.",
          "Neither considers the complexity of the model, only the size of the data."
        ],
        "correct_index": 0,
        "rationale": "MDL and Kolmogorov complexity both view 'understanding' or 'learning' as 'compressing.' The best explanation for data is the one that allows for the shortest description of that data.",
        "distractor_analysis": {
          "0": "Correct. They share the same underlying philosophical goal.",
          "1": "Incorrect; while Kolmogorov complexity is non-computable, MDL is a practical framework used in statistics.",
          "2": "Incorrect; they are general concepts applicable to any data type.",
          "3": "Incorrect; MDL explicitly balances model complexity and data fit; Kolmogorov includes the program (model) size."
        }
      },
      {
        "id": 13,
        "topic": "Mutual Information",
        "difficulty": "analysis",
        "question": "How does mutual information I(X; Y) relate to the entropies H(X) and H(X|Y)?",
        "options": [
          "I(X; Y) = H(X) - H(X|Y)",
          "I(X; Y) = H(X) + H(X|Y)",
          "I(X; Y) = H(X|Y) - H(X)",
          "I(X; Y) = H(X) / H(X|Y)"
        ],
        "correct_index": 0,
        "rationale": "Mutual information is the reduction in the uncertainty of X given the knowledge of Y. This is calculated as the original entropy of X minus the remaining (conditional) entropy of X after Y is known.",
        "distractor_analysis": {
          "0": "Correct. Reduction in uncertainty is the core definition.",
          "1": "Incorrect; this would imply that knowing Y increases the uncertainty of X.",
          "2": "Incorrect; this would result in a negative value, but mutual information is non-negative.",
          "3": "Incorrect; the relationship is based on subtraction (difference in bits), not division."
        }
      },
      {
        "id": 14,
        "topic": "Kullback-Leibler (KL) Divergence",
        "difficulty": "application",
        "question": "If you use a probability distribution Q to design a code for a source that actually follows distribution P, what does KL(P||Q) represent?",
        "options": [
          "The average number of extra bits wasted per symbol due to the mismatch.",
          "The total number of bits required to send the entire message.",
          "The probability that the receiver will misinterpret the message.",
          "The minimum possible code length achievable for distribution P."
        ],
        "correct_index": 0,
        "rationale": "KL divergence measures 'relative entropy.' It quantifies the efficiency loss (in bits) when an imperfect model (Q) is used to approximate the true distribution (P).",
        "distractor_analysis": {
          "0": "Correct. It is the overhead or 'waste' due to using the wrong distribution.",
          "1": "Incorrect; the total bits would be H(P) + KL(P||Q).",
          "2": "Incorrect; KL divergence is a measure of coding efficiency, not error probability (in a noiseless context).",
          "3": "Incorrect; that is simply the entropy H(P)."
        }
      },
      {
        "id": 15,
        "topic": "Huffman Compression",
        "difficulty": "analysis",
        "question": "In a scenario where one symbol has a probability of 0.9 and three other symbols have probabilities of 0.033 each, what behavior would you expect from a Huffman code?",
        "options": [
          "The 0.9 probability symbol will be assigned a 1-bit code.",
          "All symbols will be assigned 2-bit codes because there are four possible symbols.",
          "The symbol with 0.9 probability will have a code longer than the 0.033 symbols.",
          "The code will be less efficient than a simple fixed-length code."
        ],
        "correct_index": 0,
        "rationale": "Huffman coding assigns shorter codes to more frequent symbols. With one symbol dominant at 0.9, it will inevitably be placed at the highest level of the tree, receiving a 1-bit code.",
        "distractor_analysis": {
          "0": "Correct. High frequency equals short code length.",
          "1": "Incorrect; that describes a fixed-length code, which is not what Huffman produces here.",
          "2": "Incorrect; this contradicts the core logic of Huffman coding.",
          "3": "Incorrect; Huffman is always at least as efficient as fixed-length coding for any non-uniform distribution."
        }
      },
      {
        "id": 16,
        "topic": "Entropy",
        "difficulty": "analysis",
        "question": "If we define a continuous random variable X, which distribution maximizes the entropy H[X] for a given variance?",
        "options": [
          "The Normal (Gaussian) distribution.",
          "The Uniform distribution.",
          "The Bernoulli distribution.",
          "The Zipfian distribution."
        ],
        "correct_index": 0,
        "rationale": "As noted in slide 12, for a continuous random variable, entropy is maximized when the distribution is Normal (Gaussian) for a fixed mean and variance.",
        "distractor_analysis": {
          "0": "Correct. This is a fundamental property of the Gaussian distribution in information theory.",
          "1": "Incorrect; Uniform maximizes entropy for a discrete variable with a fixed range, but not for a continuous variable with fixed variance.",
          "2": "Incorrect; Bernoulli is a discrete distribution.",
          "3": "Incorrect; Zipfian is a discrete power-law distribution typical of natural language, not maximum entropy."
        }
      },
      {
        "id": 17,
        "topic": "Minimum Description Length (MDL)",
        "difficulty": "analysis",
        "question": "In the context of MDL, how does 'Occam's Razor' justify the preference for simpler models?",
        "options": [
          "Simpler models have shorter description lengths, reducing the total cost unless the complex model fits significantly better.",
          "Simpler models are always more accurate in predicting unseen data, regardless of the training set.",
          "Occam's Razor states that complexity is an inherent sign of data noise.",
          "MDL prefers simpler models only when the amount of available data is infinite."
        ],
        "correct_index": 0,
        "rationale": "MDL treats model selection as a trade-off. A simpler model is easier to describe (short L(M)). We only accept a more complex model (longer L(M)) if it provides a much better fit (shorter L(D|M)).",
        "distractor_analysis": {
          "0": "Correct. The 'cost' of the model is its description length.",
          "1": "Incorrect; simplicity does not guarantee accuracy, but it protects against overfitting.",
          "2": "Incorrect; Occam's Razor is a heuristic preference for simplicity, not a statement that all complexity is noise.",
          "3": "Incorrect; MDL is especially useful for finite, limited data sets to prevent overfitting."
        }
      },
      {
        "id": 18,
        "topic": "Noisy-Channel Coding Theorem",
        "difficulty": "application",
        "question": "What happens if the source information rate R exceeds the channel capacity C?",
        "options": [
          "Reliable, error-free transmission becomes impossible.",
          "The transmission will simply take longer to complete, but remain error-free.",
          "The encoder will automatically reduce the entropy of the source to match C.",
          "The channel capacity C will increase to accommodate the higher rate R."
        ],
        "correct_index": 0,
        "rationale": "Shannon's Noisy-Channel Coding Theorem states that error-free transmission is only possible if the rate R is less than or equal to the capacity C. If R > C, errors are inevitable.",
        "distractor_analysis": {
          "0": "Correct. Capacity is a hard physical/mathematical limit for reliable communication.",
          "1": "Incorrect; errors cannot be eliminated by simply waiting longer if the rate exceeds capacity.",
          "2": "Incorrect; the encoder cannot change the source's entropy without losing information (lossy compression).",
          "3": "Incorrect; capacity is an inherent fixed property of the channel and its noise levels."
        }
      },
      {
        "id": 19,
        "topic": "Information Content",
        "difficulty": "application",
        "question": "A weather station predicts 'Sunny' with 95% probability and 'Earthquake' with 0.01% probability. Which report carries more information content if it actually occurs?",
        "options": [
          "The 'Earthquake' report, because it is much more surprising.",
          "The 'Sunny' report, because it is more likely to be correct.",
          "Both carry the same information because they come from the same source.",
          "Neither carries information until the event is verified by a second source."
        ],
        "correct_index": 0,
        "rationale": "Information content is h(x) = -log2 P(x). Since 0.0001 is much smaller than 0.95, its negative log is much higher, reflecting higher surprise/information.",
        "distractor_analysis": {
          "0": "Correct. Rarity equals high information.",
          "1": "Incorrect; likelihood reduces information content (surprise).",
          "2": "Incorrect; information depends on the probability of the specific event, not the source identity.",
          "3": "Incorrect; information content is a property of the probability of the message itself."
        }
      },
      {
        "id": 20,
        "topic": "Entropy",
        "difficulty": "analysis",
        "question": "If you are told that the joint entropy H(X, Y) is exactly equal to H(X) + H(Y), what is the value of the mutual information I(X; Y)?",
        "options": [
          "0 bits",
          "1 bit",
          "It depends on whether X and Y are continuous or discrete.",
          "It is equal to the conditional entropy H(X|Y)."
        ],
        "correct_index": 0,
        "rationale": "The property H(X, Y) = H(X) + H(Y) is the definition of additivity for independent variables. As established earlier, independent variables share 0 mutual information.",
        "distractor_analysis": {
          "0": "Correct. No overlap in information means mutual information is zero.",
          "1": "Incorrect; there is no reason for it to be 1.",
          "2": "Incorrect; this identity holds for both types if they are independent.",
          "3": "Incorrect; if they are independent, H(X|Y) = H(X), which is usually not zero."
        }
      },
      {
        "id": 21,
        "topic": "Kullback-Leibler (KL) Divergence",
        "difficulty": "analysis",
        "question": "What is the result of KL(P||Q) if there exists a state 'x' such that P(x) > 0 but Q(x) = 0?",
        "options": [
          "The divergence becomes infinite.",
          "The divergence becomes 0 because the state is ignored.",
          "The divergence is calculated only for the states where both are non-zero.",
          "The divergence becomes negative, indicating an impossible model."
        ],
        "correct_index": 0,
        "rationale": "KL divergence involves the term P(x) * log(P(x)/Q(x)). If Q(x) is 0 while P(x) is non-zero, the ratio involves division by zero, leading to infinite divergence. This means model Q is 'infinitely' bad at representing P.",
        "distractor_analysis": {
          "0": "Correct. You cannot use a model that predicts an event is impossible (Q=0) if that event actually happens (P>0).",
          "1": "Incorrect; the mismatch is catastrophic and cannot be ignored.",
          "2": "Incorrect; the definition requires the summation over all states in the domain of P.",
          "3": "Incorrect; KL divergence can never be negative."
        }
      },
      {
        "id": 22,
        "topic": "Kolmogorov Complexity",
        "difficulty": "application",
        "question": "A data scientist finds that a 1GB file can be compressed down to 1KB. What does this suggest about the file's Kolmogorov complexity?",
        "options": [
          "It has very low Kolmogorov complexity because it can be generated by a very small program.",
          "It has very high Kolmogorov complexity because it contains a lot of data.",
          "The Kolmogorov complexity is exactly 1KB.",
          "The Kolmogorov complexity cannot be determined because compression is lossy."
        ],
        "correct_index": 0,
        "rationale": "Kolmogorov complexity is essentially the limit of lossless compression. If a massive file can be represented by a tiny one, the 'true' amount of information (complexity) is small.",
        "distractor_analysis": {
          "0": "Correct. High compressibility implies low algorithmic entropy.",
          "1": "Incorrect; file size (data volume) is not the same as complexity (information).",
          "2": "Incorrect; 1KB is an upper bound on the complexity, but the true complexity might be even smaller.",
          "3": "Incorrect; the scenario implies we are talking about the information content, and Huffman/LZW are lossless."
        }
      },
      {
        "id": 23,
        "topic": "Huffman Compression",
        "difficulty": "analysis",
        "question": "Which of the following scenarios would render a Huffman code no more efficient than a fixed-length code?",
        "options": [
          "All possible symbols in the source have exactly the same probability.",
          "The number of symbols is a power of 2.",
          "The source symbols are highly correlated with each other.",
          "The entropy of the source is very high."
        ],
        "correct_index": 0,
        "rationale": "Huffman coding gains efficiency by exploiting non-uniform probabilities. If every symbol is equally likely (uniform distribution), every symbol will end up at the same depth in the tree, behaving like a fixed-length code.",
        "distractor_analysis": {
          "0": "Correct. Uniformity removes the advantage of variable-length coding.",
          "1": "Incorrect; even if the count is a power of 2, if probabilities are skewed, Huffman is better.",
          "2": "Incorrect; correlation often allows for even better compression (like LZW), but it doesn't make Huffman worse.",
          "3": "Incorrect; High entropy means you need more bits, but if it's non-uniform, Huffman still helps."
        }
      },
      {
        "id": 24,
        "topic": "Entropy",
        "difficulty": "analysis",
        "question": "How does the 'Conditional Entropy' H(Y|X) differ conceptually from the 'Joint Entropy' H(X, Y)?",
        "options": [
          "H(Y|X) is the uncertainty remaining in Y after X is known, while H(X, Y) is the total uncertainty of the pair.",
          "H(Y|X) is always greater than or equal to H(X, Y).",
          "H(Y|X) measures the information shared between X and Y, while H(X, Y) measures the information they don't share.",
          "H(Y|X) only applies to independent variables, while H(X, Y) applies to dependent ones."
        ],
        "correct_index": 0,
        "rationale": "H(X, Y) is the 'total' information in the system. H(Y|X) is a 'subset' of that—specifically the information in Y that is not already explained by X.",
        "distractor_analysis": {
          "0": "Correct. This captures the logic of 'remaining uncertainty.'",
          "1": "Incorrect; H(X, Y) = H(X) + H(Y|X), so H(X, Y) is always greater than or equal to H(Y|X).",
          "2": "Incorrect; the shared information is Mutual Information I(X; Y).",
          "3": "Incorrect; both concepts apply to any set of random variables."
        }
      },
      {
        "id": 25,
        "topic": "Minimum Description Length (MDL)",
        "difficulty": "analysis",
        "question": "Why does MDL tend to naturally favor models that generalize well to new data?",
        "options": [
          "Because models that overfit are overly complex, resulting in a high model description length L(M) that MDL penalizes.",
          "Because MDL requires that all models be tested on a separate validation set.",
          "Because MDL assumes the data is noiseless, forcing models to find only true patterns.",
          "Because MDL only considers the error rate on the training data."
        ],
        "correct_index": 0,
        "rationale": "Overfitting usually involves adding many parameters to 'memorize' noise. In MDL, these parameters increase L(M). MDL only allows this if the reduction in L(D|M) is large enough to compensate, which noise-fitting usually doesn't provide.",
        "distractor_analysis": {
          "0": "Correct. The complexity penalty in L(M) acts as a regularizer.",
          "1": "Incorrect; MDL is a criterion applied to the training set/model itself, not a validation procedure.",
          "2": "Incorrect; MDL explicitly accounts for the possibility of noise by looking for the most compact representation.",
          "3": "Incorrect; if it only considered training error, it would always pick the most complex model."
        }
      },
      {
        "id": 26,
        "topic": "Information Content",
        "difficulty": "recall",
        "question": "What is the fundamental unit of information assumed in the slide deck?",
        "options": [
          "The bit",
          "The byte",
          "The nats",
          "The symbol"
        ],
        "correct_index": 0,
        "rationale": "Slide 4 explicitly states: 'The bit is the most fundamental unit of information.'",
        "distractor_analysis": {
          "0": "Correct. Bits are the standard based on base-2 logarithms used in the slides.",
          "1": "Incorrect; a byte is a collection of 8 bits, not the 'fundamental' unit.",
          "2": "Incorrect; nats use natural logarithms (base e), which were not the primary focus.",
          "3": "Incorrect; a symbol is what carries information, not the unit of measure for the information itself."
        }
      },
      {
        "id": 27,
        "topic": "Noisy Channels",
        "difficulty": "application",
        "question": "In the repetition code example (Slide 24), why is the bit '0' sent as '000' instead of just '0'?",
        "options": [
          "To introduce systematic redundancy so that errors can be detected and corrected.",
          "To increase the information content of the message.",
          "Because the channel requires all messages to be exactly 3 bits long.",
          "To increase the transmission rate R above the channel capacity C."
        ],
        "correct_index": 0,
        "rationale": "Redundancy is the primary tool for dealing with noise. By sending multiple copies, the receiver can use a 'majority vote' to reconstruct the original signal even if one bit is flipped.",
        "distractor_analysis": {
          "0": "Correct. Redundancy allows for error resilience.",
          "1": "Incorrect; redundancy actually *decreases* the average information per transmitted bit.",
          "2": "Incorrect; repetition is a choice for reliability, not a requirement of bit length.",
          "3": "Incorrect; increasing redundancy *decreases* the rate R."
        }
      },
      {
        "id": 28,
        "topic": "Mutual Information",
        "difficulty": "analysis",
        "question": "Which of the following best describes the 'symmetry' of mutual information?",
        "options": [
          "I(X; Y) is always equal to I(Y; X).",
          "The mutual information is equal to the sum of the individual entropies.",
          "Mutual information is only symmetric if X and Y have the same number of states.",
          "The symmetry implies that the joint entropy H(X, Y) is zero."
        ],
        "correct_index": 0,
        "rationale": "Mutual information measures the information shared between two variables. The amount of information X tells you about Y is identically the amount of information Y tells you about X.",
        "distractor_analysis": {
          "0": "Correct. Mutual information is a symmetric measure.",
          "1": "Incorrect; that would be true only if they were independent and the result was joint entropy (not mutual info).",
          "2": "Incorrect; symmetry is a property of the functional form, independent of domain size.",
          "3": "Incorrect; joint entropy is only zero if both variables are deterministic constants."
        }
      },
      {
        "id": 29,
        "topic": "Zipf's Law",
        "difficulty": "recall",
        "question": "According to Zipf's Law, how is the frequency of a word related to its rank in a frequency table?",
        "options": [
          "The frequency is inversely proportional to the rank.",
          "The frequency increases as the rank increases.",
          "The frequency is the square of the rank.",
          "There is no predictable relationship between frequency and rank."
        ],
        "correct_index": 0,
        "rationale": "Zipf's Law states f(t) ≈ C / r(t), where f is frequency and r is rank. This means the most frequent word (rank 1) occurs twice as often as the second most frequent (rank 2), and so on.",
        "distractor_analysis": {
          "0": "Correct. This is the definition of Zipf's power law.",
          "1": "Incorrect; higher rank (e.g., the 100th word) means lower frequency.",
          "2": "Incorrect; the relationship is inverse, not a square.",
          "3": "Incorrect; Zipf's Law is a well-documented empirical observation in linguistics."
        }
      },
      {
        "id": 30,
        "topic": "Heap's Law",
        "difficulty": "analysis",
        "question": "What is the primary implication of Heap's Law for a data scientist working with massive text corpora?",
        "options": [
          "The vocabulary size grows sublinearly, meaning you will continue to encounter new words, but at a decreasing rate.",
          "The vocabulary size is fixed once the document reaches 100,000 words.",
          "Every new document added to a corpus will contain exactly the same number of unique words.",
          "The number of unique words is always half of the total number of words."
        ],
        "correct_index": 0,
        "rationale": "Heap's Law (V = Kn^β with β < 1) indicates that the vocabulary (V) grows as document size (n) increases, but the rate of discovery of new words slows down over time.",
        "distractor_analysis": {
          "0": "Correct. Sublinear growth is the key characteristic.",
          "1": "Incorrect; the vocabulary continues to grow indefinitely, just more slowly.",
          "2": "Incorrect; word counts vary by document and topic.",
          "3": "Incorrect; the ratio of unique words to total words changes as the corpus grows."
        }
      },
      {
        "id": 31,
        "topic": "TF-IDF",
        "difficulty": "analysis",
        "question": "In the TF-IDF weighting scheme, what is the purpose of the 'Inverse Document Frequency' (IDF) component?",
        "options": [
          "To down-weight common words (like 'the' or 'is') that appear in many documents and thus carry little discriminative information.",
          "To ensure that longer documents are given higher priority in search results.",
          "To count how many times a specific term appears within a single document.",
          "To normalize the word counts so they sum to exactly 1.0."
        ],
        "correct_index": 0,
        "rationale": "IDF measures how unique a word is across the whole corpus. If a word appears everywhere, it's likely a 'stop word' and not useful for identifying the specific topic of a single document.",
        "distractor_analysis": {
          "0": "Correct. IDF captures the 'representativeness' or 'specificity' of a term.",
          "1": "Incorrect; TF-IDF often uses normalization to *remove* the influence of document length.",
          "2": "Incorrect; that is the 'Term Frequency' (TF) component.",
          "3": "Incorrect; TF-IDF weights are not probabilities and do not need to sum to 1."
        }
      },
      {
        "id": 32,
        "topic": "Entropy",
        "difficulty": "application",
        "question": "If you have an 8-state system where all states are equally likely, how many bits on average are needed to encode a state?",
        "options": [
          "3 bits",
          "8 bits",
          "1 bit",
          "log(8) / log(10) bits"
        ],
        "correct_index": 0,
        "rationale": "As shown in the example on slide 10, for 8 equally likely states, H(X) = log2(8) = 3 bits.",
        "distractor_analysis": {
          "0": "Correct. 2^3 = 8.",
          "1": "Incorrect; 8 bits would be needed for 256 states.",
          "2": "Incorrect; 1 bit only covers 2 states.",
          "3": "Incorrect; this is a change-of-base formula that doesn't result in bits (base-2)."
        }
      },
      {
        "id": 33,
        "topic": "Kullback-Leibler (KL) Divergence",
        "difficulty": "analysis",
        "question": "In Jensen-Shannon (JS) Divergence, why is it often preferred over standard KL Divergence in machine learning applications?",
        "options": [
          "JS Divergence is symmetric and always provides a finite value.",
          "JS Divergence is easier to calculate because it ignores the logarithm.",
          "JS Divergence allows for negative values which help in gradient descent.",
          "JS Divergence only requires one distribution instead of two."
        ],
        "correct_index": 0,
        "rationale": "As shown on slide 15, JS divergence is a symmetric measure based on KL. Because it compares distributions to their average, it avoids the 'division by zero' (infinite) issues of KL divergence.",
        "distractor_analysis": {
          "0": "Correct. Symmetry and finiteness are its two primary advantages.",
          "1": "Incorrect; it is still based on KL divergence, which uses logarithms.",
          "2": "Incorrect; JS divergence, like KL, is non-negative.",
          "3": "Incorrect; all divergence measures require two distributions to compare."
        }
      },
      {
        "id": 34,
        "topic": "Noiseless Coding Theorem",
        "difficulty": "analysis",
        "question": "The Lempel-Ziv-Welch (LZW) algorithm is mentioned as a 'more sophisticated' algorithm. In what conceptual way does it extend Shannon’s basic theorem?",
        "options": [
          "It encodes sequences of states (blocks) rather than individual symbols, capturing inter-symbol dependencies.",
          "It allows for lossy compression, which Shannon’s Noiseless theorem prohibits.",
          "It ignores entropy entirely and uses a purely deterministic approach.",
          "It only works if the source has zero entropy."
        ],
        "correct_index": 0,
        "rationale": "While basic Huffman coding works on single symbols, LZW builds a dictionary of sequences. This allows it to compress data where the 'surprise' of a symbol depends on the symbols preceding it.",
        "distractor_analysis": {
          "0": "Correct. Block-encoding/dictionary methods exploit dependencies to reach the entropy limit of the *source*.",
          "1": "Incorrect; LZW is a lossless algorithm.",
          "2": "Incorrect; LZW is fundamentally tied to the information content and entropy of the data it processes.",
          "3": "Incorrect; if entropy were zero, the file would be one repeating character and trivial to compress."
        }
      },
      {
        "id": 35,
        "topic": "Entropy",
        "difficulty": "recall",
        "question": "What does the term 'Joint Entropy' H(X, Y) represent?",
        "options": [
          "The total uncertainty contained in a system of two random variables considered together.",
          "The average information that X and Y share.",
          "The ratio of the entropy of X to the entropy of Y.",
          "The difference between the entropy of the sender and the receiver."
        ],
        "correct_index": 0,
        "rationale": "Joint entropy is the entropy of the joint distribution of two variables; it measures the total uncertainty of the pair (X, Y).",
        "distractor_analysis": {
          "0": "Correct. Total system uncertainty.",
          "1": "Incorrect; shared information is Mutual Information.",
          "2": "Incorrect; joint entropy is an additive/subtractive concept, not a ratio.",
          "3": "Incorrect; this is not a standard definition in information theory."
        }
      },
      {
        "id": 36,
        "topic": "Huffman Compression",
        "difficulty": "application",
        "question": "If you add a 9th possible state with a very low probability to the example on Slide 18, how would the Huffman tree change?",
        "options": [
          "The new state would be added at the deepest possible level of the tree, likely sharing a parent with the previous least-likely state.",
          "The entire tree would be discarded and replaced with a fixed-length 4-bit code.",
          "The most frequent state (x1) would have its codeword length increased to accommodate the new state.",
          "The tree would become a perfectly balanced binary tree."
        ],
        "correct_index": 0,
        "rationale": "Huffman's bottom-up construction always starts by combining the two least frequent nodes. A new, very rare state would simply pair up with the current rarest state at the bottom of the tree.",
        "distractor_analysis": {
          "0": "Correct. Rare states are pushed to the bottom (longest codes).",
          "1": "Incorrect; Huffman is dynamic and adapts to the number of states and their probabilities.",
          "2": "Incorrect; the top of the tree (frequent symbols) remains short unless the probabilities change significantly.",
          "3": "Incorrect; balanced trees only occur when probabilities are near-equal."
        }
      },
      {
        "id": 37,
        "topic": "Kolmogorov Complexity",
        "difficulty": "analysis",
        "question": "Why is it impossible to write a program that calculates the exact Kolmogorov Complexity for any arbitrary string?",
        "options": [
          "Because it is related to the Halting Problem; you cannot always determine if a shorter program will eventually produce the string.",
          "Because the complexity depends on the specific programming language used.",
          "Because strings can be infinitely long, making the calculation take infinite time.",
          "Because the value changes every time the data is compressed."
        ],
        "correct_index": 0,
        "rationale": "Kolmogorov complexity is 'uncomputable' because to find the absolute shortest program, you would have to test all possible programs. The Halting Problem prevents us from knowing if a candidate program will ever finish and output the string.",
        "distractor_analysis": {
          "0": "Correct. This is the theoretical basis for its uncomputability.",
          "1": "Incorrect; while the exact value might vary by a constant, the concept is universal (Invariance Theorem).",
          "2": "Incorrect; complexity is defined for finite strings; length is not the issue, logic is.",
          "3": "Incorrect; Kolmogorov complexity is a theoretical property of the string itself, not a measurement of a specific compression run."
        }
      },
      {
        "id": 38,
        "topic": "Information Content",
        "difficulty": "analysis",
        "question": "What is the consequence of the 'closed world assumption' mentioned in Slide 17 for information theory?",
        "options": [
          "It assumes the sender and receiver share the same semantic context, allowing context to be ignored.",
          "It assumes that no new information can ever be created in the universe.",
          "It limits communication to happen only between two computers in the same room.",
          "It assumes that information is always lost during transmission."
        ],
        "correct_index": 0,
        "rationale": "Standard information theory (Shannon) focuses on the *syntax* (the bits). It assumes the meaning (semantics) is already agreed upon or irrelevant to the technical task of transmission.",
        "distractor_analysis": {
          "0": "Correct. Semantic context is factored out by this assumption.",
          "1": "Incorrect; information theory is about the transmission of *new* messages (news).",
          "2": "Incorrect; 'closed' refers to the shared knowledge/rules, not physical location.",
          "3": "Incorrect; information theory assumes information is *measurable* and *never lost* (unless the channel is noisy)."
        }
      },
      {
        "id": 39,
        "topic": "Entropy",
        "difficulty": "analysis",
        "question": "Compare the entropy of a fair 6-sided die and a fair 20-sided die. Which is true?",
        "options": [
          "The 20-sided die has higher entropy because it has more possible outcomes, increasing uncertainty.",
          "Both have the same entropy because they are both 'fair'.",
          "The 6-sided die has higher entropy because each outcome is more likely.",
          "Entropy cannot be compared between systems with different numbers of states."
        ],
        "correct_index": 0,
        "rationale": "For uniform distributions, H(X) = log2(n). Since 20 > 6, log2(20) > log2(6). There is more uncertainty in the 20-sided die.",
        "distractor_analysis": {
          "0": "Correct. More states at equal probability equals more uncertainty.",
          "1": "Incorrect; 'fairness' means uniform distribution, but the number of outcomes still matters.",
          "2": "Incorrect; higher probability of individual outcomes actually *lowers* entropy if the number of states is small.",
          "3": "Incorrect; entropy is a universal measure in bits and can be compared across any discrete variables."
        }
      },
      {
        "id": 40,
        "topic": "Minimum Description Length (MDL)",
        "difficulty": "recall",
        "question": "Which historical figure's philosophy is the basis for the Minimum Description Length principle?",
        "options": [
          "William of Ockham",
          "Claude Shannon",
          "Andrey Kolmogorov",
          "Thomas Bayes"
        ],
        "correct_index": 0,
        "rationale": "Slide 13 explicitly links MDL to Occam's Razor, attributed to William of Ockham (1285–1349).",
        "distractor_analysis": {
          "0": "Correct. The principle of parsimony.",
          "1": "Incorrect; Shannon provided the math for information theory, but MDL is the application of Ockham's philosophy.",
          "2": "Incorrect; Kolmogorov developed algorithmic information theory, a cousin of MDL.",
          "3": "Incorrect; Bayes is the father of Bayesian probability, which is related but not the source of the 'simplest hypothesis' razor."
        }
      },
      {
        "id": 41,
        "topic": "Channel Capacity",
        "difficulty": "analysis",
        "question": "What does 'Channel Capacity' C represent in a noisy channel?",
        "options": [
          "The maximum rate at which information can be transmitted with an arbitrarily small error probability.",
          "The physical limit on the number of bits that can be stored in the cable.",
          "The average noise level of the channel measured in decibels.",
          "The total length of the message divided by the time it takes to send it."
        ],
        "correct_index": 0,
        "rationale": "Channel capacity is defined as the maximum mutual information between the input and output, which defines the upper limit for reliable communication.",
        "distractor_analysis": {
          "0": "Correct. This is Shannon's definition of capacity.",
          "1": "Incorrect; capacity is a rate (bits per use/second), not a static storage volume.",
          "2": "Incorrect; noise affects capacity, but capacity is the *result* of that noise on information flow.",
          "3": "Incorrect; this would just be the raw transmission speed, not the limit for error-free delivery."
        }
      },
      {
        "id": 42,
        "topic": "Mutual Information",
        "difficulty": "application",
        "question": "If X is a message and Y is a noisy version of that message, what does I(X; Y) tell the receiver?",
        "options": [
          "How many bits of the original message X can be recovered from the received message Y.",
          "The exact number of bit-flips that occurred during transmission.",
          "The entropy of the noise in the channel.",
          "The total length of the message X."
        ],
        "correct_index": 0,
        "rationale": "Mutual information quantifies the overlap. In a communication context, it is the amount of information that successfully 'passed through' the noise from X to Y.",
        "distractor_analysis": {
          "0": "Correct. It's the recoverable/shared information.",
          "1": "Incorrect; I(X;Y) is a statistical average, not a count of specific errors.",
          "2": "Incorrect; this is related to the conditional entropy H(Y|X).",
          "3": "Incorrect; mutual information is about content, not the raw length."
        }
      },
      {
        "id": 43,
        "topic": "Entropy",
        "difficulty": "analysis",
        "question": "Why is the entropy of a language (like English) much lower than the log of the number of words in its dictionary?",
        "options": [
          "Because languages have significant redundancy and rules (grammar, common phrases) that make words predictable.",
          "Because humans only use a small fraction of the words available in a dictionary.",
          "Because English words are generally short in length.",
          "Because the 'closed world assumption' does not apply to natural languages."
        ],
        "correct_index": 0,
        "rationale": "Entropy is maximized only for uniform, independent outcomes. In language, letters and words follow strict patterns (e.g., 'q' is usually followed by 'u'), which significantly reduces uncertainty and thus entropy.",
        "distractor_analysis": {
          "0": "Correct. Context and structure reduce entropy.",
          "1": "Incorrect; even if we used all words, if we used them in predictable patterns, entropy would be low.",
          "2": "Incorrect; word length (physical size) is not information entropy.",
          "3": "Incorrect; the assumption is a modeling tool, not the cause of language's statistical structure."
        }
      },
      {
        "id": 44,
        "topic": "Repetition Codes",
        "difficulty": "analysis",
        "question": "In the repetition code example (Slide 24), what happens to the 'Source Information Rate' as you increase the number of repetitions (e.g., from 3 to 101)?",
        "options": [
          "The rate decreases, because you are sending more 'empty' bits for every 'real' bit of information.",
          "The rate increases, because the message is now more likely to be correct.",
          "The rate stays the same, because the source entropy has not changed.",
          "The rate becomes equal to the channel capacity."
        ],
        "correct_index": 0,
        "rationale": "Rate R is (useful bits / total bits). By sending 101 bits for every 1 source bit, the rate R = 1/101, which is much lower than R = 1/3. You trade off speed for reliability.",
        "distractor_analysis": {
          "0": "Correct. Efficiency is sacrificed for error correction.",
          "1": "Incorrect; reliability increases, but the *rate* of transmission of new info decreases.",
          "2": "Incorrect; the 'Source Information Rate' refers to the ratio of information to symbols sent over the channel.",
          "3": "Incorrect; rate and capacity are independent; rate is a choice, capacity is a limit."
        }
      },
      {
        "id": 45,
        "topic": "TF-IDF",
        "difficulty": "application",
        "question": "If the term 'Artificial' appears 10 times in Document A and 10 times in Document B, but Document A is twice as long as Document B, which will have a higher Term Frequency (TF) for that word?",
        "options": [
          "Document B, because the term is more concentrated relative to the document length.",
          "Document A, because it has more total content.",
          "Both will have the same TF because the raw count is 10.",
          "Neither; TF can only be calculated for the most frequent word in the corpus."
        ],
        "correct_index": 0,
        "rationale": "Slide 33 shows TF as a normalized measure (count / max_count or total_words). A word appearing 10 times in a short document represents a higher 'density' or 'importance' for that document than 10 times in a long one.",
        "distractor_analysis": {
          "0": "Correct. Normalization makes TF relative to document size.",
          "1": "Incorrect; raw counts favor long documents, which normalization specifically corrects.",
          "2": "Incorrect; information retrieval requires accounting for document length.",
          "3": "Incorrect; TF is calculated for any term in any document."
        }
      },
      {
        "id": 46,
        "topic": "Entropy",
        "difficulty": "recall",
        "question": "What is the lower bound of entropy H(X) for any discrete random variable?",
        "options": [
          "0",
          "-1",
          "1",
          "Negative infinity"
        ],
        "correct_index": 0,
        "rationale": "Slide 12: log n ≥ H[X] ≥ 0. Entropy is a measure of uncertainty; you cannot have 'less than zero' uncertainty.",
        "distractor_analysis": {
          "0": "Correct. Zero represents absolute certainty.",
          "1": "Incorrect; probabilities are [0,1], and -P*log(P) is always non-negative.",
          "2": "Incorrect; entropy can be much smaller than 1 (e.g., a very biased coin).",
          "3": "Incorrect; this is physically and mathematically impossible for discrete entropy."
        }
      },
      {
        "id": 47,
        "topic": "Source Information Rate",
        "difficulty": "analysis",
        "question": "When is a source considered 'memory-less' in information theory?",
        "options": [
          "When the probability of the current symbol does not depend on any of the previous symbols.",
          "When the source cannot store any of the messages it has sent.",
          "When the entropy of the source is zero.",
          "When the source uses a fixed-length code for all messages."
        ],
        "correct_index": 0,
        "rationale": "As shown in Slide 26, a source is memory-less if the joint entropy of a block of symbols is simply the sum of their individual entropies, meaning they are independent.",
        "distractor_analysis": {
          "0": "Correct. This is the definition of independence in a sequence.",
          "1": "Incorrect; 'memory' refers to statistical dependency, not hardware storage.",
          "2": "Incorrect; memory-less sources can have very high entropy (e.g., a sequence of fair coin flips).",
          "3": "Incorrect; coding choice is independent of the source's statistical properties."
        }
      },
      {
        "id": 48,
        "topic": "Ziv-Lempel Compression",
        "difficulty": "recall",
        "question": "In LZ77 compression, what does a triple (previous, length, new) represent?",
        "options": [
          "A pointer to a previous match, the length of that match, and the next character after the match.",
          "The probability of the symbol, its length in bits, and its new ASCII code.",
          "The distance to the end of the file, the length of the window, and a new dictionary index.",
          "The previous character, the length of the whole string, and a new bit sequence."
        ],
        "correct_index": 0,
        "rationale": "Slide 21: 'previous' is the distance to the previous occurrence, 'length' is the length of the string, and 'new' is the symbol following it.",
        "distractor_analysis": {
          "0": "Correct. This is the standard encoding for LZ77.",
          "1": "Incorrect; LZ77 is a dictionary/sliding window method, not a probability-based method like Huffman.",
          "2": "Incorrect; LZ77 looks backward, not toward the end of the file.",
          "3": "Incorrect; this does not match the sliding window logic described."
        }
      },
      {
        "id": 49,
        "topic": "Information Theory Overview",
        "difficulty": "recall",
        "question": "In what year and publication was the foundation of Information Theory established?",
        "options": [
          "1948, 'A Mathematical Theory of Communication'",
          "1952, 'The Art of Computer Programming'",
          "1936, 'On Computable Numbers'",
          "1925, 'Foundations of AI and Data Science'"
        ],
        "correct_index": 0,
        "rationale": "Slide 4 cites Claude Shannon's landmark publication in 1948 as the start of the field.",
        "distractor_analysis": {
          "0": "Correct. This is the 'bible' of Information Theory.",
          "1": "Incorrect; this is Donald Knuth's famous series started much later.",
          "2": "Incorrect; this is Alan Turing's paper on computation/Turing machines.",
          "3": "Incorrect; this is likely the title of the current course, not the historical paper."
        }
      },
      {
        "id": 50,
        "topic": "Mutual Information",
        "difficulty": "analysis",
        "question": "If you have two variables X and Y, and X perfectly determines Y (i.e., Y = f(X)), what is the relationship between I(X; Y) and H(Y)?",
        "options": [
          "I(X; Y) = H(Y)",
          "I(X; Y) = 0",
          "I(X; Y) = H(X) + H(Y)",
          "I(X; Y) = H(X) / H(Y)"
        ],
        "correct_index": 0,
        "rationale": "Mutual information is I(X; Y) = H(Y) - H(Y|X). If X determines Y, there is no uncertainty in Y once X is known, so H(Y|X) = 0. Therefore, I(X; Y) = H(Y). All information in Y is shared with X.",
        "distractor_analysis": {
          "0": "Correct. Knowing X tells you everything about Y.",
          "1": "Incorrect; this would only be true if they were independent, the opposite of this scenario.",
          "2": "Incorrect; this would imply they have no information in common.",
          "3": "Incorrect; mutual information is measured in bits (subtraction), not ratios."
        }
      }
    ]
  },
  "foundations-04": {
    "title": "Foundations of AI - Supervised Classification",
    "questions": [
      {
        "id": 1,
        "topic": "Feature Types",
        "difficulty": "recall",
        "question": "According to the slide on feature types, which property is characteristically absent in ordinal features but present in continuous features?",
        "options": [
          "Ordering/Ranking",
          "Scale/Unit measurement",
          "Median tendency",
          "Quantile dispersion"
        ],
        "correct_index": 1,
        "rationale": "As shown in the slide table, ordinal features possess 'Order' but lack 'Scale', whereas continuous features possess both.",
        "distractor_analysis": {
          "0": "Both ordinal and continuous features can be ordered.",
          "1": "Correct. Continuous features have a defined scale/distance between values.",
          "2": "The median is a valid measure of central tendency for ordinal features.",
          "3": "Quantiles are the standard way to measure dispersion for ordinal features."
        }
      },
      {
        "id": 2,
        "topic": "Feature Selection",
        "difficulty": "application",
        "question": "A data scientist is selecting features for a classification task. Based on the provided guidelines, which feature pair would be considered the most ideal for model efficiency?",
        "options": [
          "Two features that are highly correlated with each other and the target class.",
          "Two features that are independent of each other but both show strong class correlation.",
          "Two features that are extremely specific to individual training instances.",
          "Two features that provide general information across all classes equally."
        ],
        "correct_index": 1,
        "rationale": "Features should ideally be independent of each other to avoid redundancy but have strong correlation with the class to provide predictive power.",
        "distractor_analysis": {
          "0": "Correlated features are redundant and violate the preference for independence.",
          "1": "Correct. This maximizes information gain while minimizing redundancy.",
          "2": "Features that are too specific lead to overfitting.",
          "3": "Features that are too general provide no discriminative power between classes."
        }
      },
      {
        "id": 3,
        "topic": "Feature Transformations",
        "difficulty": "analysis",
        "question": "When transforming a 'Continuous' feature into an 'Ordinal' feature, what is the specific terminology used for this process on the slides?",
        "options": [
          "Normalization",
          "Calibration",
          "Discretization",
          "Grouping"
        ],
        "correct_index": 2,
        "rationale": "The transformation matrix on slide 7 explicitly labels the path from Continuous to Ordinal (or Categorical) as 'Discretization'.",
        "distractor_analysis": {
          "0": "Normalization is continuous-to-continuous scaling.",
          "1": "Calibration is used for Categorical/Ordinal to Continuous mapping.",
          "2": "Correct. It involves mapping continuous ranges to discrete bins.",
          "3": "Grouping is the process of merging existing Categorical values."
        }
      },
      {
        "id": 4,
        "topic": "Multi-class Classification",
        "difficulty": "recall",
        "question": "Which of the following models is explicitly noted as one that does NOT naturally generalize to multi-class classification, often requiring geometric adaptations?",
        "options": [
          "Naive Bayes",
          "Decision Trees",
          "Support Vector Machines",
          "Neural Networks"
        ],
        "correct_index": 2,
        "rationale": "Slide 12 states that geometric models like SVMs and Winnow do not naturally generalize to multiclass, unlike probabilistic or rule-based models.",
        "distractor_analysis": {
          "0": "Naive Bayes handles multi-class naturally via probability distributions.",
          "1": "Decision Trees naturally split into multiple leaves/classes.",
          "2": "Correct. SVM is inherently a binary geometric separator.",
          "3": "Neural Networks generalize via multi-unit output layers."
        }
      },
      {
        "id": 5,
        "topic": "Multi-class Strategy",
        "difficulty": "analysis",
        "question": "In a 'One-of' classification task involving 'k' classes, how is the final label for an instance determined after applying all classifiers separately?",
        "options": [
          "The label is assigned if any single classifier returns a positive result.",
          "The instance is assigned to the class that yields the maximum score.",
          "The decisions are independent; the instance can belong to multiple classes.",
          "A hierarchical tree determines the label based on a sequence of binary splits."
        ],
        "correct_index": 1,
        "rationale": "Slide 13 specifies that for 'One-of', we assign the instance to the class with the maximum score.",
        "distractor_analysis": {
          "0": "This describes a loose OR logic not mentioned in the slides.",
          "1": "Correct. The max score identifies the 'best fit' among exclusive classes.",
          "2": "This describes 'Any-of' (multi-label) classification.",
          "3": "This describes a hierarchical classifier, not the standard 'One-vs-Rest' logic."
        }
      },
      {
        "id": 6,
        "topic": "Overfitting",
        "difficulty": "application",
        "question": "If a Neural Network's performance is significantly higher on the training set than the test set, which adjustment based on the slide's theory would most likely mitigate this?",
        "options": [
          "Increasing the number of parameters to fit training data better.",
          "Reducing the training duration (learning time).",
          "Decreasing the size of the training set.",
          "Increasing the dimensionality of the feature space."
        ],
        "correct_index": 1,
        "rationale": "Slide 15 lists 'Learning is performed for too long' as a cause of overfitting; thus, reducing duration helps.",
        "distractor_analysis": {
          "0": "More parameters typically increase overfitting risk.",
          "1": "Correct. Stopping early prevents the model from memorizing noise.",
          "2": "Small training sets are a cause of overfitting, not a cure.",
          "3": "High dimensionality is a major driver of the 'Curse of Dimensionality' and overfitting."
        }
      },
      {
        "id": 7,
        "topic": "Curse of Dimensionality",
        "difficulty": "analysis",
        "question": "How does the 'Curse of Dimensionality' specifically affect distance-based similarity measures like those used in KNN?",
        "options": [
          "Distances become exponentially smaller, causing numerical underflow.",
          "The difference between the distance to the nearest and farthest neighbor becomes negligible.",
          "Computational complexity decreases because data points are further apart.",
          "Feature vectors become linearly dependent as dimensions increase."
        ],
        "correct_index": 1,
        "rationale": "Slide 17 notes that distance-based similarities become 'non-discriminative' because in high-dimensional space, all points tend to become equidistant.",
        "distractor_analysis": {
          "0": "Volume grows, so distances generally appear to increase, but the relative difference disappears.",
          "1": "Correct. This makes 'nearness' lose its meaning for classification.",
          "2": "Computational complexity increases, it does not decrease.",
          "3": "Dimensionality increases the independence of vectors unless they are redundant."
        }
      },
      {
        "id": 8,
        "topic": "Curse of Dimensionality",
        "difficulty": "analysis",
        "question": "Considering the volume of a unit sphere (r=1), what happens to the data distribution as the number of dimensions (D) increases toward infinity?",
        "options": [
          "Data concentrates almost entirely at the center of the sphere.",
          "Data is distributed uniformly throughout the volume.",
          "Data concentrates in a thin shell near the surface of the sphere.",
          "The volume of the sphere collapses to zero."
        ],
        "correct_index": 2,
        "rationale": "Slide 17's formula 1 - (1 - ε)^D shows that as D increases, the fraction of volume in the outer shell (between 1-ε and 1) approaches 1.",
        "distractor_analysis": {
          "0": "Mathematically, the 'center' volume fraction approaches zero in high D.",
          "1": "Uniformity is lost as almost all volume shifts to the shell.",
          "2": "Correct. This is a counter-intuitive property of high-dimensional space.",
          "3": "The volume doesn't collapse; our ability to populate it with representative data does."
        }
      },
      {
        "id": 9,
        "topic": "Cross Validation",
        "difficulty": "recall",
        "question": "In N-fold cross-validation, what is the ideal frequency of each class in a single fold?",
        "options": [
          "One class should dominate each fold to test model robustness.",
          "Classes should be randomly distributed without regard to frequency.",
          "Each fold should contain exactly one instance of each class.",
          "Proportional to its frequency in the full dataset."
        ],
        "correct_index": 3,
        "rationale": "Slide 18 states that ideally, class frequency in a fold should be proportional to its frequency in the full dataset (stratification).",
        "distractor_analysis": {
          "0": "This would bias the training/testing phase of that fold.",
          "1": "Random distribution is the baseline, but 'proportional' is the listed ideal.",
          "2": "This is only possible if all classes have equal counts and N matches that count.",
          "3": "Correct. This ensures each fold is a representative 'mini-version' of the whole."
        }
      },
      {
        "id": 10,
        "topic": "Bootstrapping",
        "difficulty": "analysis",
        "question": "What is a primary difference between the training set in N-fold Cross-Validation versus Bootstrapping?",
        "options": [
          "Bootstrapping samples without replacement, while CV samples with replacement.",
          "Bootstrapping allows the same instance to appear multiple times in the training set.",
          "CV results in a training set that is ~63.2% of the total data.",
          "Bootstrapping ensures that every instance is used exactly once for training."
        ],
        "correct_index": 1,
        "rationale": "Slide 19 defines Bootstrapping as sampling 'with replacement', meaning instances can be repeated in the training set.",
        "distractor_analysis": {
          "0": "It is the opposite; CV is an exhaustive partition, Bootstrap uses replacement.",
          "1": "Correct. This is the defining characteristic of the Bootstrap method.",
          "2": "Bootstrapping results in ~63.2% *unique* instances, CV uses (N-1)/N (e.g., 90%).",
          "3": "CV ensures usage across folds; Bootstrapping is stochastic and may omit instances from training entirely."
        }
      },
      {
        "id": 11,
        "topic": "Error Types",
        "difficulty": "recall",
        "question": "Which error type represents the theoretical minimum error rate that any classifier can achieve on a given task?",
        "options": [
          "Training error",
          "Test error",
          "Bayes error",
          "Generalized error"
        ],
        "correct_index": 2,
        "rationale": "Slide 22 defines the Bayes error as the result of the 'Bayes optimal classifier' which minimizes the probability of misclassification.",
        "distractor_analysis": {
          "0": "Training error can often be zero (overfitting) but is not the theoretical limit of accuracy.",
          "1": "Test error is an estimate of real-world performance for a specific model.",
          "2": "Correct. It is the error due to overlapping class distributions.",
          "3": "Generalized error is the expected error on unseen data for a specific algorithm."
        }
      },
      {
        "id": 12,
        "topic": "Loss Functions",
        "difficulty": "application",
        "question": "In a medical diagnosis model for a severe disease, if the cost of missing a sick patient is much higher than the cost of a false alarm, which matrix element must be weighted more heavily?",
        "options": [
          "Ground Truth: Disease / Predicted: Disease",
          "Ground Truth: OK / Predicted: OK",
          "Ground Truth: Disease / Predicted: OK",
          "Ground Truth: OK / Predicted: Disease"
        ],
        "correct_index": 2,
        "rationale": "Missing a sick patient is a False Negative. Slide 23's loss matrix example shows 'Ground Truth: disease / Predicted: ok' with a high value (1000).",
        "distractor_analysis": {
          "0": "This is a True Positive, which should have zero or low loss.",
          "1": "This is a True Negative, which should have zero or low loss.",
          "2": "Correct. This is a False Negative (miss), the most dangerous error in this scenario.",
          "3": "This is a False Positive (false alarm), which is less critical than a miss here."
        }
      },
      {
        "id": 13,
        "topic": "Model Selection",
        "difficulty": "analysis",
        "question": "When comparing two models, h and h', using k-fold cross-validation, what statistical distribution is typically used to determine if their error means are significantly different?",
        "options": [
          "Normal distribution",
          "Chi-square distribution",
          "t-distribution with k-1 degrees of freedom",
          "Poisson distribution"
        ],
        "correct_index": 2,
        "rationale": "Slide 25 states that the difference in errors is t-distributed with k-1 degrees of freedom.",
        "distractor_analysis": {
          "0": "While errors are approx. normal, the *difference* in sample means follows t.",
          "1": "Chi-square is used for feature independence tests, not mean error comparison.",
          "2": "Correct. This is the standard for the paired t-test logic presented.",
          "3": "Poisson is for count data, not continuous error rate averages."
        }
      },
      {
        "id": 14,
        "topic": "Accuracy",
        "difficulty": "analysis",
        "question": "Why might a classifier with 99% accuracy still be considered 'bad' or misleading?",
        "options": [
          "Because accuracy does not account for the variance of the features.",
          "If the dataset is highly imbalanced (e.g., 99% of instances belong to one class).",
          "If the classifier's precision is equal to its recall.",
          "If the decision threshold is set exactly at 0.5."
        ],
        "correct_index": 1,
        "rationale": "Slide 26 asks 'Is an accuracy of 99% good?'. In imbalanced data, a dummy classifier predicting only the majority class would hit 99% accuracy without learning.",
        "distractor_analysis": {
          "0": "Variance is a feature property, but accuracy's failure is a class distribution problem.",
          "1": "Correct. This is the 'accuracy paradox' in skewed datasets.",
          "2": "Equal precision and recall often indicate a well-balanced model.",
          "3": "A 0.5 threshold is standard; accuracy's failure is independent of the threshold value itself."
        }
      },
      {
        "id": 15,
        "topic": "Precision vs. Recall",
        "difficulty": "application",
        "question": "A search engine wants to ensure that every result it shows is highly relevant, even if it misses some relevant documents. Which metric should it prioritize?",
        "options": [
          "Recall",
          "Accuracy",
          "Precision",
          "Sensitivity"
        ],
        "correct_index": 2,
        "rationale": "Slide 27: Precision = TP / (TP + FP). It represents the fraction of predicted positives that are actually positive. High precision means low False Positives.",
        "distractor_analysis": {
          "0": "Recall (Sensitivity) prioritizes finding *all* relevant documents, potentially increasing noise.",
          "1": "Accuracy is too general and doesn't distinguish between the types of errors.",
          "2": "Correct. Precision measures the 'purity' of the positive predictions.",
          "3": "Sensitivity is a synonym for Recall, which focuses on completeness."
        }
      },
      {
        "id": 16,
        "topic": "F1-measure",
        "difficulty": "recall",
        "question": "The F1-measure is described as which type of mathematical mean between Precision and Recall?",
        "options": [
          "Arithmetic mean",
          "Geometric mean",
          "Harmonic mean",
          "Weighted average"
        ],
        "correct_index": 2,
        "rationale": "Slide 29 explicitly states: 'The F1-score is the harmonic mean between Precision and Recall.'",
        "distractor_analysis": {
          "0": "Arithmetic mean would over-reward a model with one very high and one very low score.",
          "1": "Geometric mean is used in other contexts but not the standard F1 formula.",
          "2": "Correct. The harmonic mean is sensitive to low values, requiring both to be high.",
          "3": "While it can be weighted (F-alpha), the standard F1 is specifically the harmonic mean."
        }
      },
      {
        "id": 17,
        "topic": "Precision-Recall Curve",
        "difficulty": "analysis",
        "question": "Which condition is required to plot a Precision-Recall curve rather than just a single point?",
        "options": [
          "The dataset must have more than two classes.",
          "The model must provide a continuous prediction probability/score to vary a threshold.",
          "The precision must be strictly greater than the recall.",
          "Cross-validation must be performed at least 10 times."
        ],
        "correct_index": 1,
        "rationale": "Slide 30 states: 'With decreasing decision threshold, plot precision recall values'. This requires a model that outputs scores, not just hard labels.",
        "distractor_analysis": {
          "0": "Curves can be used for binary or multi-class (one-against-all).",
          "1": "Correct. Without a threshold to vary, you only have one Precision/Recall pair.",
          "2": "Precision and recall can be in any relationship.",
          "3": "Cross-validation is a validation strategy, not a plotting requirement."
        }
      },
      {
        "id": 18,
        "topic": "ROC Curve",
        "difficulty": "recall",
        "question": "In an ROC curve, what are the axes used for plotting?",
        "options": [
          "Precision (y) vs. Recall (x)",
          "Sensitivity (y) vs. 1-Specificity (x)",
          "True Positive Rate (y) vs. Precision (x)",
          "Accuracy (y) vs. Error Rate (x)"
        ],
        "correct_index": 1,
        "rationale": "Slide 31 labels the y-axis as Sensitivity (TPR) and the x-axis as 1 - Specificity (FPR).",
        "distractor_analysis": {
          "0": "This is a Precision-Recall curve.",
          "1": "Correct. This shows the trade-off between benefits (TPR) and costs (FPR).",
          "2": "This combines axes from two different curve types.",
          "3": "Accuracy vs Error is redundant and not a standard curve."
        }
      },
      {
        "id": 19,
        "topic": "AUC",
        "difficulty": "analysis",
        "question": "What does the Area Under the Curve (AUC) fundamentally represent regarding a model's capabilities?",
        "options": [
          "The probability that the model is 100% accurate.",
          "The ranking accuracy of the model.",
          "The optimal decision threshold for the specific dataset.",
          "The ratio of training error to test error."
        ],
        "correct_index": 1,
        "rationale": "Slide 31 explicitly states that the AUC 'represents the ranking accuracy'.",
        "distractor_analysis": {
          "0": "AUC measures performance across all thresholds, not a single probability of perfection.",
          "1": "Correct. It measures how well the model separates the scores of different classes.",
          "2": "AUC is threshold-independent; it summarizes the curve generated by *all* thresholds.",
          "3": "AUC has nothing to do with the ratio of training/test error."
        }
      },
      {
        "id": 20,
        "topic": "Metric Selection",
        "difficulty": "application",
        "question": "If you need to perform a 'deeper analysis of thresholds' for a classifier, which tool should you use according to the summary slide?",
        "options": [
          "Confusion Matrix",
          "F1-score",
          "ROC curves",
          "Specific Accuracy"
        ],
        "correct_index": 2,
        "rationale": "Slide 33 categorizes 'ROC curves' and 'Precision-recall curves' under 'For deeper analysis of thresholds'.",
        "distractor_analysis": {
          "0": "Confusion matrices are for a single fixed threshold.",
          "1": "F1-score is a single scalar for a fixed threshold.",
          "2": "Correct. Curves visualize the impact of changing the threshold.",
          "3": "Accuracy is a single point metric if the threshold is not an issue."
        }
      },
      {
        "id": 21,
        "topic": "Discretization",
        "difficulty": "analysis",
        "question": "Which method of discretization relies on the target class labels to determine bin boundaries?",
        "options": [
          "Distance-based binning",
          "Unsupervised clustering",
          "Entropy-based splitting",
          "Equal-width binning"
        ],
        "correct_index": 2,
        "rationale": "Slide 8 categorizes 'Dependency, entropy' and 'Accuracy, error' under 'Supervised' discretization, which uses class information.",
        "distractor_analysis": {
          "0": "Distance/similarity is categorized under Unsupervised.",
          "1": "Clustering is explicitly categorized under Unsupervised.",
          "2": "Correct. Entropy splits are calculated based on class labels.",
          "3": "Equal-width is a standard unsupervised strategy not involving labels."
        }
      },
      {
        "id": 22,
        "topic": "Terminology",
        "difficulty": "recall",
        "question": "How is an 'instance' abstractly described in the context of supervised learning according to the slides?",
        "options": [
          "As a single class label",
          "Through a feature vector with feature values",
          "As a set of similar classes",
          "As a collection of training sets"
        ],
        "correct_index": 1,
        "rationale": "Slide 4 defines an instance as: 'Abstractly described through a feature vector with feature values, x = (x1, ..., xn)'.",
        "distractor_analysis": {
          "0": "A class label is assigned to an instance, not its description.",
          "1": "Correct. This is the standard vector representation.",
          "2": "A class is a set of similar instances, not the other way around.",
          "3": "Training sets are collections of instances, not an abstract description of one."
        }
      },
      {
        "id": 23,
        "topic": "Feature Characteristics",
        "difficulty": "application",
        "question": "Which measure of central tendency is appropriate for a 'Categorical' feature like 'Eye Color'?",
        "options": [
          "Mean",
          "Median",
          "Mode",
          "Standard Deviation"
        ],
        "correct_index": 2,
        "rationale": "Slide 6 lists 'mode' as the tendency measure for Categorical features and 'n/a' for others.",
        "distractor_analysis": {
          "0": "Mean requires a scale and order (continuous).",
          "1": "Median requires at least an order (ordinal/continuous).",
          "2": "Correct. Mode just identifies the most frequent category.",
          "3": "Standard deviation requires a scale/distance."
        }
      },
      {
        "id": 24,
        "topic": "Model Robustness",
        "difficulty": "analysis",
        "question": "Which of these classifications algorithms is categorized as 'Non-linear' in the provided slides?",
        "options": [
          "Naive Bayes",
          "Logistic Regression",
          "Support Vector Machines",
          "K-Nearest Neighbors"
        ],
        "correct_index": 3,
        "rationale": "Slide 11 lists K-Nearest Neighbors under 'Non-linear classification models'.",
        "distractor_analysis": {
          "0": "Naive Bayes is listed under Linear (Slide 10).",
          "1": "Logistic Regression is listed under Linear (Slide 10).",
          "2": "Support Vector Machines (basic) is listed under Linear (Slide 10).",
          "3": "Correct. KNN relies on local neighborhoods, creating non-linear boundaries."
        }
      },
      {
        "id": 25,
        "topic": "Validation",
        "difficulty": "recall",
        "question": "In the training and validation workflow, what is the 'Test set' used for specifically?",
        "options": [
          "To learn the model parameters",
          "To choose between different feature types",
          "To validate the model with hidden classes",
          "To perform discretization of continuous features"
        ],
        "correct_index": 2,
        "rationale": "Slide 9 and 14 state the test set contains instances where classes are 'hidden to test the classifier' for validation.",
        "distractor_analysis": {
          "0": "The training set is used to learn parameters.",
          "1": "Feature selection is done before or during training, not on the test set.",
          "2": "Correct. It simulates real-world performance on unseen data.",
          "3": "Discretization is a preprocessing step applied to all data or during training."
        }
      },
      {
        "id": 26,
        "topic": "Overfitting",
        "difficulty": "application",
        "question": "Why does high dimensionality (many features) often lead to overfitting?",
        "options": [
          "It makes the training set more representative of the population.",
          "It allows the model to find coincidental patterns in the training noise.",
          "It reduces the space the model needs to search for a solution.",
          "It forces the model to ignore outliers."
        ],
        "correct_index": 1,
        "rationale": "Slide 16 notes that in high dimensions, training data becomes quickly 'non-representative', leading to overfitting on noise.",
        "distractor_analysis": {
          "0": "High dimensionality actually makes training data *less* representative.",
          "1": "Correct. More features provide more 'hooks' to memorize random noise.",
          "2": "The instance space grows exponentially, making the search much harder.",
          "3": "High dimensions typically make models *more* sensitive to specific instance variations."
        }
      },
      {
        "id": 27,
        "topic": "N-fold Cross Validation",
        "difficulty": "application",
        "question": "In a 10-fold cross-validation, how many times is a specific instance used for 'testing' (validation)?",
        "options": [
          "9 times",
          "10 times",
          "1 time",
          "It depends on the weighting of the parameters"
        ],
        "correct_index": 2,
        "rationale": "In N-fold CV, the dataset is split into N folds. Each fold serves as the test set exactly once while the others serve as training.",
        "distractor_analysis": {
          "0": "An instance is used for *training* 9 times in 10-fold CV.",
          "1": "An instance is only in the test set of one specific fold.",
          "2": "Correct. It is tested once to provide an unbiased error estimate.",
          "3": "The error estimation logic (Slide 18) is independent of the split mechanics."
        }
      },
      {
        "id": 28,
        "topic": "Bootstrapping",
        "difficulty": "analysis",
        "question": "For a dataset with 'n' elements, why is the expected fraction of unique instances in a bootstrap training set approximately 0.632 as n goes to infinity?",
        "options": [
          "Because 36.8% of the data is inherently noisy and discarded.",
          "It is the limit of (1 - 1/n)^n subtracted from 1.",
          "Because the model only needs 63.2% of data to converge.",
          "It represents the Golden Ratio of data science sampling."
        ],
        "correct_index": 1,
        "rationale": "Slide 19 shows the math: 1 - lim (1 - 1/n)^n = 1 - 1/e ≈ 0.632.",
        "distractor_analysis": {
          "0": "The 36.8% are simply the instances not selected due to random replacement.",
          "1": "Correct. This is the probability of an element being picked at least once.",
          "2": "There is no theoretical requirement for convergence at this specific percentage.",
          "3": "This is a humorous distractor with no scientific basis."
        }
      },
      {
        "id": 29,
        "topic": "Generalized Error",
        "difficulty": "recall",
        "question": "Generalized error is defined on Slide 21 as the expected value of which other error?",
        "options": [
          "Training error",
          "Test error",
          "Bayes error",
          "Absolute loss"
        ],
        "correct_index": 1,
        "rationale": "Slide 21 explicitly shows: Generalized error = E(test error).",
        "distractor_analysis": {
          "0": "Training error is often biased and not a good proxy for generalization.",
          "1": "Correct. It reflects how the model will perform on the broader population.",
          "2": "Bayes error is a theoretical lower bound, not the model's expected error.",
          "3": "Absolute loss is a specific cost function used to *calculate* error."
        }
      },
      {
        "id": 30,
        "topic": "Loss Functions",
        "difficulty": "recall",
        "question": "Which loss function behaves as an indicator function, returning 1 for misclassification and 0 for correct classification?",
        "options": [
          "Absolute loss",
          "Quadratic loss",
          "0-1 loss",
          "Information loss"
        ],
        "correct_index": 2,
        "rationale": "Slide 24 defines 0-1 loss: l(y, t) = 1 if y ≠ t, and 0 if y = t.",
        "distractor_analysis": {
          "0": "Absolute loss measures the magnitude of the difference (y-t).",
          "1": "Quadratic loss squares the difference, penalizing larger errors more.",
          "2": "Correct. This is the simplest binary error metric.",
          "3": "Information loss uses log-likelihoods."
        }
      },
      {
        "id": 31,
        "topic": "Confusion Matrix",
        "difficulty": "recall",
        "question": "In the confusion matrix, how are 'False Positives' defined?",
        "options": [
          "Actual class is C, predicted is C-bar",
          "Actual class is C-bar, predicted is C",
          "Actual class is C, predicted is C",
          "Actual class is C-bar, predicted is C-bar"
        ],
        "correct_index": 1,
        "rationale": "Slide 26: False Positives (FP) occur when the predicted value is C but the actual value is C-bar.",
        "distractor_analysis": {
          "0": "This is a False Negative.",
          "1": "Correct. The model 'falsely' predicted the 'positive' class C.",
          "2": "This is a True Positive.",
          "3": "This is a True Negative."
        }
      },
      {
        "id": 32,
        "topic": "Recall",
        "difficulty": "analysis",
        "question": "What does Recall (also known as Sensitivity) fundamentally measure?",
        "options": [
          "The accuracy of the positive predictions made by the model.",
          "The model's ability to identify all actual positive instances.",
          "The fraction of negative instances correctly identified.",
          "The trade-off between False Positives and True Negatives."
        ],
        "correct_index": 1,
        "rationale": "Slide 27/28: Recall = TP / (TP + FN). It measures how many of the 'actual' positives were captured.",
        "distractor_analysis": {
          "0": "This describes Precision.",
          "1": "Correct. It's about 'coverage' of the actual class C.",
          "2": "This describes Specificity.",
          "3": "This describes a relationship found in the ROC x-axis, not Recall itself."
        }
      },
      {
        "id": 33,
        "topic": "Specificity",
        "difficulty": "recall",
        "question": "According to the slides, what is the formula for Specificity?",
        "options": [
          "TP / (TP + FN)",
          "TN / (TN + FP)",
          "TP / (TP + FP)",
          "TN / (TN + FN)"
        ],
        "correct_index": 1,
        "rationale": "Slide 28: Specificity(h) = TN / (TN + FP).",
        "distractor_analysis": {
          "0": "This is Recall (Sensitivity).",
          "1": "Correct. It measures the 'purity' of the negative class detection.",
          "2": "This is Precision.",
          "3": "This is an incorrect combination of terms."
        }
      },
      {
        "id": 34,
        "topic": "Precision-Recall Curve",
        "difficulty": "analysis",
        "question": "On a Precision-Recall curve, where is the point of 'better performance' typically located?",
        "options": [
          "Bottom-left (0,0)",
          "Bottom-right (1,0)",
          "Top-left (0,1)",
          "Top-right (1,1)"
        ],
        "correct_index": 3,
        "rationale": "As per the graph on Slide 30, better performance is in the upper right quadrant where both Precision and Recall are 1.",
        "distractor_analysis": {
          "0": "This is the worst performance.",
          "1": "This represents perfect recall but zero precision.",
          "2": "This represents perfect precision but zero recall.",
          "3": "Correct. It represents a model with no false positives and no false negatives."
        }
      },
      {
        "id": 35,
        "topic": "ROC vs PR Curve",
        "difficulty": "analysis",
        "question": "Which of these is ONLY useful if the model's decision threshold can be varied?",
        "options": [
          "F1-measure",
          "Accuracy",
          "Precision-Recall curve",
          "Specificity"
        ],
        "correct_index": 2,
        "rationale": "Slide 33 notes that PR curves and ROC curves are 'For deeper analysis of thresholds'. Point metrics like F1 or Accuracy don't require varying a threshold.",
        "distractor_analysis": {
          "0": "F1 is calculated for a single threshold.",
          "1": "Accuracy is a single point on the confusion matrix.",
          "2": "Correct. A curve is a collection of points generated by threshold variance.",
          "3": "Specificity is a fixed value once the threshold is chosen."
        }
      },
      {
        "id": 36,
        "topic": "Feature Selection Rules",
        "difficulty": "recall",
        "question": "What is 'Rule 2' of feature selection and transformation according to Slide 5?",
        "options": [
          "Get to know your data",
          "Eliminate all categorical features",
          "Make plausible assumptions (e.g., normal distribution)",
          "Always prioritize mutual information"
        ],
        "correct_index": 2,
        "rationale": "Slide 5 lists Rule 1: Get to know your data; Rule 2: Make plausible assumptions.",
        "distractor_analysis": {
          "0": "This is Rule 1.",
          "1": "Categorical features are often essential and should not be eliminated by default.",
          "2": "Correct. Assumptions about distribution help in choosing the right transformation.",
          "3": "Mutual information is a tool, not a fundamental 'rule' of the process."
        }
      },
      {
        "id": 37,
        "topic": "Training Data",
        "difficulty": "application",
        "question": "Which scenario is most likely to lead to 'non-representative' training data?",
        "options": [
          "A dataset where features are independent.",
          "A dataset with low dimensionality and many instances.",
          "A dataset where the instance space grows exponentially relative to samples.",
          "A dataset that follows a Pareto principle for income distribution."
        ],
        "correct_index": 2,
        "rationale": "Slide 16 states that with increasing dimensions, the space grows exponentially, causing training data to become non-representative.",
        "distractor_analysis": {
          "0": "Independence is generally a positive feature trait.",
          "1": "This is the ideal scenario for a representative dataset.",
          "2": "Correct. This is a core part of the Curse of Dimensionality.",
          "3": "The Pareto principle is a valid assumption (Rule 2) and doesn't imply non-representativeness by itself."
        }
      },
      {
        "id": 38,
        "topic": "Model Selection",
        "difficulty": "analysis",
        "question": "If you use Bootstrapping for model selection, how is the total error 'err' calculated to avoid over-optimism?",
        "options": [
          "It is simply the average error on the training set.",
          "It is the error on the 36.8% test set only.",
          "It is a weighted sum of the test error (0.632) and training error (0.368).",
          "It is the sum of the Bayes error and the training error."
        ],
        "correct_index": 2,
        "rationale": "Slide 20 provides the formula: err = 1/m * Σ (0.632 * err_test + 0.368 * err_training).",
        "distractor_analysis": {
          "0": "Training error alone is overly optimistic.",
          "1": "Using only the test error ignores the information in the training split.",
          "2": "Correct. This specific weighting (0.632/0.368) is used to balance the estimation.",
          "3": "Bayes error is unknown and not used in this validation formula."
        }
      },
      {
        "id": 39,
        "topic": "Any-of Classification",
        "difficulty": "application",
        "question": "In an 'Any-of' classification scenario, what is the 'Typical solution' for making decisions across classes?",
        "options": [
          "The decision of one classifier influences the next in a sequence.",
          "The decision of one classifier has no influence on the others.",
          "Classes are merged until only two remain.",
          "The class with the highest probability is chosen, and all others are discarded."
        ],
        "correct_index": 1,
        "rationale": "Slide 13 states for Any-of: 'The decision of one classifier has no influence on the decisions of the other classifiers.'",
        "distractor_analysis": {
          "0": "This describes a hierarchical or chained classifier.",
          "1": "Correct. Since an instance can be in many or no classes, decisions must be independent.",
          "2": "This would change the problem into binary classification.",
          "3": "This describes 'One-of' classification."
        }
      },
      {
        "id": 40,
        "topic": "Discrete Features",
        "difficulty": "analysis",
        "question": "A dataset contains 'House Numbers'. Why are these classified as 'Ordinal' rather than 'Categorical' or 'Continuous' in these slides?",
        "options": [
          "Because they can be ordered but do not have a meaningful scale/distance.",
          "Because you can perform mathematical addition on them.",
          "Because they represent a real-valued height or weight.",
          "Because they have no ordering or scale whatsoever."
        ],
        "correct_index": 0,
        "rationale": "Slide 6 explicitly lists 'house numbers' as Ordinal because they 'can be ordered but do not have a scale'.",
        "distractor_analysis": {
          "0": "Correct. Number 10 is after 8, but the 'distance' between them doesn't represent a physical quantity like 2 meters.",
          "1": "Adding house numbers (10+12=22) usually results in a meaningless value, indicating a lack of scale.",
          "2": "These are continuous features, which house numbers are not.",
          "3": "This describes Categorical features (e.g., Eye Color)."
        }
      },
      {
        "id": 41,
        "topic": "Feature Selection",
        "difficulty": "recall",
        "question": "Which statistical test is mentioned as an example for choosing 'good' features based on independence?",
        "options": [
          "Student's t-test",
          "Chi-square (χ2)-test",
          "F-test",
          "Z-test"
        ],
        "correct_index": 1,
        "rationale": "Slide 5 mentions 'χ2-test of independence' as a tool for feature selection.",
        "distractor_analysis": {
          "0": "t-test is used for comparing model error means (Slide 25).",
          "1": "Correct. It helps determine if a feature is independent of the class.",
          "2": "F-test is for variance comparison, not listed here for selection.",
          "3": "Z-test is for large sample mean comparison, not listed for selection."
        }
      },
      {
        "id": 42,
        "topic": "One-of vs. Any-of",
        "difficulty": "recall",
        "question": "In 'One-of' classification, what does 'k' represent?",
        "options": [
          "The number of instances",
          "The number of features",
          "The number of classes",
          "The number of neighbors"
        ],
        "correct_index": 2,
        "rationale": "Slide 12: 'For k > 2 different classes C1, ..., Ck'.",
        "distractor_analysis": {
          "0": "Instances are denoted by 'x' or 'n'.",
          "1": "Features are usually 'x_i' elements.",
          "2": "Correct. It signifies the cardinality of the label set.",
          "3": "This is specific to the KNN algorithm."
        }
      },
      {
        "id": 43,
        "topic": "Cross Validation",
        "difficulty": "recall",
        "question": "What is 'Leave-one-out cross validation' as defined in the slides?",
        "options": [
          "A method where one feature is removed at a time.",
          "A case of N-fold CV where each fold is a single instance.",
          "A method that ignores the most frequent class.",
          "A technique to remove outliers from the training set."
        ],
        "correct_index": 1,
        "rationale": "Slide 18: 'Leave-one-out cross validation: N-fold cross validation where each fold is a single instance'.",
        "distractor_analysis": {
          "0": "This is 'Leave-one-out feature selection', a different concept.",
          "1": "Correct. If you have 100 instances, you do 100-fold CV.",
          "2": "This is a form of data cleaning or balancing, not LOOCV.",
          "3": "LOOCV uses all data; it doesn't remove outliers permanently."
        }
      },
      {
        "id": 44,
        "topic": "Overfitting",
        "difficulty": "recall",
        "question": "According to Slide 15, which of the following is NOT a listed cause of overfitting?",
        "options": [
          "Small training set",
          "High dimensionality",
          "Non-representative training instances",
          "Too few parameters in the model"
        ],
        "correct_index": 3,
        "rationale": "Too few parameters usually leads to underfitting. The slide lists 'Parameters are set to the best performing values on the training set' (over-optimization) as a cause.",
        "distractor_analysis": {
          "0": "Listed as a primary cause.",
          "1": "Listed as a primary cause.",
          "2": "Listed as a primary cause.",
          "3": "Correct. This is generally associated with underfitting, not overfitting."
        }
      },
      {
        "id": 45,
        "topic": "Error Types",
        "difficulty": "analysis",
        "question": "If a model has zero Training Error but a high Test Error, what is the most likely diagnosis?",
        "options": [
          "The model has reached the Bayes optimal limit.",
          "The model is suffering from the Curse of Dimensionality / Overfitting.",
          "The test set is too small.",
          "The features were not correctly discretized."
        ],
        "correct_index": 1,
        "rationale": "Slide 15/21: A large gap between training performance and test performance is the definition of overfitting.",
        "distractor_analysis": {
          "0": "Bayes error affects both training and test error (you can't go below it).",
          "1": "Correct. It memorized the training data but failed to generalize.",
          "2": "A small test set leads to unreliable error estimates, but a *high* error specifically suggests overfitting.",
          "3": "Incorrect discretization might lower overall performance, but the training/test gap is the hallmark of overfitting."
        }
      },
      {
        "id": 46,
        "topic": "Metric Selection",
        "difficulty": "application",
        "question": "Which measure would you use if you want a single metric that balances both the 'purity' of positive predictions and the 'completeness' of positive captures?",
        "options": [
          "Specificity",
          "F1-measure",
          "Accuracy",
          "AUC"
        ],
        "correct_index": 1,
        "rationale": "Slide 29: F1-measure combines Precision (purity) and Recall (completeness) into one harmonic mean.",
        "distractor_analysis": {
          "0": "Specificity focuses on the negative class only.",
          "1": "Correct. It's the standard for balancing Precision and Recall.",
          "2": "Accuracy treats all types of errors equally and can be misleading.",
          "3": "AUC measures ranking performance across all thresholds, not a single balance of purity/completeness at a fixed point."
        }
      },
      {
        "id": 47,
        "topic": "Decision Thresholds",
        "difficulty": "analysis",
        "question": "What happens to the number of 'False Positives' as you decrease the decision threshold in a classification model?",
        "options": [
          "They stay the same.",
          "They will always decrease.",
          "They will likely increase.",
          "They become True Negatives."
        ],
        "correct_index": 2,
        "rationale": "Decreasing the threshold makes the model more 'lenient', classifying more instances as positive. This increases both TP and FP (Slide 31/32 ROC logic).",
        "distractor_analysis": {
          "0": "Thresholds directly change the count of predicted positives.",
          "1": "Decreasing the threshold increases the volume of positives, increasing error risk.",
          "2": "Correct. More instances cross the lower bar, including incorrect ones.",
          "3": "No, they are predicted as Positives, so they can't be True Negatives."
        }
      },
      {
        "id": 48,
        "topic": "Multi-class Accuracy",
        "difficulty": "recall",
        "question": "How can overall performance be reported for multi-class classification according to the 'one-against-all' summary?",
        "options": [
          "As the minimum precision across all classes.",
          "As a weighted average of metrics across all classes.",
          "By only reporting the accuracy of the most frequent class.",
          "By multiplying the errors of each independent classifier."
        ],
        "correct_index": 1,
        "rationale": "Slide 37: 'Overall performance could be reported as the weighted average of precision, recall, accuracy, error...' based on class proportions.",
        "distractor_analysis": {
          "0": "Minimum precision is a specific 'worst-case' metric, not the standard overall report.",
          "1": "Correct. Weighting by class proportion ensures a fair summary.",
          "2": "This would ignore the model's failure on minority classes.",
          "3": "Multiplication has no logical basis in error reporting here."
        }
      },
      {
        "id": 49,
        "topic": "Feature Ordering",
        "difficulty": "recall",
        "question": "In the 'Features' table (Slide 6), which feature type does NOT support an 'Order' property?",
        "options": [
          "Categorical",
          "Ordinal",
          "Continuous",
          "Discrete"
        ],
        "correct_index": 0,
        "rationale": "The table on Slide 6 has an 'X' under 'Order' for Categorical features.",
        "distractor_analysis": {
          "0": "Correct. Categories (like Colors) have no inherent mathematical ranking.",
          "1": "Ordinal features are defined by their ability to be ordered.",
          "2": "Continuous features are real-valued and ordered.",
          "3": "Discrete is a broad category including Ordinal, which *is* ordered."
        }
      },
      {
        "id": 50,
        "topic": "Metric Suitability",
        "difficulty": "analysis",
        "question": "If your goal is 'deeper analysis of thresholds', which of these measures is explicitly EXCLUDED from that list in the provided slides?",
        "options": [
          "ROC curves",
          "Precision-recall curves",
          "Hypothesis testing",
          "Accuracy"
        ],
        "correct_index": 3,
        "rationale": "Slide 33 lists ROC, PR curves, and Hypothesis testing for 'deeper analysis of thresholds', but lists Accuracy under 'If threshold analysis is not an issue'.",
        "distractor_analysis": {
          "0": "Included in the deeper analysis list.",
          "1": "Included in the deeper analysis list.",
          "2": "Included in the deeper analysis list.",
          "3": "Correct. Accuracy is a static point metric."
        }
      }
    ]
  },
  "foundations-05": {
    "title": "Foundations of AI - Regression",
    "questions": [
  {
    "id": 1,
    "topic": "Regression Fundamentals",
    "difficulty": "recall",
    "question": "In the context of regression methods, what is the primary objective when finding a 'best-fitting curve' for a set of data points?",
    "options": [
      "To exactly interpolate every given data point in the sample",
      "To minimize the discrepancy between the unknown function and the observed values",
      "To maximize the distance between the residuals and the regression function",
      "To eliminate the presence of Gaussian noise from the original dataset"
    ],
    "correct_index": 1,
    "rationale": "Regression aims to find a function that best approximates the underlying trend of data that has been perturbed by noise, minimizing the error between predictions and observations.",
    "distractor_analysis": {
      "0": "Interpolation is about passing through every point, which leads to overfitting in noisy data.",
      "1": "Correct. This is the definition of finding the 'best fit' curve.",
      "2": "We want to minimize residuals, not maximize the distance from the function.",
      "3": "Noise is inherent; we cannot eliminate it from the original data, only model around it."
    }
  },
  {
    "id": 2,
    "topic": "Residuals",
    "difficulty": "application",
    "question": "If a regression model yields a residual (epsilon) of zero for a specific observation, what does this imply about that data point?",
    "options": [
      "The point is an outlier that the model is ignoring",
      "The noise at that point was significantly higher than the Gaussian average",
      "The model's prediction for that instance perfectly matches the observed target value",
      "The feature vector for that point contains only zero values"
    ],
    "correct_index": 2,
    "rationale": "Residuals are defined as the difference between the actual observed value and the predicted value. A zero residual means no error.",
    "distractor_analysis": {
      "0": "Outliers typically result in very large residuals.",
      "1": "Higher noise would likely lead to a larger residual if the model is smooth.",
      "2": "Correct. Residual = Target - Prediction; zero means they are equal.",
      "3": "The feature values do not determine the residual; the accuracy of the prediction does."
    }
  },
  {
    "id": 3,
    "topic": "Error Functions",
    "difficulty": "analysis",
    "question": "What do Sum-of-Squares Error (SSE), Mean Squared Error (MSE), and Root-Mean-Square Error (RMSE) fundamentally have in common?",
    "options": [
      "They all treat positive and negative errors with the same magnitude of penalty",
      "They all provide error values in the same unit as the target variable",
      "They all decrease linearly as the number of observations (n) increases",
      "They all remain unaffected by the presence of large outliers"
    ],
    "correct_index": 0,
    "rationale": "All three are based on squaring the residuals, which ensures that negative and positive differences are treated as positive penalties.",
    "distractor_analysis": {
      "0": "Correct. Squaring removes the sign, making -5 and +5 error equal.",
      "1": "SSE and MSE use squared units; only RMSE returns to the original unit.",
      "2": "SSE typically increases with n, while MSE and RMSE average it out.",
      "3": "Squared errors are highly sensitive to outliers because the squaring amplifies large differences."
    }
  },
  {
    "id": 4,
    "topic": "Coefficient of Determination",
    "difficulty": "recall",
    "question": "The Coefficient of Determination (R²) is used to evaluate model quality. What does it represent on a variance basis?",
    "options": [
      "The ratio of feature variance to noise variance",
      "How well the observed outcomes are replicated by the model compared to the mean",
      "The absolute average distance of points from the regression line",
      "The probability that the next data point will fall on the regression line"
    ],
    "correct_index": 1,
    "rationale": "R² measures the proportion of variance in the dependent variable that is predictable from the independent variable(s) relative to a baseline mean model.",
    "distractor_analysis": {
      "0": "This describes a signal-to-noise ratio, not R².",
      "1": "Correct. It compares the model's residuals to the total sum of squares (variance from the mean).",
      "2": "This describes Mean Absolute Error (MAE).",
      "3": "R² is a summary statistic of fit, not a predictive probability of exact intersection."
    }
  },
  {
    "id": 5,
    "topic": "Univariate Linear Regression",
    "difficulty": "application",
    "question": "In a univariate linear regression model (g = w0 + w1h + epsilon), what happens to the regression line if the slope coefficient w1 is zero?",
    "options": [
      "The line becomes vertical, passing through the mean of h",
      "The line becomes a horizontal constant at the value of the intercept w0",
      "The model predicts a value of zero for all inputs regardless of h",
      "The residuals are eliminated because the model is perfectly flat"
    ],
    "correct_index": 1,
    "rationale": "If w1 (the slope) is zero, the term w1h disappears, leaving g = w0, which is a horizontal line.",
    "distractor_analysis": {
      "0": "A vertical line would mean an undefined slope, not zero.",
      "1": "Correct. No change in h affects g, so it stays at the intercept.",
      "2": "This only happens if w0 is also zero.",
      "3": "Residuals would actually likely increase if there is any relationship between h and g."
    }
  },
  {
    "id": 6,
    "topic": "Multivariate Linear Regression",
    "difficulty": "analysis",
    "question": "In the general form of multivariate linear regression (t = Xw + epsilon), how does the 'X' matrix change as we add more features for each observation?",
    "options": [
      "The number of rows increases, representing a larger sample size",
      "The number of columns increases, where each column represents a new feature",
      "The matrix becomes a vector to simplify the dot product with w",
      "The matrix values are normalized to ensure the noise vector remains constant"
    ],
    "correct_index": 1,
    "rationale": "In the matrix X (n x m), n is the number of observations and m is the number of features. Adding features adds columns.",
    "distractor_analysis": {
      "0": "Adding observations increases rows, not adding features.",
      "1": "Correct. Each feature is represented by a column in the design matrix.",
      "2": "X must remain a matrix to handle multiple features and observations simultaneously.",
      "3": "While normalization is common, it is not an inherent structural change caused by adding features."
    }
  },
  {
    "id": 7,
    "topic": "Correlation between Features",
    "difficulty": "analysis",
    "question": "According to the slides, what is a consequence of having highly correlated features in a dataset for multivariate regression?",
    "options": [
      "The sample provides much less information about the true underlying function",
      "The model becomes computationally simpler because it can ignore redundant features",
      "The R² value automatically reaches 1.0 due to feature synergy",
      "The residuals are guaranteed to follow a non-Gaussian distribution"
    ],
    "correct_index": 0,
    "rationale": "When features are highly correlated, it is difficult for the model to distinguish their individual contributions, leading to numerical instability and less information gain.",
    "distractor_analysis": {
      "0": "Correct. Redundancy leads to 'thin' data coverage in the feature space.",
      "1": "Correlation often makes the solution more sensitive and unstable (ill-conditioned), not simpler.",
      "2": "Correlation does not guarantee high accuracy; it can actually mask errors.",
      "3": "Noise distribution is an assumption of the data generation process, not a result of feature correlation."
    }
  },
  {
    "id": 8,
    "topic": "Regularization Logic",
    "difficulty": "application",
    "question": "Why would an instructional designer describe regularization as 'making training a little bit harder'?",
    "options": [
      "It requires more computational power to calculate the gradient",
      "It adds a penalty term that prevents the model from perfectly fitting the training data",
      "It increases the amount of noise in the training set to test model robustness",
      "It forces the use of a smaller dataset to prevent the model from seeing too many patterns"
    ],
    "correct_index": 1,
    "rationale": "Regularization adds a constraint (the norm of the weights) to the loss function, preventing the model from chasing every small variance in the training data.",
    "distractor_analysis": {
      "0": "The math is slightly more complex, but the 'hardness' refers to the optimization constraint.",
      "1": "Correct. The model must balance low error with small weight magnitudes.",
      "2": "Regularization does not add noise; it constrains the model's reaction to existing noise.",
      "3": "Regularization allows you to use all your data while still preventing overfitting."
    }
  },
  {
    "id": 9,
    "topic": "Ridge vs Lasso",
    "difficulty": "analysis",
    "question": "Comparing Ridge (L2) and Lasso (L1) regularization, what is the specific effect of Lasso on the weight vector (w)?",
    "options": [
      "It shrinks all weights equally toward zero without ever reaching it",
      "It favors 'sparser' solutions by setting some weight coefficients exactly to zero",
      "It increases the weights of the most important features to counteract noise",
      "It is less sensitive to the regularization parameter (lambda) than Ridge"
    ],
    "correct_index": 1,
    "rationale": "Lasso's diamond-shaped constraint space often intersects the loss contours at the axes, resulting in zero-valued coefficients.",
    "distractor_analysis": {
      "0": "This describes Ridge (L2), which shrinks weights asymptotically.",
      "1": "Correct. Sparser solutions are the hallmark of L1 regularization.",
      "2": "Regularization always penalizes weight magnitude; it never encourages growth.",
      "3": "The slides explicitly state Lasso is more sensitive to the choice of lambda."
    }
  },
  {
    "id": 10,
    "topic": "Polynomial Curve Fitting",
    "difficulty": "application",
    "question": "If you increase the degree 'M' of a polynomial curve fitting model while keeping the dataset size 'N' small, what is the most likely outcome?",
    "options": [
      "The model will achieve a better generalization to new, unseen data",
      "The model will experience 'overfitting', capturing noise as if it were a true pattern",
      "The R² value will decrease significantly on the training set",
      "The basis functions will become linearly dependent, preventing a solution"
    ],
    "correct_index": 1,
    "rationale": "High degree polynomials have high flexibility. With small N, the model uses this flexibility to pass through noisy points, which is overfitting.",
    "distractor_analysis": {
      "0": "Overfitted models generalize poorly.",
      "1": "Correct. This is the classic trade-off shown in the slides (M=9 vs M=3).",
      "2": "R² on the training set usually increases (it fits the noise), even if the model is bad.",
      "3": "Basis functions remain independent, but the model becomes poorly conditioned."
    }
  },
  {
    "id": 11,
    "topic": "Stochastic Gradient Descent (SGD)",
    "difficulty": "recall",
    "question": "What is the primary difference between standard Gradient Descent and Stochastic Gradient Descent (SGD) in the context of the training data used per step?",
    "options": [
      "SGD uses the entire dataset to compute one gradient update",
      "SGD approximates the gradient using only one or a small subset of data points",
      "SGD only updates the intercept w0 and ignores the slope coefficients",
      "SGD requires the loss function to be non-differentiable"
    ],
    "correct_index": 1,
    "rationale": "SGD updates parameters per data point (or minibatch) rather than calculating the average gradient of the whole dataset.",
    "distractor_analysis": {
      "0": "This is Batch Gradient Descent.",
      "1": "Correct. The slide shows the approximation symbol (≈) using one point (xi, ti).",
      "2": "SGD updates all parameters in the weight vector w.",
      "3": "Gradient descent requires differentiability to calculate the gradient."
    }
  },
  {
    "id": 12,
    "topic": "SGD Hyperparameters",
    "difficulty": "application",
    "question": "In the optimization landscape, how does a 'Big learning rate' typically manifest in the loss curve over time?",
    "options": [
      "The loss decreases very slowly and smoothly toward the global minimum",
      "The loss may oscillate or even increase, as the updates 'overshoot' the minimum",
      "The loss curve becomes perfectly flat because the updates are too large to be registered",
      "The loss reaches the minimum instantly but then remains stuck there forever"
    ],
    "correct_index": 1,
    "rationale": "A high learning rate creates large steps. If the step size is larger than the 'valley' of the loss function, the model jumps over the minimum to a higher error point.",
    "distractor_analysis": {
      "0": "This describes a 'Small learning rate'.",
      "1": "Correct. Overshooting leads to oscillation or divergence.",
      "2": "The curve would be erratic, not flat.",
      "3": "Instant convergence is rare, and high rates usually prevent staying at the minimum."
    }
  },
  {
    "id": 13,
    "topic": "Forward Step-wise Regression",
    "difficulty": "recall",
    "question": "What is the core logic behind 'Step 2' in Forward Step-wise Regression?",
    "options": [
      "To restart the training from scratch using a different basis function",
      "To fit a second simple model to the 'residuals' (errors) of the first model",
      "To multiply the weights of the first model by the weights of the second",
      "To remove the most important feature from the first step to test robustness"
    ],
    "correct_index": 1,
    "rationale": "Forward step-wise regression is additive. Each new step attempts to explain the error left behind by the previous steps.",
    "distractor_analysis": {
      "0": "It builds upon the previous step, it doesn't restart.",
      "1": "Correct. The target for step 2 is (ti - y1).",
      "2": "Models are added, not multiplied.",
      "3": "Features are added in this method, not removed (that would be backward elimination)."
    }
  },
  {
    "id": 14,
    "topic": "Regression Use Cases",
    "difficulty": "application",
    "question": "In which of the following scenarios is a regression method most appropriate?",
    "options": [
      "Classifying an image as either a 'dog' or a 'cat'",
      "Predicting the exact price of a house based on its square footage and location",
      "Clustering a set of customers into groups based on shopping habits",
      "Sorting a list of names into alphabetical order"
    ],
    "correct_index": 1,
    "rationale": "Regression is used for predicting continuous, scalar output values (target variables).",
    "distractor_analysis": {
      "0": "This is classification (discrete labels).",
      "1": "Correct. Price is a continuous numeric target.",
      "2": "This is unsupervised clustering.",
      "3": "This is a basic algorithmic sorting task, not a statistical learning problem."
    }
  },
  {
    "id": 15,
    "topic": "Multivariate Least Squares",
    "difficulty": "analysis",
    "question": "When solving multivariate least squares, the solution involves (XTX)⁻¹. What is a geometric function of this term according to the slides?",
    "options": [
      "It projects the noise vector onto a higher dimensional space",
      "It decorrelates, centers, and normalizes the features",
      "It calculates the probability of the target variable being Gaussian",
      "It maximizes the distance between the training and testing sets"
    ],
    "correct_index": 1,
    "rationale": "The inverse of the covariance matrix (related to XTX) effectively 'whitens' the data, removing correlations and scaling the features.",
    "distractor_analysis": {
      "0": "It relates to the projection of t onto the column space of X, not the noise vector alone.",
      "1": "Correct. This is a key insight from the 'General solution' slide.",
      "2": "Gaussianity is an assumption, not something calculated by this matrix.",
      "3": "The solution only focuses on the training set (X and t)."
    }
  },
  {
    "id": 16,
    "topic": "Regression Terminology",
    "difficulty": "recall",
    "question": "In the terminology provided, what does 'x_i' represent?",
    "options": [
      "The scalar target variable or label",
      "The input variables or features",
      "The regression function itself",
      "The standard deviation of the noise"
    ],
    "correct_index": 1,
    "rationale": "The slides define x_i as the input variable, often called a feature, independent variable, or regressor.",
    "distractor_analysis": {
      "0": "This is 't_i'.",
      "1": "Correct.",
      "2": "This is 'f' or 'y'.",
      "3": "This is usually 'sigma'."
    }
  },
  {
    "id": 17,
    "topic": "Polynomial Curve Fitting",
    "difficulty": "analysis",
    "question": "Comparing M=0 and M=9 in polynomial curve fitting for a sine wave (as shown in slides), why is M=0 often considered a poor model?",
    "options": [
      "It is too flexible and overfits the noise at every point",
      "It represents a constant value and fails to capture any of the data's periodicity",
      "It creates a vertical line that cannot be solved via least squares",
      "It requires too much data (N=100) to reach a stable solution"
    ],
    "correct_index": 1,
    "rationale": "A degree M=0 polynomial is a horizontal line. It has zero flexibility (high bias), making it 'underfit' a periodic sine wave.",
    "distractor_analysis": {
      "0": "This describes high M (like M=9).",
      "1": "Correct. It is a simple flat line (w0) which cannot follow a curve.",
      "2": "A degree 0 polynomial is a horizontal line, not vertical.",
      "3": "Actually, it is very stable with little data; it's just inaccurate."
    }
  },
  {
    "id": 18,
    "topic": "Basis Functions",
    "difficulty": "recall",
    "question": "How do Gaussian basis functions differ from Polynomial basis functions in their sensitivity to changes in input 'x'?",
    "options": [
      "Polynomial functions only affect nearby points, while Gaussian functions affect all points",
      "Gaussian functions only affect nearby points, while Polynomial functions affect all points",
      "Both are globally sensitive and change identical amounts across the entire range",
      "Polynomial functions are only used for classification, while Gaussian are for regression"
    ],
    "correct_index": 1,
    "rationale": "Gaussian basis functions are 'local'—their value drops off rapidly away from their center. Polynomials (like x^n) increase globally as x increases.",
    "distractor_analysis": {
      "0": "This is the exact opposite of the truth.",
      "1": "Correct. This is explicitly noted in the 'Choices for basis functions' slide.",
      "2": "They have very different local/global behaviors.",
      "3": "Both are widely used in regression curve fitting."
    }
  },
  {
    "id": 19,
    "topic": "SGD and Loss Curves",
    "difficulty": "application",
    "question": "During training with SGD, you notice the loss curve is decreasing but extremely slowly, appearing almost flat. What is the most logical adjustment?",
    "options": [
      "Decrease the learning rate to prevent overshooting",
      "Increase the learning rate to take larger steps toward the minimum",
      "Add more features to the dataset to make the gradient steeper",
      "Switch to Lasso regularization to eliminate noise"
    ],
    "correct_index": 1,
    "rationale": "A slow, flat descent usually indicates the learning rate is too small, meaning the updates to the weights are too tiny to make progress in reasonable time.",
    "distractor_analysis": {
      "0": "Decreasing it would make the already slow process even slower.",
      "1": "Correct. Larger steps are needed to 'move' down the gradient.",
      "2": "You should fix the optimizer first before changing the data structure.",
      "3": "Regularization helps with overfitting, not with the speed of convergence in a flat landscape."
    }
  },
  {
    "id": 20,
    "topic": "Multivariate Factors",
    "difficulty": "recall",
    "question": "In the general form of multivariate linear regression, what does the weight vector 'w' represent?",
    "options": [
      "The noise associated with each observation",
      "A specific weight or importance coefficient for each individual feature",
      "The average value of all target variables in the dataset",
      "The number of instances (n) divided by the number of features (m)"
    ],
    "correct_index": 1,
    "rationale": "In the vector w, each element w_j corresponds to the coefficient assigned to the j-th feature in the prediction sum.",
    "distractor_analysis": {
      "0": "Noise is represented by epsilon.",
      "1": "Correct. It defines the 'slope' for each feature dimension.",
      "2": "This would be a single scalar value, not a vector.",
      "3": "This is a ratio of dimensions, not the weights."
    }
  },
  {
    "id": 21,
    "topic": "Error Function Commonalities",
    "difficulty": "analysis",
    "question": "If we want to minimize the likelihood of our model being skewed by a single massive outlier, why might 'squared' error functions (MSE/SSE) be problematic?",
    "options": [
      "They ignore large errors entirely to focus on the mean",
      "They penalize large errors disproportionately more than small errors",
      "They only work if the dataset contains exactly zero noise",
      "They are computationally impossible to solve for multivariate cases"
    ],
    "correct_index": 1,
    "rationale": "Because the error is squared, an outlier with a distance of 10 has a penalty of 100, while an error of 1 has a penalty of 1. This 'pulls' the model toward the outlier.",
    "distractor_analysis": {
      "0": "They do the opposite; they are very sensitive to large errors.",
      "1": "Correct. Squaring 'inflates' the impact of outliers.",
      "2": "They are specifically designed to handle noisy data.",
      "3": "They are the standard choice for multivariate solutions (Normal Equation)."
    }
  },
  {
    "id": 22,
    "topic": "Coefficient of Determination",
    "difficulty": "application",
    "question": "A researcher finds an R² value of 0.98 for Model A and 0.57 for Model B on the same dataset. What can be concluded about Model A?",
    "options": [
      "Model A is 98% more likely to be correct in its future predictions",
      "Model A replicates the variance of the observed outcomes much better than Model B",
      "Model A uses 98 features, while Model B only uses 57 features",
      "Model A has found a linear relationship, while Model B is definitely a curve"
    ],
    "correct_index": 1,
    "rationale": "R² is a measure of fit quality based on variance. Higher R² means the model explains more of the data's variability.",
    "distractor_analysis": {
      "0": "R² is not a probability of correctness; it's a measure of fit.",
      "1": "Correct. This relates directly to the 'R² = 98.92% vs 57.13%' visual in the slides.",
      "2": "R² and the number of features are not directly tied in that way.",
      "3": "Both could be linear; one just fits the specific data much better."
    }
  },
  {
    "id": 23,
    "topic": "Forward Step-wise Regression",
    "difficulty": "analysis",
    "question": "When should one stop the 'Forward Step-wise Regression' process according to the logic presented?",
    "options": [
      "After exactly two steps, regardless of the error",
      "When no significant improvement in training error is made by adding another model",
      "When the weights w1 and w2 become equal in magnitude",
      "Only when the training error reaches exactly zero"
    ],
    "correct_index": 1,
    "rationale": "The goal is to add complexity only as long as it provides a meaningful reduction in residuals.",
    "distractor_analysis": {
      "0": "The number of steps depends on the complexity of the data.",
      "1": "Correct. This is the stopping criterion defined in the slides.",
      "2": "Weight magnitudes don't determine the stopping point; error reduction does.",
      "3": "Reaching zero error usually implies massive overfitting, not a healthy stopping point."
    }
  },
  {
    "id": 24,
    "topic": "Univariate Linear Regression",
    "difficulty": "recall",
    "question": "In the height vs. weight example, what is the 'intercept' (w0) representing conceptually?",
    "options": [
      "The weight of an individual with zero height",
      "The rate at which weight increases per centimeter of height",
      "The average height of all participants in the study",
      "The total amount of noise in the measurements"
    ],
    "correct_index": 0,
    "rationale": "In y = mx + b, 'b' (or w0) is the value of y when x = 0.",
    "distractor_analysis": {
      "0": "Correct. Even if physically impossible, this is the mathematical intercept.",
      "1": "This is the slope (w1).",
      "2": "This is h-bar (the empirical mean).",
      "3": "Noise is a separate term (epsilon), not a constant intercept."
    }
  },
  {
    "id": 25,
    "topic": "Regularization",
    "difficulty": "analysis",
    "question": "According to the slides, what is a limiting case for Lp-regularization where no sparse solutions are achieved?",
    "options": [
      "When p is less than 1 (e.g., p=0.5)",
      "When p is exactly 1 (Lasso)",
      "When p is greater than 1 (e.g., p=2 or p=4)",
      "When the learning rate is set to zero"
    ],
    "correct_index": 2,
    "rationale": "Sparsity is a property of the 'sharpness' of the constraint at the axes. This only occurs when p ≤ 1.",
    "distractor_analysis": {
      "0": "p=0.5 actually produces extremely sparse/sharp solutions.",
      "1": "p=1 is the Lasso, which is the most famous sparse regularizer.",
      "2": "Correct. The 'Further considerations' slide explicitly states p > 1 yields no sparse solutions.",
      "3": "The learning rate is an optimization parameter, not a property of the regularization function."
    }
  },
  {
    "id": 26,
    "topic": "Effect of Correlation",
    "difficulty": "application",
    "question": "If features x1 and x2 are almost identical (high correlation), how does the 'learned plane' in multivariate regression behave compared to the 'true plane'?",
    "options": [
      "It becomes perfectly stable because the redundancy reinforces the signal",
      "It may tilt erratically because the model cannot distinguish which feature to assign weight to",
      "It automatically ignores one feature and assigns all weight to the other",
      "It creates a 4D projection that simplifies back to univariate regression"
    ],
    "correct_index": 1,
    "rationale": "High correlation makes the (XTX) matrix nearly singular (ill-conditioned), making the weight estimates highly sensitive to small noise fluctuations.",
    "distractor_analysis": {
      "0": "Redundancy in features is generally negative for model stability.",
      "1": "Correct. This is illustrated by the 'Effect of correlation' planes in the slides.",
      "2": "This only happens if you specifically use Lasso (L1); standard least squares will struggle.",
      "3": "Dimensions are based on feature count; correlation doesn't change the dimensionality."
    }
  },
  {
    "id": 27,
    "topic": "Ridge vs Lasso",
    "difficulty": "recall",
    "question": "Which regularization technique is described as being 'much more sensitive to the choice of lambda'?",
    "options": [
      "Ridge (L2)",
      "Lasso (L1)",
      "Polynomial Curve Fitting",
      "Stochastic Gradient Descent"
    ],
    "correct_index": 1,
    "rationale": "The slides explicitly state that 'Lasso regression is much more sensitive to the choice of lambda'.",
    "distractor_analysis": {
      "0": "Ridge is generally more robust to lambda variations.",
      "1": "Correct.",
      "2": "This is a model type, not a regularization technique.",
      "3": "This is an optimization algorithm."
    }
  },
  {
    "id": 28,
    "topic": "SGD Interaction",
    "difficulty": "analysis",
    "question": "How does a 'Small learning rate' interact with the optimization landscape compared to a 'Big learning rate'?",
    "options": [
      "It reaches the minimum faster but with higher final error",
      "It follows the gradient more precisely but takes much longer to converge",
      "It ignores the gradient entirely and moves in random directions",
      "It only works if the loss function is perfectly linear"
    ],
    "correct_index": 1,
    "rationale": "A small learning rate takes tiny, cautious steps. It is reliable but slow.",
    "distractor_analysis": {
      "0": "Small rates are slower by definition.",
      "1": "Correct. This is shown in the 'Small learning rate' vs 'Big learning rate' diagrams.",
      "2": "It still follows the gradient; it just does so slowly.",
      "3": "It works on any differentiable surface, not just linear ones."
    }
  },
  {
    "id": 29,
    "topic": "General Multivariate Factors",
    "difficulty": "recall",
    "question": "In the general form of multivariate linear regression, what is 'epsilon' defined as?",
    "options": [
      "The matrix of feature vectors",
      "The noise vector",
      "The identity matrix",
      "The vector of target variables"
    ],
    "correct_index": 1,
    "rationale": "Epsilon (ε) represents the residuals or noise vector in the equation t = Xw + ε.",
    "distractor_analysis": {
      "0": "This is 'X'.",
      "1": "Correct.",
      "2": "This is 'I'.",
      "3": "This is 't'."
    }
  },
  {
    "id": 30,
    "topic": "Linear Regression for Classification",
    "difficulty": "analysis",
    "question": "The 'Outlook' slide mentions using linear regression for classification. What is the 'goal' in this context?",
    "options": [
      "To find the exact weight of a fruit (e.g., a pear)",
      "To learn a weight vector 'w*' for a decision boundary",
      "To calculate the total volume of all apples in a dataset",
      "To ensure that all data points fall exactly on a single line"
    ],
    "correct_index": 1,
    "rationale": "For classification, we don't want to predict a continuous value but rather find a boundary that separates different classes (e.g., apples vs. pears).",
    "distractor_analysis": {
      "0": "Finding weight is a regression task, not classification.",
      "1": "Correct. The goal is to define the decision boundary (w* · x = b).",
      "2": "This is a regression/summation task.",
      "3": "Points in classification are usually spread out in groups, not on a line."
    }
  },
  {
    "id": 31,
    "topic": "Polynomial Curve Fitting",
    "difficulty": "analysis",
    "question": "When considering 'Larger sample vs Regularization', what is the primary benefit of having a larger dataset (N=100) even with a high-degree polynomial (M=9)?",
    "options": [
      "The model becomes simpler and reduces M automatically",
      "The additional data points 'constrain' the model, reducing the likelihood of fitting noise",
      "The R² value is guaranteed to decrease to zero",
      "The noise vector 'epsilon' is completely eliminated by the large sample size"
    ],
    "correct_index": 1,
    "rationale": "More data points provide a clearer signal of the underlying function, making it harder for a complex model to 'wiggle' through noise without hitting other data points.",
    "distractor_analysis": {
      "0": "M is a hyperparameter chosen by the user; it doesn't change itself.",
      "1": "Correct. This is a key comparison in the 'Impact of data and regularization' slide.",
      "2": "R² should remain high if the fit is good.",
      "3": "Noise exists regardless of sample size; we just estimate the function better."
    }
  },
  {
    "id": 32,
    "topic": "Error Functions",
    "difficulty": "recall",
    "question": "Which error function is specifically defined as the square root of the Mean Squared Error?",
    "options": [
      "Sum-of-squares error",
      "Root-mean-square error (RMSE)",
      "Coefficient of determination",
      "Residual standard deviation"
    ],
    "correct_index": 1,
    "rationale": "RMSE = sqrt(MSE). It is often preferred because it has the same units as the target variable.",
    "distractor_analysis": {
      "0": "SSE is the raw sum, not averaged or rooted.",
      "1": "Correct.",
      "2": "R² is a ratio of variances, not a direct error magnitude.",
      "3": "While related, RMSE is the specific formula mentioned in the error functions slide."
    }
  },
  {
    "id": 33,
    "topic": "Coefficient of Determination",
    "difficulty": "analysis",
    "question": "If a model has an R² value of 0, what does this imply about its predictive power?",
    "options": [
      "The model is perfect and has zero residuals",
      "The model performs no better than simply predicting the mean of the data for every input",
      "The model has exactly one outlier that is ruining the entire fit",
      "The model is using the wrong unit of measurement for height"
    ],
    "correct_index": 1,
    "rationale": "R² compares the model's error to the variance of the data (error of the mean). R² = 0 means the model error equals the total variance.",
    "distractor_analysis": {
      "0": "A perfect model has R² = 1.",
      "1": "Correct. It means the features provide zero explanatory power beyond the baseline mean.",
      "2": "A single outlier usually lowers R² but rarely brings it to exactly zero unless the dataset is tiny.",
      "3": "R² is a dimensionless ratio; units cancel out."
    }
  },
  {
    "id": 34,
    "topic": "Univariate Linear Regression",
    "difficulty": "application",
    "question": "In the provided Least-squares method for height and weight, what are we attempting to minimize?",
    "options": [
      "The sum of the heights of all study participants",
      "The sum of the squared differences between observed weights and predicted weights",
      "The number of features used in the multivariate matrix",
      "The learning rate used in the SGD optimizer"
    ],
    "correct_index": 1,
    "rationale": "The goal of least squares is to minimize the sum of the squared residuals.",
    "distractor_analysis": {
      "0": "Height is an input (h), not a value to be minimized.",
      "1": "Correct. This is the definition of the cost function on slide 8.",
      "2": "This example is univariate, so there is only one feature.",
      "3": "The learning rate is a tool to achieve the minimum, not the value being minimized itself."
    }
  },
  {
    "id": 35,
    "topic": "Multivariate Linear Regression",
    "difficulty": "analysis",
    "question": "What is the role of the identity matrix (I) in the solution for Regularized Multivariate Regression (w = (XTX + lambda I)⁻¹ XTt)?",
    "options": [
      "It ensures that the matrix is always invertible by adding values to the diagonal",
      "It multiplies all target variables by 1 to prevent data loss",
      "It converts the weight vector into a scalar value",
      "It removes the intercept w0 from the final equation"
    ],
    "correct_index": 0,
    "rationale": "Adding lambda*I to XTX is the mathematical implementation of Ridge regression. It prevents the matrix from being singular (non-invertible) if features are perfectly correlated.",
    "distractor_analysis": {
      "0": "Correct. This is a primary numerical benefit of Ridge regression.",
      "1": "The identity matrix does not interact with 't' in that way in this term.",
      "2": "w remains a vector.",
      "3": "The intercept is typically included in the weights; the identity matrix doesn't 'remove' it."
    }
  },
  {
    "id": 36,
    "topic": "Regularization Norms",
    "difficulty": "recall",
    "question": "Which regularization norm is associated with 'Ridge' regression?",
    "options": [
      "L1 norm (sum of absolute weights)",
      "L2 norm (sum of squared weights)",
      "L-infinity norm (maximum weight)",
      "L0 norm (count of non-zero weights)"
    ],
    "correct_index": 1,
    "rationale": "Ridge regression uses the L2 norm penalty.",
    "distractor_analysis": {
      "0": "This is Lasso.",
      "1": "Correct.",
      "2": "Not discussed in the context of these slides.",
      "3": "This is the theoretical ideal for sparsity but not what Ridge uses."
    }
  },
  {
    "id": 37,
    "topic": "Stochastic Gradient Descent",
    "difficulty": "application",
    "question": "In the SGD rule (w' = w - eta * grad), if the gradient is zero, what happens to the weight vector?",
    "options": [
      "The weight vector is reset to zero",
      "The weight vector remains unchanged, indicating a local or global minimum has been reached",
      "The weight vector doubles in size to try and find a new gradient",
      "The learning rate eta automatically increases to compensate"
    ],
    "correct_index": 1,
    "rationale": "If grad = 0, then w' = w - 0, so w' = w. This is the definition of a stationary point in optimization.",
    "distractor_analysis": {
      "0": "Only the change is zero, not the vector itself.",
      "1": "Correct. No gradient means no direction to move.",
      "2": "There is no 'doubling' logic in the standard SGD formula.",
      "3": "Eta is a hyperparameter and does not self-adjust based on the gradient in the basic formula."
    }
  },
  {
    "id": 38,
    "topic": "Forward Step-wise Regression",
    "difficulty": "analysis",
    "question": "Why would one prefer Forward Step-wise Regression over a single complex Multivariate Regression?",
    "options": [
      "It is guaranteed to be 100 times faster to compute",
      "It allows for building a model incrementally, which can be easier to interpret and control",
      "It uses Gaussian noise to its advantage to find better patterns",
      "It is the only method that works for datasets with more than 10 features"
    ],
    "correct_index": 1,
    "rationale": "Step-wise regression is an iterative approach that can help identify the most impactful components of a model one by one.",
    "distractor_analysis": {
      "0": "It is often slower because it requires multiple training steps.",
      "1": "Correct. Incrementality provides an 'auditable' path of improvement.",
      "2": "Noise is always a nuisance; no method 'prefers' it.",
      "3": "Multivariate least squares handles many features efficiently via matrix math."
    }
  },
  {
    "id": 39,
    "topic": "General Formulation",
    "difficulty": "recall",
    "question": "In curve fitting, what are functions like 'phi_j(x)' called?",
    "options": [
      "Target functions",
      "Basis functions",
      "Error functions",
      "Residual functions"
    ],
    "correct_index": 1,
    "rationale": "The slides explicitly identify phi_j(x) as 'basis functions'.",
    "distractor_analysis": {
      "0": "The target function is what we are trying to estimate (f).",
      "1": "Correct.",
      "2": "Error functions measure the quality (e.g., MSE).",
      "3": "Residuals are the differences (epsilon)."
    }
  },
  {
    "id": 40,
    "topic": "Effect of Correlation",
    "difficulty": "analysis",
    "question": "According to the visual 'Effect of correlation between features', a 'Blue plane' is learned by decomposing the problem into two univariate regressions. How does this compare to the 'Green plane' (multivariate)?",
    "options": [
      "The Blue plane is always more accurate because it is simpler",
      "The Blue plane may fail to capture the true relationship if features are correlated, unlike the Green plane",
      "The Green plane is only used when the data points are red dots",
      "There is no difference between them as long as the learning rate is small"
    ],
    "correct_index": 1,
    "rationale": "Univariate decomposition ignores the interactions/correlations between features that the multivariate model (Green plane) can account for.",
    "distractor_analysis": {
      "0": "Decomposition is usually less accurate because it loses context.",
      "1": "Correct. This is a core point of the comparison on slide 19.",
      "2": "The color of the dots in the diagram is just for visualization.",
      "3": "The underlying mathematical assumptions are different; learning rate won't bridge that gap."
    }
  },
  {
    "id": 41,
    "topic": "Lasso Regularization",
    "difficulty": "application",
    "question": "You are working on a genomics dataset with 10,000 features, but you suspect only 50 are actually relevant. Which technique should you use?",
    "options": [
      "Standard Least Squares Regression",
      "Ridge Regularization",
      "Lasso Regularization",
      "Forward Step-wise Regression with M=9"
    ],
    "correct_index": 2,
    "rationale": "Lasso encourages sparsity, meaning it will likely set the weights of the 9,950 irrelevant features to zero, effectively performing feature selection.",
    "distractor_analysis": {
      "0": "Standard least squares will overfit by trying to use all 10,000 features.",
      "1": "Ridge will keep all features, just with small weights.",
      "2": "Correct. Sparsity is the key here.",
      "3": "Step-wise could work but is computationally inefficient for 10,000 features compared to Lasso."
    }
  },
  {
    "id": 42,
    "topic": "SGD Hyperparameters",
    "difficulty": "analysis",
    "question": "If you are training an SGD model and the loss curve is oscillating wildly up and down, what is the 'theoretical logic' for your next move?",
    "options": [
      "Increase the number of features to stabilize the plane",
      "Decrease the learning rate because the current 'steps' are jumping across the optimal valley",
      "Switch to a polynomial model with a higher degree M",
      "Assume the data is perfectly clean and remove all regularization"
    ],
    "correct_index": 1,
    "rationale": "Oscillation is a classic sign of a learning rate that is too high, preventing the model from settling into the minimum.",
    "distractor_analysis": {
      "0": "More features might actually increase instability.",
      "1": "Correct. Smaller steps allow the model to descend into the valley.",
      "2": "Higher complexity might make oscillation worse.",
      "3": "Wild oscillation is an optimization issue, not necessarily a noise issue."
    }
  },
  {
    "id": 43,
    "topic": "Multivariate Solution",
    "difficulty": "recall",
    "question": "What is the computational complexity of computing the inverse matrix (XTX)⁻¹ in a multivariate regression?",
    "options": [
      "Linear: O(n)",
      "Polynomial: O(n²m + m³)",
      "Exponential: O(2^n)",
      "Logarithmic: O(log n)"
    ],
    "correct_index": 1,
    "rationale": "The slides explicitly mention the 'Caution' that the complexity of computing (XTX)⁻¹ is O(n²m + m³).",
    "distractor_analysis": {
      "0": "Matrix inversion is far more complex than linear time.",
      "1": "Correct.",
      "2": "It's not exponential, but it is slow for very high dimensions.",
      "3": "Matrix operations are not logarithmic."
    }
  },
  {
    "id": 44,
    "topic": "Regression vs Tree Models",
    "difficulty": "analysis",
    "question": "According to the 'Further considerations' slide, how does impurity reduction in tree models translate to regression?",
    "options": [
      "It is equivalent to increasing the learning rate",
      "It translates to variance reduction",
      "It removes all outliers from the dataset",
      "It forces the model to be a straight line"
    ],
    "correct_index": 1,
    "rationale": "In regression trees, the 'impurity' is measured by the variance of the targets in a leaf; reducing it improves the model.",
    "distractor_analysis": {
      "0": "These are different algorithmic concepts.",
      "1": "Correct. This is a direct quote from the slide.",
      "2": "Trees are sensitive to outliers; they don't remove them automatically.",
      "3": "Trees are non-linear models; they are specifically not straight lines."
    }
  },
  {
    "id": 45,
    "topic": "Residual Behavior",
    "difficulty": "application",
    "question": "If we discover that our residuals (epsilon) are NOT normally distributed (Gaussian), which fundamental assumption of Least-squares regression mentioned in the slides is violated?",
    "options": [
      "The assumption that features are 0-centered",
      "The assumption that error terms are i.i.d. normally distributed errors",
      "The assumption that the intercept w0 must be positive",
      "The assumption that R² must always be greater than 0.5"
    ],
    "correct_index": 1,
    "rationale": "The probabilistic view of least squares relies on the assumption: epsilon ~ N(0, sigma²).",
    "distractor_analysis": {
      "0": "Zero-centering features is a preprocessing step, not a noise assumption.",
      "1": "Correct. This is the core 'Probabilistic view' assumption.",
      "2": "Intercepts can be negative.",
      "3": "R² can be any value between 0 and 1; there is no requirement for it to be > 0.5."
    }
  },
  {
    "id": 46,
    "topic": "Polynomial Curve Fitting",
    "difficulty": "analysis",
    "question": "Why is 'overfitting' in high-degree polynomials (M=9) considered a failure of the model?",
    "options": [
      "Because the model takes too much time to compute the weights",
      "Because the model is fitting the specific noise of the sample rather than the general trend",
      "Because the R² value becomes 0, which is mathematically impossible",
      "Because it makes the target variable 't' become a vector instead of a scalar"
    ],
    "correct_index": 1,
    "rationale": "Overfitting captures random noise as if it were a signal, making the model useless for predicting new data where that specific noise pattern won't exist.",
    "distractor_analysis": {
      "0": "Overfitting is a problem of accuracy/generalization, not just speed.",
      "1": "Correct. Noise is idiosyncratic; trends are universal.",
      "2": "R² for overfitted models on training data is often very high (~1).",
      "3": "The structure of the data doesn't change based on the model degree."
    }
  },
  {
    "id": 47,
    "topic": "SGD Landscape",
    "difficulty": "recall",
    "question": "In the gradient descent rule, what is the purpose of the 'orthogonal unit vectors' (e_i)?",
    "options": [
      "They define the direction of each individual parameter change in the gradient",
      "They are used to multiply the noise to make it Gaussian",
      "They determine the total number of observations in the dataset",
      "They are used to normalize the R² value to a range of 0 to 1"
    ],
    "correct_index": 0,
    "rationale": "The gradient is a vector sum of partial derivatives along the axes defined by these unit vectors.",
    "distractor_analysis": {
      "0": "Correct. They represent the axes of the weight space.",
      "1": "Noise is not multiplied by unit vectors.",
      "2": "Number of observations is 'n'.",
      "3": "R² normalization is a mathematical property, not a unit vector function."
    }
  },
  {
    "id": 48,
    "topic": "Forward Step-wise Regression",
    "difficulty": "application",
    "question": "If a dataset has a very strong linear trend and almost no secondary patterns, how many steps would you expect 'Forward Step-wise Regression' to take before stopping?",
    "options": [
      "One step, as the first simple function will capture the majority of the relationship",
      "Exactly 100 steps, as it must iterate through every possible weight",
      "It will never stop because the residuals will never be exactly zero",
      "It will take more steps than a polynomial model of degree M=9"
    ],
    "correct_index": 0,
    "rationale": "If the trend is purely linear, the first step (fitting a simple linear model) will leave very small residuals, leading to no 'significant improvement' in step 2.",
    "distractor_analysis": {
      "0": "Correct. The stopping criterion will be met early.",
      "1": "It stops based on improvement, not a fixed count.",
      "2": "The stopping criterion is based on 'significant improvement', not zero error.",
      "3": "Step-wise is often more efficient for simple data."
    }
  },
  {
    "id": 49,
    "topic": "Error Functions",
    "difficulty": "analysis",
    "question": "Why does the 'Mean Squared Error' divide the sum of squared errors by 'n'?",
    "options": [
      "To ensure the error value is always less than 1",
      "To make the error measure independent of the dataset size",
      "To convert the target variable from a vector to a scalar",
      "To remove the intercept w0 from the final error calculation"
    ],
    "correct_index": 1,
    "rationale": "Dividing by 'n' calculates the average error. Without it, a larger dataset would always appear to have 'more' error than a smaller one, even if it were more accurate.",
    "distractor_analysis": {
      "0": "MSE can be much larger than 1.",
      "1": "Correct. It allows for comparison between different sized datasets.",
      "2": "Targets are already scalars in these regression contexts.",
      "3": "W0 is part of the model that determines the error; it's not 'removed' by division."
    }
  },
  {
    "id": 50,
    "topic": "General Logic",
    "difficulty": "recall",
    "question": "According to the 'Summary' slide, what does regularization help to mitigate?",
    "options": [
      "Underfitting",
      "Overfitting",
      "The number of observations (n)",
      "The value of the intercept w0"
    ],
    "correct_index": 1,
    "rationale": "The summary slide explicitly states: 'Regularization mitigates overfitting'.",
    "distractor_analysis": {
      "0": "Regularization can actually cause underfitting if too strong.",
      "1": "Correct.",
      "2": "It has no effect on the count of observations.",
      "3": "It penalizes weights but doesn't specifically target only the intercept."
    }
  }
]
  },
  "foundations-06": {
    "title": "Foundations of AI - Linear Classifiers",
    "questions":
    [
{
"id": 1,
"topic": "Linear Classifiers",
"difficulty": "recall",
"question": "According to the lecture, why are linear models often preferred in 'high-stakes' applications like healthcare or finance over complex black-box models?",
"options": [
"They inherently achieve higher accuracy on non-linear datasets.",
"They are computationally more expensive to construct, ensuring robustness.",
"They are easier to audit for regulatory compliance and are theoretically well-understood.",
"They automatically eliminate the need for feature engineering."
],
"correct_index": 2,
"rationale": "Slide 4 explicitly states that linear models are preferred in high-stakes applications because they are easy to audit (regulatory aspects) and theoretically well-understood.",
"distractor_analysis": {
"0": "Linear models usually have lower accuracy on non-linear data compared to complex models.",
"1": "Linear models are 'cheap' to construct, not expensive.",
"2": "Correct as per the 'Why Linear Models' slide.",
"3": "Linear models often require more manual feature engineering than deep learning."
}
},
{
"id": 2,
"topic": "XOR Problem",
"difficulty": "analysis",
"question": "A single-layer Perceptron fails to solve the XOR problem because the classes are not linearly separable. What is the fundamental geometric implication of this failure?",
"options": [
"The decision boundary must be a hyperplane in the input dimension.",
"No single straight line can partition the 2D input space such that (0,1) and (1,0) are on one side while (0,0) and (1,1) are on the other.",
"The Perceptron algorithm only works with continuous values, not Boolean features.",
"The learning rate must be set to zero to handle Exclusive-Or logic."
],
"correct_index": 1,
"rationale": "Slide 8 and 47 show that XOR data points are arranged such that a linear separator (a line in 2D) cannot separate the 'true' and 'false' outputs. The classes are intertwined.",
"distractor_analysis": {
"0": "This is a property of the model, not the reason why it specifically fails XOR.",
"1": "This describes the geometric impossibility of linear separation for XOR.",
"2": "Perceptrons can handle Boolean features (AND/OR).",
"3": "The learning rate does not fix the fundamental lack of linear separability."
}
},
{
"id": 3,
"topic": "Naive Bayes",
"difficulty": "recall",
"question": "What is the primary 'Naive' assumption in the Naive Bayes classifier that allows it to bypass the 'Curse of Dimensionality'?",
"options": [
"Features follow a multivariate Normal distribution.",
"The probability of a document is independent of the class label.",
"Features are conditionally independent given the class topic.",
"The size of the vocabulary is assumed to be infinite."
],
"correct_index": 2,
"rationale": "Slide 30 states the assumption of conditional independence between terms given the topic, which simplifies the joint probability calculation.",
"distractor_analysis": {
"0": "This is an assumption for Gaussian NB, but the 'Naive' part refers to independence.",
"1": "If document and label were independent, the classifier wouldn't work.",
"2": "This is the core definition of the Naive assumption.",
"3": "Vocabulary size is finite (m)."
}
},
{
"id": 4,
"topic": "Support Vector Machines",
"difficulty": "application",
"question": "In a hard-margin SVM, what happens to the decision boundary if we add a new training point that is far away from the existing boundary but correctly classified?",
"options": [
"The boundary shifts significantly to accommodate the new point.",
"The margin width increases to maximize the distance to this new point.",
"The boundary remains unchanged because the point is not a support vector.",
"The SVM converts itself into a soft-margin classifier automatically."
],
"correct_index": 2,
"rationale": "Slides 15-18 show that the boundary is determined solely by 'support vectors' (points on the edge of the margin). Points outside the margin do not affect the optimization.",
"distractor_analysis": {
"0": "Non-support vectors do not affect the hyperplane.",
"1": "The margin is defined by the closest points, not distant ones.",
"2": "Correct: only points on the margin determine the boundary.",
"3": "Hard vs. soft margin is a user-defined optimization choice, not an automatic switch."
}
},
{
"id": 5,
"topic": "Perceptron Training",
"difficulty": "analysis",
"question": "If the training data is NOT linearly separable, what is the guaranteed behavior of the Perceptron learning algorithm?",
"options": [
"It will converge to a solution that minimizes the number of misclassifications.",
"It will oscillate indefinitely and never come to an end.",
"It will automatically reduce the learning rate to zero.",
"It will eventually find a non-linear kernel to separate the data."
],
"correct_index": 1,
"rationale": "Theorem 1 on Slide 12 explicitly states: 'If the data is not linearly separable, the algorithm will not come to an end.'",
"distractor_analysis": {
"0": "The basic Perceptron does not minimize the global error; it only stops if it classifies everything correctly.",
"1": "Correct: this is a known limitation of the convergence theorem.",
"2": "The learning rate is typically fixed or scheduled, not auto-reduced by separability.",
"3": "The basic Perceptron algorithm does not include a kernel trick."
}
},
{
"id": 6,
"topic": "Kernel Trick",
"difficulty": "application",
"question": "How does the 'Kernel Trick' enable an SVM to find a non-linear decision boundary in the original feature space?",
"options": [
"By adding more layers of neurons to the model.",
"By mapping the data into a lower-dimensional space where it becomes linear.",
"By mapping the data into a higher-dimensional space where a linear hyperplane can separate the classes.",
"By using a random number generator to perturb the weights during training."
],
"correct_index": 2,
"rationale": "Slide 24 explains the kernel trick maps input instances to a higher-dimensional classification space (d2 > d1) to find a separating hyperplane.",
"distractor_analysis": {
"0": "This describes Neural Networks, not the Kernel Trick in SVMs.",
"1": "It maps to a higher dimension, not lower.",
"2": "Correct: non-linear in original space becomes linear in the higher-dimensional space.",
"3": "Weight perturbation is a technique for local minima, not the kernel trick."
}
},
{
"id": 7,
"topic": "Activation Functions",
"difficulty": "analysis",
"question": "What is the primary reason why non-linear activation functions (like the Sigmoid) are essential in multi-layer Neural Networks?",
"options": [
"Without them, the composition of multiple layers would still result in a simple linear function.",
"They ensure that the weights of the network never exceed the range [0, 1].",
"They allow the network to train faster by simplifying the gradient calculation.",
"They are required to make the network 'translation invariant' across all layers."
],
"correct_index": 0,
"rationale": "Slide 48 notes that cascaded linear threshold units can only produce (piece-wise) linear functions. Non-linearity is needed to represent non-linear relationships.",
"distractor_analysis": {
"0": "Correct: stacking linear layers is mathematically equivalent to a single linear layer.",
"1": "Weights are not restricted to [0, 1] by the activation; the output of the sigmoid is.",
"2": "Sigmoids can actually slow down training due to the vanishing gradient problem.",
"3": "Translation invariance is a property of CNN architectures, not general sigmoids."
}
},
{
"id": 8,
"topic": "Gradient Descent",
"difficulty": "comparison",
"question": "When comparing Batch Gradient Descent to Stochastic Gradient Descent (SGD), which statement accurately describes a trade-off mentioned in the lecture?",
"options": [
"SGD is guaranteed to reach the global minimum faster than Batch Gradient Descent.",
"Batch Gradient Descent is better for massive datasets because it updates weights more frequently.",
"SGD is an approximation that is more efficient when the training data is too large for the memory.",
"Batch Gradient Descent uses the 'Delta Rule' while SGD uses the 'Chain Rule'."
],
"correct_index": 2,
"rationale": "Slide 46 mentions SGD should be applied when training data is too large and is an approximation of Gradient Descent.",
"distractor_analysis": {
"0": "SGD is not guaranteed to reach the global minimum; it fluctuates around it.",
"1": "Batch is slower on large data because it processes all samples before one update.",
"2": "Correct: SGD handles large-scale data better by updating per-sample.",
"3": "Both use the same fundamental calculus; the difference is in the sample size per update."
}
},
{
"id": 9,
"topic": "Logistic Regression",
"difficulty": "recall",
"question": "What is the relationship between the 'Logit' function and the 'Logistic' (Sigmoid) function?",
"options": [
"They are identical functions used in different fields.",
"The Logit function is the inverse of the Logistic function.",
"The Logit function is the derivative of the Logistic function.",
"The Logit function is the non-linear version of the Logistic function."
],
"correct_index": 1,
"rationale": "Slide 51 and 52 show that the logit of probability p equals the linear combination, while the sigmoid of the linear combination gives p.",
"distractor_analysis": {
"0": "They have different mathematical forms.",
"1": "Correct: Logit maps [0,1] to real numbers; Sigmoid maps real numbers to [0,1].",
"2": "The derivative of the sigmoid is f(x)(1-f(x)).",
"3": "Both are non-linear; this doesn't describe their relationship."
}
},
{
"id": 10,
"topic": "Laplace Smoothing",
"difficulty": "application",
"question": "In the context of Naive Bayes, if a specific word in a test document never appeared in the training set for 'Topic A', what is the consequence without smoothing?",
"options": [
"The classifier will assign 'Topic A' a probability of 0.5 as a default.",
"The classifier will ignore that word and calculate the probability based on others.",
"The joint probability for 'Topic A' will become zero, regardless of other evidence.",
"The classifier will throw a 'Curse of Dimensionality' error."
],
"correct_index": 2,
"rationale": "Slides 30-31 show that if P(word|topic) is zero, the product of probabilities becomes zero. Smoothing (Laplace) is used to prevent this.",
"distractor_analysis": {
"0": "It doesn't default to 0.5; it calculates based on raw counts.",
"1": "The standard product formula means the whole term becomes 0.",
"2": "Correct: zero-frequency words zero out the entire product.",
"3": "This is a conceptual problem with feature counts, not a software error name."
}
},
{
"id": 11,
"topic": "Backpropagation",
"difficulty": "recall",
"question": "Which mathematical principle is primarily used during the 'Backpropagation' algorithm to adjust weights in a multi-layer neural network?",
"options": [
"The Pythagorean Theorem",
"The Chain Rule of calculus",
"The Law of Large Numbers",
"Bayes' Theorem"
],
"correct_index": 1,
"rationale": "Slides 55-57 describe backpropagation as calculating the influence of a node on the output error, which requires propagating derivatives through layers via the chain rule.",
"distractor_analysis": {
"0": "Not relevant to gradient calculation.",
"1": "Correct: essential for finding gradients in composite functions (layers).",
"2": "Relevant to statistics, not weight updates.",
"3": "Relevant to generative models (Naive Bayes), not the backprop weight update rule."
}
},
{
"id": 12,
"topic": "ANN Properties",
"difficulty": "comparison",
"question": "According to the 'Expressiveness of ANNs' theorems, what is the minimum number of layers required to approximate any arbitrary function to any accuracy?",
"options": [
"One layer (input only)",
"Two layers",
"Three layers",
"At least ten layers"
],
"correct_index": 2,
"rationale": "Slide 58 states: 'Any arbitrary function can be approximated to arbitrary accuracy by a network with three layers.'",
"distractor_analysis": {
"0": "An input layer alone does no computation.",
"1": "Two layers can represent any Boolean function or bounded continuous function, but 'arbitrary' often implies more complexity.",
"2": "Correct: as per the specific slide theorem provided.",
"3": "Depth improves efficiency, but the theorem specifies three layers for the property."
}
},
{
"id": 13,
"topic": "Soft-Margin SVM",
"difficulty": "analysis",
"question": "In a Soft-Margin SVM, what is the purpose of the 'slack variables' (ξ)?",
"options": [
"To increase the dimensionality of the input space.",
"To allow the model to ignore outliers by permitting some classification errors.",
"To transform the quadratic problem into a linear programming problem.",
"To ensure that the parameter vector w is always of unit length."
],
"correct_index": 1,
"rationale": "Slide 22 and 23 explain that slack variables represent real-valued errors to handle 'almost' linearly separable data.",
"distractor_analysis": {
"0": "Kernels increase dimensionality, not slack variables.",
"1": "Correct: they provide a 'soft' margin that tolerates some misclassification.",
"2": "SVM remains a quadratic programming problem.",
"3": "This is a normalization constraint, not the role of slack."
}
},
{
"id": 14,
"topic": "Perceptron Training",
"difficulty": "recall",
"question": "In the Perceptron training algorithm, when is a weight update triggered?",
"options": [
"After every single training instance is processed.",
"Only when the model makes a prediction error.",
"Only when the model predicts the correct label with low confidence.",
"At the end of each epoch, after averaging all gradients."
],
"correct_index": 1,
"rationale": "Slide 10 shows the algorithm: 'If l(xi) != sign(w' * xi) then w' <- w' + ...'. It only updates on error.",
"distractor_analysis": {
"0": "It only updates if the condition (error) is met.",
"1": "Correct: the algorithm is error-driven.",
"2": "The perceptron has no notion of 'confidence' in its basic form (hard threshold).",
"3": "This describes Batch Gradient Descent, not the basic Perceptron algorithm."
}
},
{
"id": 15,
"topic": "ANN Issues",
"difficulty": "analysis",
"question": "What is 'grokking' in the context of training artificial neural networks on small algorithmic datasets?",
"options": [
"A state where the network forgets all previously learned information.",
"The process of the network failing to learn even the training data.",
"A phenomenon where generalization beyond overfitting occurs after long training.",
"The conversion of an ANN into a Decision Tree for better interpretability."
],
"correct_index": 2,
"rationale": "Slide 60 mentions 'generalization beyond overfitting has been observed (grokking)' with a supporting link.",
"distractor_analysis": {
"0": "This is catastrophic forgetting.",
"1": "This is underfitting.",
"2": "Correct: it refers to sudden generalization long after the training error has hit zero.",
"3": "Grokking is a training behavior, not a model conversion."
}
},
{
"id": 16,
"topic": "RNNs and Transformers",
"difficulty": "recall",
"question": "What specific type of data problem are Recurrent Artificial Neural Networks (RNNs) primarily designed to solve?",
"options": [
"High-resolution image classification.",
"Sequential data where the output at time 't' depends on previous inputs.",
"Tabular data with millions of independent features.",
"Finding the maximum margin in a binary classification task."
],
"correct_index": 1,
"rationale": "Slide 61 states RNNs are 'For learning from sequences, e.g., from textual or time series data' where output at t is used as input for t+1.",
"distractor_analysis": {
"0": "That is the domain of CNNs.",
"1": "Correct: RNNs handle temporal/sequential dependencies.",
"2": "Tabular data is often handled by standard feed-forward networks or trees.",
"3": "That is the domain of SVMs."
}
},
{
"id": 17,
"topic": "Linear vs. Non-linear",
"difficulty": "comparison",
"question": "Which of the following models is inherently a 'Generative' model rather than a 'Discriminative' one?",
"options": [
"Support Vector Machines (SVM)",
"Logistic Regression",
"Naive Bayes",
"Perceptron"
],
"correct_index": 2,
"rationale": "Slide 37 explicitly labels Naive Bayes as a 'generative model' because it models the joint probability P(f, c).",
"distractor_analysis": {
"0": "SVM is discriminative; it models the boundary directly.",
"1": "Logistic Regression is discriminative (Slide 49).",
"2": "Correct: it models how the data is generated given the class.",
"3": "Perceptron is a discriminative linear classifier."
}
},
{
"id": 18,
"topic": "Log-odds",
"difficulty": "analysis",
"question": "Why is the log-odds representation useful for the Naive Bayes model in real-world text classification?",
"options": [
"It prevents the model from ever making a mistake on unseen words.",
"It turns the product of probabilities into a sum, which is more numerically stable.",
"It automatically balances the number of positive and negative training samples.",
"It allows the model to ignore the class prior P(c)."
],
"correct_index": 1,
"rationale": "Slide 34-36 show the log-odds model. Summing log-probabilities prevents underflow and is conceptually cleaner for 'adding' evidence from tokens.",
"distractor_analysis": {
"0": "No model is perfect on unseen data; smoothing is needed.",
"1": "Correct: log(a*b) = log(a) + log(b), avoiding tiny numbers near zero.",
"2": "Log-odds does not perform data resampling.",
"3": "The prior P(c) is still included in the final term of the log-odds formula (Slide 34)."
}
},
{
"id": 19,
"topic": "SVM Margin",
"difficulty": "recall",
"question": "In SVM theory, what is the mathematical relationship between the margin width and the norm of the weight vector ||w||?",
"options": [
"Margin width is directly proportional to ||w||.",
"Margin width is equal to the square of ||w||.",
"Increasing the margin width is equivalent to minimizing ||w||.",
"The margin width is independent of ||w|| if the offset 't' is zero."
],
"correct_index": 2,
"rationale": "Slide 17 states: 'Margin width: 2/||w||' and 'Increasing width <=> Minimizing ||w||'.",
"distractor_analysis": {
"0": "It is inversely proportional.",
"1": "It is 2 divided by the norm, not the square.",
"2": "Correct: this is why the SVM optimization objective is to minimize ||w||^2.",
"3": "The norm ||w|| determines the 'scale' of the transition and thus the width."
}
},
{
"id": 20,
"topic": "Multinomial Naive Bayes",
"difficulty": "comparison",
"question": "What is the primary difference between standard Naive Bayes (Bernoulli) and Multinomial Naive Bayes?",
"options": [
"Multinomial NB handles multiple class labels, while Bernoulli only handles two.",
"Multinomial NB accounts for the frequency of word occurrences, while Bernoulli only tracks presence/absence.",
"Bernoulli NB requires the Kernel Trick to work on text data.",
"Multinomial NB is a non-linear classifier, whereas Bernoulli is linear."
],
"correct_index": 1,
"rationale": "Slide 38 states: 'In the previous model, we dismissed multiple occurrences of words. Assuming that documents follow a multinomial distribution...'.",
"distractor_analysis": {
"0": "Both can handle multiple classes.",
"1": "Correct: Multinomial uses word counts (frequency).",
"2": "Neither inherently uses the kernel trick in their basic forms.",
"3": "Both are generally considered linear classifiers in the log-space."
}
},
{
"id": 21,
"topic": "Gradient Descent",
"difficulty": "application",
"question": "What happens in Gradient Descent if the learning rate (η) is set to a value that is much too large?",
"options": [
"The algorithm will converge to the global minimum in a single step.",
"The weight updates will be so small that the error never changes.",
"The algorithm may overstep the minimum and cause the error to diverge.",
"The model will automatically switch to using the Logistic function."
],
"correct_index": 2,
"rationale": "General optimization logic (Slide 42-43): The gradient indicates direction. A large step in that direction can jump over the 'valley' and climb the other side, increasing error.",
"distractor_analysis": {
"0": "A large rate usually leads to instability, not perfect convergence.",
"1": "This happens when the learning rate is too small.",
"2": "Correct: 'Overshooting' is a common issue with high learning rates.",
"3": "Activation functions are architectural choices, not triggered by learning rates."
}
},
{
"id": 22,
"topic": "Perceptron",
"difficulty": "recall",
"question": "The Perceptron was invented in 1957 by which researcher?",
"options": [
"T. Mitchell",
"F. Rosenblatt",
"P. Flach",
"T. Joachims"
],
"correct_index": 1,
"rationale": "Slide 9 explicitly credits F. Rosenblatt in 1957.",
"distractor_analysis": {
"0": "Mitchell is cited as a source for the Gradient Descent slides.",
"1": "Correct.",
"2": "Flach is cited as the source for the textbook 'Machine Learning'.",
"3": "Joachims is cited for his work on linear SVM training time."
}
},
{
"id": 23,
"topic": "Kernel Trick",
"difficulty": "analysis",
"question": "Which of the following is a 'Typical kernel function' mentioned in the slides that allows for mapping into infinite-dimensional spaces?",
"options": [
"The Polynomial Kernel",
"The Gaussian (RBF) Kernel",
"The Sigmoid Neural Kernel",
"The Multinomial Bayes Kernel"
],
"correct_index": 1,
"rationale": "Slide 24 lists the Gaussian kernel. In ML theory, the RBF kernel corresponds to an infinite-dimensional feature map.",
"distractor_analysis": {
"0": "Polynomial kernel maps to a finite, higher-dimensional space.",
"1": "Correct.",
"2": "Sigmoid activations are used in ANNs, but the Gaussian is the standard SVM example for this.",
"3": "Not a standard term or mentioned kernel."
}
},
{
"id": 24,
"topic": "Sigmoid Unit",
"difficulty": "recall",
"question": "In the Logistic Regression model, what is the specific mathematical form of the sigmoid function σ(s)?",
"options": [
"σ(s) = s^2 / (1 + s^2)",
"σ(s) = 1 / (1 + e^-s)",
"σ(s) = log(s / (1-s))",
"σ(s) = exp(-s^2)"
],
"correct_index": 1,
"rationale": "Slide 48 and 50 show the sigmoid formula clearly.",
"distractor_analysis": {
"0": "This is not the sigmoid function.",
"1": "Correct.",
"2": "This is the Logit function (inverse of sigmoid).",
"3": "This is a Gaussian function."
}
},
{
"id": 25,
"topic": "Support Vector Machines",
"difficulty": "comparison",
"question": "Comparing SVMs to Perceptrons, what is the 'additional' goal that an SVM aims for that the basic Perceptron does not?",
"options": [
"The Perceptron only tries to classify correctly; SVM also tries to maximize the margin.",
"The Perceptron tries to maximize the margin; SVM only tries to classify correctly.",
"SVM uses the Delta Rule, while the Perceptron uses the Kernel Trick.",
"SVM is designed for Boolean logic, whereas Perceptrons are for continuous data."
],
"correct_index": 0,
"rationale": "Slide 21 explicitly states: 'For the Perceptron the only goal is to classify every instance correctly. In addition, an SVM aims at maximizing the margin...'.",
"distractor_analysis": {
"0": "Correct: SVM is a 'Maximum-margin classifier'.",
"1": "This is reversed.",
"2": "Perceptrons use a simpler update; SVMs use quadratic optimization and kernels.",
"3": "Both can handle both types of data, but Perceptrons were famously used for Boolean logic (AND/OR)."
}
},
{
"id": 26,
"topic": "Naive Bayes",
"difficulty": "analysis",
"question": "How does the Naive Bayes classifier handle the 'Curse of Dimensionality' when dealing with a large vocabulary?",
"options": [
"By using a non-linear separator to reduce the number of features.",
"By assuming each feature is independent, thus reducing the number of parameters to estimate from exponential to linear.",
"By requiring that the number of training documents is greater than the vocabulary size squared.",
"By only considering the top 10 most frequent words in every document."
],
"correct_index": 1,
"rationale": "Slide 29 and 30 discuss the 'Impossible to estimate' joint probability vs. the simplified 'conditional independence' product.",
"distractor_analysis": {
"0": "NB is generally a linear classifier in its decision boundary.",
"1": "Correct: independence allows us to estimate P(word_i | class) individually.",
"2": "NB actually works well with small datasets relative to feature count.",
"3": "While feature selection is possible, it's not the 'Naive Bayes' solution to the curse."
}
},
{
"id": 27,
"topic": "Logistic Regression",
"difficulty": "application",
"question": "If you have 'sparse data' (many features with mostly zero values), what does the lecture suggest you might consider for Logistic Regression?",
"options": [
"Using a Perceptron instead, as it handles zeros better.",
"Discretization of the feature domains.",
"Removing the intercept (w0) from the model.",
"Increasing the number of training iterations to infinity."
],
"correct_index": 1,
"rationale": "Slide 54 states: 'In case of sparse data, discretization of the feature domains can be considered.'",
"distractor_analysis": {
"0": "Not mentioned; Perceptrons also struggle with sparsity without care.",
"1": "Correct.",
"2": "The intercept is usually necessary for the logit to cross zero.",
"3": "This would likely lead to overfitting or numerical instability."
}
},
{
"id": 28,
"topic": "ANN Theorems",
"difficulty": "recall",
"question": "Which type of functions can be approximated with 'arbitrary small error' by a neural network with just two layers?",
"options": [
"Any arbitrary discontinuous function.",
"Every bounded continuous function.",
"Only linear functions.",
"Only the XOR function."
],
"correct_index": 1,
"rationale": "Slide 58 states: 'Every bounded continuous function can be approximated with arbitrary small error by a network with two layers.'",
"distractor_analysis": {
"0": "Arbitrary functions (including discontinuous ones) require three layers according to the theorem listed.",
"1": "Correct.",
"2": "One layer can do linear; two layers go beyond that.",
"3": "XOR can be done with two layers, but the theorem covers a much broader class."
}
},
{
"id": 29,
"topic": "Perceptron training algorithm",
"difficulty": "analysis",
"question": "What is the 'Delta Rule' (also known as the Least-Mean-Square Rule) mentioned in the context of Stochastic Gradient Descent?",
"options": [
"It is the rule used to calculate the kernel matrix for SVMs.",
"It is the weight update rule: wi = wi + η(l(x) - o(x))xi.",
"It is the logic for deciding when to add a hidden layer to an ANN.",
"It is the probability threshold for the Naive Bayes classifier."
],
"correct_index": 1,
"rationale": "Slide 45 identifies the weight update equation as the 'Delta Rule or Least-Mean-Square Rule'.",
"distractor_analysis": {
"0": "Not related to kernels.",
"1": "Correct.",
"2": "Not related to architecture selection.",
"3": "Not related to Bayes."
}
},
{
"id": 30,
"topic": "Activation Functions",
"difficulty": "recall",
"question": "Why is the Sigmoid function considered 'easy to differentiate'?",
"options": [
"Its derivative is a constant value of 1.",
"Its derivative can be expressed in terms of the function itself: σ'(s) = σ(s)(1 - σ(s)).",
"It is a linear function, and all linear functions have simple derivatives.",
"It does not have a derivative, which simplifies the backpropagation code."
],
"correct_index": 1,
"rationale": "Slide 50 explicitly shows: '∂σ(s)/∂s = σ(s)(1 - σ(s))'.",
"distractor_analysis": {
"0": "That is the derivative of f(x) = x.",
"1": "Correct.",
"2": "It is non-linear.",
"3": "It must be differentiable for backpropagation to work."
}
},
{
"id": 31,
"topic": "Linear Classifiers",
"difficulty": "comparison",
"question": "A model is called 'translation invariant' if:",
"options": [
"It produces the same output regardless of the class label.",
"It does not depend on the origin of the coordinate system.",
"It can only be used with data that has been translated into English.",
"It uses the same weight vector for every training instance."
],
"correct_index": 1,
"rationale": "Slide 14 states: 'A geometric model is called translation invariant if it does not depend on the origin of the coordinate system.'",
"distractor_analysis": {
"0": "This would be a useless model.",
"1": "Correct.",
"2": "This is a language processing joke distractor.",
"3": "Models always use one weight vector for the whole set, but that's not the definition of translation invariance."
}
},
{
"id": 32,
"topic": "Naive Bayes",
"difficulty": "analysis",
"question": "In the Naive Bayes parameter estimation example for 'E-mails', how are the probabilities P(d|+) calculated for a document d = (0, 1, 1)?",
"options": [
"By adding the weights of the support vectors.",
"By multiplying the probabilities of word absence or presence: (1 - P(a|+)) * P(b|+) * P(c|+).",
"By finding the Euclidean distance between the document and the positive center.",
"By taking the average of the feature values."
],
"correct_index": 1,
"rationale": "Slide 33 shows the calculation: P(d|+) = (1 - 2/4) * (3/4) * (1/4). The (1 - p) corresponds to the '0' (absence) of term 'a'.",
"distractor_analysis": {
"0": "SVM logic.",
"1": "Correct: product of conditional probabilities based on the feature vector.",
"2": "Geometric classifier logic.",
"3": "This is not how probability is estimated."
}
},
{
"id": 33,
"topic": "SVM Soft-Margin",
"difficulty": "comparison",
"question": "In the SVM optimization problem, what does the parameter 'C' represent?",
"options": [
"The number of classes in the dataset.",
"The speed of light in the feature space.",
"The user-specified trade-off between margin width and classification error.",
"The learning rate for the Perceptron algorithm."
],
"correct_index": 2,
"rationale": "Slide 22 defines C as: 'user-specified trade-off between margin width and error.'",
"distractor_analysis": {
"0": "Usually denoted by 'k' or 'classes'.",
"1": "Physics distractor.",
"2": "Correct.",
"3": "Learning rate is usually η (eta) or 'r'."
}
},
{
"id": 34,
"topic": "ANN Common issues",
"difficulty": "recall",
"question": "What is a commonly cited 'issue' with Artificial Neural Networks regarding their output results?",
"options": [
"They are too simple to understand.",
"Explaining the influence of an input unit on the result is difficult.",
"They cannot handle more than two input features.",
"They always converge to the global minimum on the first try."
],
"correct_index": 1,
"rationale": "Slide 60 states: 'Explaining the input or unit influence on the produced result is difficult'.",
"distractor_analysis": {
"0": "They are complex 'black boxes'.",
"1": "Correct: lack of interpretability is a major issue.",
"2": "They can handle millions of features.",
"3": "They often get stuck in local minima (Slide 46)."
}
},
{
"id": 35,
"topic": "Gradient Descent",
"difficulty": "analysis",
"question": "What does the 'negated gradient' indicate in the context of the Gradient Descent Rule?",
"options": [
"The direction of the steepest ascent.",
"The direction of the steepest descent.",
"The point where the error is exactly zero.",
"The total number of support vectors in the model."
],
"correct_index": 1,
"rationale": "Slide 42 states: 'The negated gradient indicates the direction of the steepest descent.'",
"distractor_analysis": {
"0": "The gradient itself points to steepest ascent.",
"1": "Correct: we move opposite to the gradient to minimize error.",
"2": "It's a direction, not a specific point.",
"3": "Not related to SVM support vectors."
}
},
{
"id": 36,
"topic": "XOR Problem",
"difficulty": "application",
"question": "To resolve the XOR problem in a neural network, what architectural change is sufficient?",
"options": [
"Adding a second layer (a hidden layer).",
"Increasing the learning rate to 1.0.",
"Removing the bias (intercept) term.",
"Using only linear activation functions."
],
"correct_index": 0,
"rationale": "Slide 47 and 58 note that while a single perceptron fails, a two-layer perceptron can represent any Boolean function (including XOR).",
"distractor_analysis": {
"0": "Correct: hidden layers allow for non-linear decision boundaries.",
"1": "Rate doesn't change the linear nature of the model.",
"2": "Bias is helpful for shifting the boundary.",
"3": "Linear functions would still fail XOR."
}
},
{
"id": 37,
"topic": "Logit vs Logistic",
"difficulty": "comparison",
"question": "Which of these functions maps a probability 'P' from the range [0, 1] to the range [-∞, +∞]?",
"options": [
"The Logistic function",
"The Sigmoid function",
"The Logit function",
"The Gaussian function"
],
"correct_index": 2,
"rationale": "Slide 51 and 52: Logit(p) = log(p / (1-p)). As p approaches 0, it goes to -∞; as p approaches 1, it goes to +∞.",
"distractor_analysis": {
"0": "Maps real numbers to [0,1].",
"1": "Same as Logistic.",
"2": "Correct.",
"3": "Maps real numbers to a bell curve."
}
},
{
"id": 38,
"topic": "Probabilistic classifiers",
"difficulty": "recall",
"question": "When defining a probabilistic classifier from a linear one, what is typically used to model the class-conditional densities P(d(xi)|y)?",
"options": [
"A uniform distribution.",
"A Gaussian mixture model.",
"A discrete lookup table.",
"A linear regression line."
],
"correct_index": 1,
"rationale": "Slide 28 states: 'We can introduce a (Gaussian) mixture model: p(d(xi)|y=+) = N(...)'",
"distractor_analysis": {
"0": "Too simple for most real data.",
"1": "Correct.",
"2": "Not mentioned in the linear-to-probabilistic derivation.",
"3": "Regression predicts values, not densities."
}
},
{
"id": 39,
"topic": "ANN Properties",
"difficulty": "analysis",
"question": "In a Feed-forward ANN, why are there 'no cycles' in terms of information processing?",
"options": [
"To prevent the network from getting stuck in an infinite loop during a forward pass.",
"To ensure the weights can be updated using the Perceptron rule.",
"Because the human brain does not have any feedback loops.",
"To ensure that the input layer is always larger than the output layer."
],
"correct_index": 0,
"rationale": "Slide 55 defines Feed-forward: 'output of a node is always forwarded to the nodes in the layer above'. Logic: Cycles would make the forward computation undefined/infinite.",
"distractor_analysis": {
"0": "Correct: information must flow in one direction for standard matrix multiplication layers.",
"1": "The Perceptron rule is for single layers, not necessarily related to acyclic flow.",
"2": "False; the brain has many feedback loops.",
"3": "Architecture size is independent of the existence of cycles."
}
},
{
"id": 40,
"topic": "Perceptron Properties",
"difficulty": "application",
"question": "A Perceptron with a 'hard threshold' activation function is essentially a special case of which other model, if we replaced the threshold with a Sigmoid?",
"options": [
"Support Vector Machine",
"Naive Bayes",
"Logistic Regression",
"Multinomial Naive Bayes"
],
"correct_index": 2,
"rationale": "Slide 48 and 49 show that by using the sigmoid (logistic) function as the threshold unit, we arrive at the Logistic Regression model.",
"distractor_analysis": {
"0": "SVM has a different optimization goal.",
"1": "Generative vs discriminative difference.",
"2": "Correct: Logistic regression is a 'softened' perceptron.",
"3": "Naive Bayes is based on counts/probabilities, not weighted sums of features."
}
},
{
"id": 41,
"topic": "Backpropagation",
"difficulty": "analysis",
"question": "In the Backpropagation algorithm, what is 'forwarded' through the network first before weights are updated?",
"options": [
"The error gradient from the previous epoch.",
"The training pair (xk, l(xk)) to compute the output ok.",
"A set of random weights to reset the neurons.",
"The inverse of the kernel matrix."
],
"correct_index": 1,
"rationale": "Slide 57 algorithm: 'Forward xk through the network and compute output ok' occurs before computing delta and updating weights.",
"distractor_analysis": {
"0": "Gradients are calculated after the forward pass.",
"1": "Correct: you need the prediction to find the error.",
"2": "Weights are only randomized at initialization.",
"3": "Not relevant to backprop."
}
},
{
"id": 42,
"topic": "Laplace smoothing",
"difficulty": "recall",
"question": "What does the parameter 'α' typically represent in Laplace smoothing for Naive Bayes?",
"options": [
"The learning rate.",
"The number of hidden layers.",
"A small constant added to the counts to avoid zero probabilities.",
"The margin width of the separator."
],
"correct_index": 2,
"rationale": "Slide 31 shows the Laplace formula adding 'α' to the numerator (counts).",
"distractor_analysis": {
"0": "Usually η.",
"1": "ANNS term.",
"2": "Correct.",
"3": "SVM term."
}
},
{
"id": 43,
"topic": "SVM Convergence",
"difficulty": "comparison",
"question": "What did T. Joachims show regarding the training time of linear SVMs?",
"options": [
"They always take quadratic training time.",
"They can be trained in linear time.",
"They are impossible to train on datasets larger than 1000 samples.",
"They require an exponential number of support vectors."
],
"correct_index": 1,
"rationale": "Slide 26 states: 'T. Joachims showed linear training time for linear SVMs'.",
"distractor_analysis": {
"0": "SVMs in general are often quadratic, but Joachims showed a linear case.",
"1": "Correct.",
"2": "SVMs are very popular precisely because they scale to large data.",
"3": "The number of support vectors is usually a small fraction of the data."
}
},
{
"id": 44,
"topic": "Naive Bayes Classifier",
"difficulty": "analysis",
"question": "What does it mean for Naive Bayes to be 'robust' to irrelevant features?",
"options": [
"It automatically sets their weights to zero.",
"Because of the independence assumption, irrelevant features just contribute a nearly constant factor to all classes.",
"It uses a non-linear kernel to project them away.",
"It only works if the irrelevant features are perfectly correlated with the relevant ones."
],
"correct_index": 1,
"rationale": "Slide 39 discusses NB performance. Conceptual logic: If a feature is irrelevant, its distribution is similar across all classes, so P(irrelevant|class_A) ≈ P(irrelevant|class_B), thus cancelling out in the odds ratio.",
"distractor_analysis": {
"0": "This is Lasso regression behavior, not NB.",
"1": "Correct: they don't bias the decision towards one class.",
"2": "NB doesn't use kernels.",
"3": "Irrelevant features are by definition not correlated with the label/class."
}
},
{
"id": 45,
"topic": "ANN issues",
"difficulty": "comparison",
"question": "How can 'overfitting' in Artificial Neural Networks be mitigated according to the lecture?",
"options": [
"By using fewer training examples.",
"By adding more variability to the examples in the training data.",
"By increasing the number of parameters until they exceed the number of data points.",
"By setting all initial weights to zero."
],
"correct_index": 1,
"rationale": "Slide 60: 'Overfitting can be mitigated with more variability in the examples of the training data'.",
"distractor_analysis": {
"0": "Fewer examples usually increase overfitting.",
"1": "Correct: more diverse data helps the model generalize.",
"2": "This actually causes overfitting.",
"3": "Zero weights prevent learning in many architectures (symmetry problem)."
}
},
{
"id": 46,
"topic": "Sigmoid unit",
"difficulty": "analysis",
"question": "In the sigmoid function, as the input 's' approaches negative infinity (-∞), what does the output σ(s) approach?",
"options": [
"1.0",
"0.5",
"0.0",
"-1.0"
],
"correct_index": 2,
"rationale": "Slide 50 graph: The sigmoid function is bounded between 0 and 1. For very negative inputs, 1/(1+e^large) approaches zero.",
"distractor_analysis": {
"0": "This happens when s approaches +∞.",
"1": "This happens when s = 0.",
"2": "Correct.",
"3": "Sigmoid never goes below 0; Tanh goes to -1."
}
},
{
"id": 47,
"topic": "ANN Properties",
"difficulty": "recall",
"question": "What is the main purpose of the 'Hidden layers' in a feed-forward neural network?",
"options": [
"To store the class labels during training.",
"To represent hierarchical feature representations.",
"To provide a backup of the input data.",
"To act as a linear separator for the output layer."
],
"correct_index": 1,
"rationale": "Slide 56 shows: 'Deep neural networks learn hierarchical feature representations' across hidden layers.",
"distractor_analysis": {
"0": "Labels are used to calculate error, not stored in neurons.",
"1": "Correct: early layers find edges, later layers find complex shapes.",
"2": "Neurons store weights, not the input data itself.",
"3": "The output layer usually handles the final separation/classification."
}
},
{
"id": 48,
"topic": "SGD vs Gradient Descent",
"difficulty": "analysis",
"question": "Why is Stochastic Gradient Descent (SGD) particularly useful for very large training datasets?",
"options": [
"It is guaranteed to find the global minimum in one epoch.",
"It avoids calculating the gradient for the entire dataset at once.",
"It automatically removes noisy data points.",
"It requires fewer weights to be stored in memory."
],
"correct_index": 1,
"rationale": "Slide 46 and 44-45 comparison: GD sums error over all 'x in TrainSet' before one update. SGD updates per sample 'xi', saving memory and computation per step.",
"distractor_analysis": {
"0": "Not guaranteed; it's an approximation.",
"1": "Correct: 'Batch' calculation is too heavy for millions of samples.",
"2": "It actually incorporates the noise of individual samples into the update.",
"3": "The number of weights (parameters) is the same for both methods."
}
},
{
"id": 49,
"topic": "Linear separator",
"difficulty": "recall",
"question": "A 'separating hyperplane' is the geometric term for:",
"options": [
"A non-linear circle around the data.",
"The decision boundary of a linear classifier.",
"The path taken by the Gradient Descent algorithm.",
"The sum of all word frequencies in a document."
],
"correct_index": 1,
"rationale": "Slide 5 and 6 use 'Decision boundary (separating hyperplane)' to describe the linear division between classes.",
"distractor_analysis": {
"0": "Hyperplanes are linear by definition.",
"1": "Correct.",
"2": "That's a trajectory in weight space.",
"3": "That's a feature value."
}
},
{
"id": 50,
"topic": "Transformers and RNNs",
"difficulty": "analysis",
"question": "While RNNs process sequences step-by-step, what is a key concept mentioned in the slides (Other types of ANNs) that is often grouped with modern Autoencoders and attention-based models?",
"options": [
"Support Vector Regression",
"Perceptron Learning Thresholds",
"Attention Networks",
"Linear Multinomial Smoothing"
],
"correct_index": 2,
"rationale": "Slide 61 lists: 'Self-Organizing Maps, Autoencoders, Attention Networks,...' as modern types.",
"distractor_analysis": {
"0": "Regression variant of SVM.",
"1": "1950s technology.",
"2": "Correct: the foundation of modern transformers.",
"3": "A hybrid distractor of Bayes and Smoothing."
}
}
]
  },
  "foundations-07": {
    "title": "Foundations of AI - Non-Linear Classifiers",
    "questions": [
{
"id": 1,
"topic": "RNN vs Feedforward Models",
"difficulty": "recall",
"question": "What is the primary architectural difference that allows Recurrent Neural Networks (RNNs) to process sequences differently than standard Feedforward Networks?",
"options": [
"RNNs utilize recurrent connections to pass information from one time step to the next.",
"RNNs use multiple layers of neurons to increase the depth of the network.",
"RNNs employ non-linear activation functions like Tanh, which Feedforward networks lack.",
"RNNs can only process fixed-size input vectors, whereas Feedforward networks are flexible."
],
"correct_index": 0,
"rationale": "RNNs are defined by their recurrent connections, allowing the hidden state at time 't' to depend on the hidden state at 't-1', creating a form of memory for sequential data.",
"distractor_analysis": {
"0": "Correct definition based on the slide defining RNNs via recurrent connections.",
"1": "Depth is a feature of most deep learning models, not specific to RNN sequence processing.",
"2": "Both architectures typically use non-linear activations.",
"3": "The opposite is true; RNNs are designed for variable sequence lengths, while standard feedforward networks require fixed input sizes."
}
},
{
"id": 2,
"topic": "Vanishing Gradient Problem",
"difficulty": "application",
"question": "In a deep RNN, if the weights associated with the recurrent connections (Wrec) are all significantly less than 1.0, what is the most likely outcome during backpropagation?",
"options": [
"The gradients will grow exponentially, causing the model weights to fluctuate wildly.",
"The model will converge faster because smaller weights prevent the cost function from oscillating.",
"The hidden state will saturate, causing the Tanh activation to output only constant values.",
"The influence of early inputs in a long sequence will effectively disappear by the time the gradient reaches them."
],
"correct_index": 3,
"rationale": "This describes the vanishing gradient problem conceptually. Multiplying small values repeatedly through the chain rule leads to gradients that approach zero for earlier time steps.",
"distractor_analysis": {
"0": "This describes the exploding gradient problem, not vanishing.",
"1": "Smaller weights often prevent learning entirely rather than speeding up convergence.",
"2": "Saturation is generally an activation function issue, not a direct consequence of small recurrent weights.",
"3": "Correct conceptual description of the vanishing gradient problem."
}
},
{
"id": 3,
"topic": "Long-Term Dependencies",
"difficulty": "analysis",
"question": "Why does the 'context vector' in a standard RNN act as a bottleneck for processing very long paragraphs?",
"options": [
"The context vector is only updated at the very end of the sequence, ignoring the middle tokens.",
"The context vector requires a separate GPU memory allocation for every word in the sequence.",
"The context vector has a fixed dimension and must compress all previous information, leading to loss of detail.",
"The context vector can only store numerical values, but language requires symbolic representations."
],
"correct_index": 2,
"rationale": "Slide 6 notes that the last hidden state is the context vector, a 'compressed encoding of the entire sequence.' Compression of long sequences into a fixed-size vector inevitably loses information.",
"distractor_analysis": {
"0": "The hidden state is updated at every step, but 'forgets' over time.",
"1": "Memory allocation for the hidden state is constant across steps in an unrolled RNN.",
"2": "Correct; fixed capacity leads to information loss as length increases.",
"3": "All neural network components store numerical values; this isn't the specific context vector bottleneck."
}
},
{
"id": 4,
"topic": "LSTM Gating Mechanisms",
"difficulty": "recall",
"question": "Which specific gate in an LSTM unit is primarily responsible for deciding which information from the previous cell state should be discarded?",
"options": [
"Input Gate",
"Forget Gate",
"Output Gate",
"Update Gate"
],
"correct_index": 1,
"rationale": "Slide 9 explicitly defines the Forget Gate as the mechanism that decides 'which info to keep' or discard from the cell state.",
"distractor_analysis": {
"0": "The input gate decides what to add, not what to discard.",
"1": "Correct; its purpose is information pruning.",
"2": "The output gate computes the hidden state, not the cell state retention.",
"3": "The 'Update Gate' belongs to the GRU architecture."
}
},
{
"id": 5,
"topic": "GRU vs LSTM",
"difficulty": "analysis",
"question": "Comparing LSTMs and GRUs, what is the primary conceptual trade-off when choosing a GRU for a sequence modeling task?",
"options": [
"GRUs are computationally more efficient due to a simplified architecture but may have less fine-grained control over memory.",
"GRUs offer higher representational power because they have more gates than LSTMs.",
"GRUs completely eliminate the vanishing gradient problem, whereas LSTMs only mitigate it.",
"GRUs are strictly better for short sequences, while LSTMs are strictly better for long sequences."
],
"correct_index": 0,
"rationale": "Slide 9 states GRUs are a 'simplified variation of LSTMs.' Simplification generally leads to fewer parameters and faster computation, at the cost of the complexity of the internal state management.",
"distractor_analysis": {
"0": "Correct; simplification is the primary trade-off.",
"1": "Incorrect; GRUs have fewer gates (Reset/Update) than LSTMs (Input/Forget/Output).",
"2": "Both mitigate the problem; neither completely eliminates it.",
"3": "Performance is task-dependent; there is no such strict rule."
}
},
{
"id": 6,
"topic": "Transformer: Encoder vs Decoder",
"difficulty": "recall",
"question": "In the context of Transformer models, how does the objective of a BERT-style model (Encoder-only) differ from a GPT-style model (Decoder-only)?",
"options": [
"BERT is trained to predict the next token, while GPT uses masked language modeling.",
"BERT is designed for image processing, while GPT is designed for text generation.",
"BERT uses masked language modeling to understand context from both directions, while GPT uses autoregressive modeling to predict the future.",
"BERT requires more parameters than GPT to achieve the same level of accuracy."
],
"correct_index": 2,
"rationale": "Slide 11 distinguishes Encoder models (BERT/Autoencoder) using 'masked language modeling' from Decoder models (GPT/Autoregressive) using 'next token prediction.'",
"distractor_analysis": {
"0": "The descriptions are swapped.",
"1": "Both are primarily language models based on the Transformer architecture.",
"2": "Correct; BERT is bidirectional (masked) and GPT is unidirectional (autoregressive).",
"3": "Parameter count is an implementation detail, not a conceptual difference in objectives."
}
},
{
"id": 7,
"topic": "Attention Mechanism",
"difficulty": "application",
"question": "If an Attention mechanism assigns a high 'importance weight' to a specific token in a sentence, what does this imply for the model's computation?",
"options": [
"The model will ignore all other tokens in the sentence during that time step.",
"The representation of the current token will be more heavily influenced by that specific token's features.",
"The model will increase the gradient clipping threshold for that specific token.",
"The token will be moved to the front of the sequence to ensure it is processed first."
],
"correct_index": 1,
"rationale": "Slide 12 explains that attention allows the model to 'focus on specific parts of the input.' This focus is achieved by weighting the contributions of input tokens to the final representation.",
"distractor_analysis": {
"0": "Weights are relative; other tokens still contribute unless their weights are exactly zero.",
"1": "Correct; high weight means high influence in the weighted sum.",
"2": "Attention weights and gradient clipping are unrelated concepts.",
"3": "Tokens do not change physical positions in the sequence during attention calculation."
}
},
{
"id": 8,
"topic": "Self-Attention",
"difficulty": "analysis",
"question": "Why is Self-Attention conceptually superior to the recurrent mechanism of RNNs for capturing dependencies in long sentences?",
"options": [
"RNNs process tokens in parallel, which makes it hard to link distant words.",
"Self-Attention uses fewer weights than an RNN cell, reducing the chance of noise.",
"RNNs can only look at the words immediately to the left, missing right-side context.",
"Self-Attention calculates a direct relationship between any two tokens regardless of their distance in the sentence."
],
"correct_index": 3,
"rationale": "Slide 12 and 13 show that attention handles long-term dependencies by 'selectively focusing.' In Self-Attention, the distance between tokens doesn't matter, whereas in RNNs, information must travel through all intermediate hidden states.",
"distractor_analysis": {
"0": "RNNs process tokens sequentially, not in parallel.",
"1": "Transformers typically have significantly more parameters than standard RNNs.",
"2": "Bidirectional RNNs can look right, but they still suffer from signal decay over distance.",
"3": "Correct; attention provides a direct 'shortcut' across the sequence."
}
},
{
"id": 9,
"topic": "ChatGPT: RLHF",
"difficulty": "analysis",
"question": "How does Reinforcement Learning with Human Feedback (RLHF) fundamentally change the model's behavior compared to standard next-token prediction?",
"options": [
"It shifts the model's objective from statistical likelihood to human-judged quality and helpfulness.",
"It allows the model to learn from unlabelled data more efficiently than supervised learning.",
"It eliminates the need for the Transformer backbone, replacing it with a reward-based system.",
"It reduces the number of parameters the model needs to store in its memory."
],
"correct_index": 0,
"rationale": "Slide 16 explains Stage 3 (RLHF) as using a 'scoring mechanism' (human) to grade responses so the model learns to 'achieve good scores' rather than just mimicking text statistics.",
"distractor_analysis": {
"0": "Correct; alignment with human preference vs. statistical probability.",
"1": "RLHF requires expensive, labelled human feedback.",
"2": "RLHF is a training method applied to the Transformer, not a replacement for it.",
"3": "RLHF is usually applied to very large models; it doesn't reduce parameters."
}
},
{
"id": 10,
"topic": "Distance Metrics",
"difficulty": "recall",
"question": "Which property of a distance metric ensures that the distance from point A to point B is always the same as the distance from point B to point A?",
"options": [
"Positivity",
"Symmetry",
"Triangle Inequality",
"Cosine Similarity"
],
"correct_index": 1,
"rationale": "Slide 18 defines symmetry as d(xi, xj) = d(xj, xi).",
"distractor_analysis": {
"0": "Positivity ensures non-negative distances.",
"1": "Correct; this is the definition of symmetry.",
"2": "Triangle Inequality concerns paths through a third point.",
"3": "This is a similarity measure, not a distance property."
}
},
{
"id": 11,
"topic": "Manhattan vs Euclidean Distance",
"difficulty": "application",
"question": "In a scenario where features are arranged in a grid-like structure (e.g., city blocks) and diagonal movement is impossible, which distance metric is most realistic?",
"options": [
"Euclidean Distance",
"Mahalanobis Distance",
"Maximum Distance",
"Manhattan Distance"
],
"correct_index": 3,
"rationale": "Slide 19 shows Manhattan distance as the sum of absolute differences, representing paths that follow axis-aligned grids (like city blocks).",
"distractor_analysis": {
"0": "Euclidean measures 'as the crow flies,' which assumes diagonal movement is possible.",
"1": "Mahalanobis accounts for data distribution, not physical movement constraints.",
"2": "Maximum distance only considers the single largest coordinate gap.",
"3": "Correct; it follows the grid lines."
}
},
{
"id": 12,
"topic": "k-Nearest Neighbors (k-NN)",
"difficulty": "analysis",
"question": "Why is k-NN considered a 'lazy' classifier, and what is its primary drawback for real-time applications with massive datasets?",
"options": [
"It uses very few parameters, making it less accurate than neural networks.",
"It only looks at the 1st neighbor, making it highly sensitive to noise.",
"It does not learn a general model during training; instead, it requires high computation time during prediction.",
"It ignores feature importance, meaning it cannot handle categorical data."
],
"correct_index": 2,
"rationale": "Slide 23 describes k-NN as 'Lazy' and 'computationally intensive' because it defers work until prediction time, comparing new inputs against all stored training instances.",
"distractor_analysis": {
"0": "It has no learned parameters, but 'lazy' refers to induction timing, not accuracy.",
"1": "The 'k' can be any number; 'lazy' refers to when training happens.",
"2": "Correct; storing data instead of learning weights makes inference slow.",
"3": "It treats features equally by default, but this isn't the definition of 'lazy'."
}
},
{
"id": 13,
"topic": "Decision Trees: Impurity",
"difficulty": "recall",
"question": "During the construction of a Decision Tree, what is the 'Best Split' intended to maximize?",
"options": [
"The number of leaves in the final tree.",
"The distance between instances in different classes.",
"The computational speed of the split algorithm.",
"The purity gain (or information gain)."
],
"correct_index": 3,
"rationale": "Slides 26 and 28 define the goal of finding the 'best split' as maximizing purity gain.",
"distractor_analysis": {
"0": "Maximizing leaves usually causes overfitting.",
"1": "This is a clustering objective; trees focus on class homogeneity.",
"2": "Speed is a constraint, but not the objective function of the split.",
"3": "Correct; the goal is to create subsets that are as 'pure' as possible."
}
},
{
"id": 14,
"topic": "Entropy vs Gini Index",
"difficulty": "analysis",
"question": "According to the slide on 'Entropy vs. Gini index,' what is a notable characteristic of the 'Square root of the Gini index'?",
"options": [
"It is insensitive to fluctuations in the class distribution.",
"It is more sensitive to outliers than standard Gini.",
"It always results in deeper trees with fewer instances per leaf.",
"It is the only metric that can handle continuous attribute values."
],
"correct_index": 0,
"rationale": "Slide 27 states that the square root of Gini 'is insensitive to fluctuations in the class distribution.'",
"distractor_analysis": {
"0": "Correct; direct quote from Slide 27 regarding relative impurity.",
"1": "The slide focuses on distribution insensitivity, not outlier sensitivity.",
"2": "Metric choice affects split points but doesn't guarantee specific depth structure.",
"3": "All three metrics (Entropy, Gini, sqrt Gini) handle continuous values via thresholds."
}
},
{
"id": 15,
"topic": "Decision Trees: Pruning",
"difficulty": "application",
"question": "If a Decision Tree perfectly classifies training data but fails on the test set, what 'Pruning' action is most appropriate?",
"options": [
"Add more features to the dataset to provide more context.",
"Replace subtrees that do not significantly improve accuracy with leaf nodes.",
"Increase the depth of the tree to capture more complex rules.",
"Change the impurity measure from Gini to Entropy."
],
"correct_index": 1,
"rationale": "Slide 39 discusses pruning as a way to 'mitigate overfitting' by removing non-productive subtrees.",
"distractor_analysis": {
"0": "Adding features can increase overfitting in an already complex tree.",
"1": "Correct; this is the core of the bottom-up pruning strategy.",
"2": "Increasing depth is the source of the overfitting problem.",
"3": "This is unlikely to solve fundamental structural overfitting."
}
},
{
"id": 16,
"topic": "Random Forests: Mechanism",
"difficulty": "recall",
"question": "What two primary sources of randomness ensure diversity in a 'Random Forest'?",
"options": [
"Random initial weights and random learning rates.",
"Random depth for each tree and random impurity metrics.",
"Bootstrapping (sampling with replacement) and random feature selection for each split.",
"Randomly deleting trees after training and random pruning."
],
"correct_index": 2,
"rationale": "Slide 33 details: 1. Bootstrapping (sampling N times with replacement) and 2. Randomly sampling features.",
"distractor_analysis": {
"0": "These are parameters for neural networks, not Random Forests.",
"1": "The algorithm specifies feature sampling, not random depth or metrics.",
"2": "Correct; these define the 'Random' in Random Forest.",
"3": "Deletion is not part of standard induction."
}
},
{
"id": 17,
"topic": "Random Forests vs Decision Trees",
"difficulty": "analysis",
"question": "Why is a Random Forest generally more robust to noise than a single Decision Tree?",
"options": [
"The averaging/voting of multiple diverse trees cancels out the errors of individual trees that overfit to noise.",
"Each tree in the forest is pruned more aggressively than a single tree.",
"Random Forests use linear decision boundaries, whereas Decision Trees are non-linear.",
"Random Forests only use the most important feature for every tree."
],
"correct_index": 0,
"rationale": "Slide 35 mentions Random Forests are 'More robust to noise.' This results from the ensemble effect where individual variances are averaged out.",
"distractor_analysis": {
"0": "Correct ensemble logic.",
"1": "Slide 35 states 'No pruning is needed' for Random Forests.",
"2": "Both models are non-linear.",
"3": "They purposely use feature subsets to increase diversity."
}
},
{
"id": 18,
"topic": "k-NN: Curse of Dimensionality",
"difficulty": "analysis",
"question": "What is the primary reason k-NN suffers from the 'curse of dimensionality'?",
"options": [
"The number of classes grows exponentially with the number of features.",
"The memory required to store distance metrics exceeds GPU capacity.",
"The labels of neighbors become correlated, leading to high bias.",
"In high-dimensional space, data points become very sparse, making distance-based 'nearest' measures less meaningful."
],
"correct_index": 3,
"rationale": "Slide 23 notes k-NN's susceptibility to the curse, where in high dimensions, distances between all points tend to converge, losing discriminative power.",
"distractor_analysis": {
"0": "Number of classes is independent of feature count.",
"1": "Storage is large (O(N*D)) but not the 'logical' curse.",
"2": "The curse is about geometric distance, not label correlation.",
"3": "Correct conceptual explanation of high-D sparsity."
}
},
{
"id": 19,
"topic": "RNN: Exploding Gradients",
"difficulty": "application",
"question": "If an RNN's loss becomes 'NaN' (Not a Number) during training, which slide-recommended technique should be applied?",
"options": [
"LSTM Gating",
"Gradient Clipping",
"Increasing sequence length",
"Changing Tanh to ReLU"
],
"correct_index": 1,
"rationale": "Slide 8 lists 'gradient clipping' as the mitigation for 'Exploding Gradient,' which causes numerical instability.",
"distractor_analysis": {
"0": "LSTMs help with vanishing gradients, but explosion still requires clipping.",
"1": "Correct; it caps gradient magnitude.",
"2": "Increasing length usually makes gradient explosion worse.",
"3": "ReLU can exacerbate explosion as it doesn't saturate."
}
},
{
"id": 20,
"topic": "Decision Trees: Path Interpretation",
"difficulty": "recall",
"question": "What feature of Decision Trees makes them valuable for regulated industries like finance?",
"options": [
"They are the most accurate models for all small datasets.",
"They are immune to bias in training data.",
"They are easy to interpret and explain, as rules can be read from paths.",
"They can be easily parallelized on cloud infrastructure."
],
"correct_index": 2,
"rationale": "Slide 32 highlights 'Easy to interpret and explain' as a key advantage.",
"distractor_analysis": {
"0": "Accuracy is dataset-dependent; interpretability is their unique strength.",
"1": "No model is immune to data bias.",
"2": "Correct; 'Explainability' is often a regulatory requirement.",
"3": "While true, this isn't the reason they are used for 'regulation'."
}
},
{
"id": 21,
"topic": "Cross-Attention",
"difficulty": "recall",
"question": "In a Transformer machine translation model, what does 'Cross-Attention' enable?",
"options": [
"The decoder focuses on specific parts of the source input sequence while generating the target output.",
"Words within the generated sentence are associated with each other.",
"The model translates two different languages in parallel.",
"The similarity between the training and test sets is calculated."
],
"correct_index": 0,
"rationale": "Slide 13 defines Cross-Attention as attention between different sequences, e.g., output vs input.",
"distractor_analysis": {
"0": "Correct; it links the decoding process to the encoded source.",
"1": "This is Self-Attention.",
"2": "Cross-attention is about inter-sequence relationships, not multi-task translation.",
"3": "Attention is an internal mechanism, not a dataset comparison tool."
}
},
{
"id": 22,
"topic": "Multi-head Attention",
"difficulty": "analysis",
"question": "What is the conceptual benefit of 'Multi-head' attention over a single head?",
"options": [
"It reduces memory usage by a factor equal to the head count.",
"It allows the model to process sequences that are multiple times longer.",
"It ensures importance weights always sum exactly to 1.0.",
"It allows the model to jointly attend to information from different representation subspaces at different positions."
],
"correct_index": 3,
"rationale": "Slide 14 states multi-head attention allows the model to 'jointly attend to information from different representation subspaces.'",
"distractor_analysis": {
"0": "More heads usually increase memory/compute.",
"1": "Sequence length limits are quadratic and independent of head count.",
"2": "Softmax ensures this even for a single head.",
"3": "Correct; like having multiple types of 'filters' for relationships."
}
},
{
"id": 23,
"topic": "LSTM Cell State",
"difficulty": "analysis",
"question": "How does the LSTM 'Cell State' conceptualize 'long-term memory' differently than a standard RNN's hidden state?",
"options": [
"The Cell State is reset to zero every five tokens to avoid confusion.",
"The Cell State acts as a highway allowing information to flow with minor linear interactions, mitigating the vanishing gradient.",
"The Cell State is only for categorical data, while the hidden state is for numerical data.",
"The Cell State stores gradients, while the hidden state stores weights."
],
"correct_index": 1,
"rationale": "Slide 9 shows the cell state as a separate line aggregating outputs, allowing long-term persistence with less interference.",
"distractor_analysis": {
"0": "Resets are not a standard feature; gates decide retention.",
"1": "Correct; it provides a 'shortcut' for long-term signals.",
"2": "Both states are numerical vectors.",
"3": "Gradients and weights are optimization components, not internal architecture states."
}
},
{
"id": 24,
"topic": "LLM Scale",
"difficulty": "recall",
"question": "According to the provided table, which model has the highest number of tunable parameters?",
"options": [
"GPT-3",
"WuDao 2.0",
"MT-NLG",
"Gato"
],
"correct_index": 1,
"rationale": "Slide 15 shows WuDao 2.0 with '1.75 trillion' parameters, the highest in the list.",
"distractor_analysis": {
"0": "GPT-3 has 175 billion.",
"1": "Correct; 1.75 trillion.",
"2": "MT-NLG has 530 billion.",
"3": "Gato has 1.18 billion."
}
},
{
"id": 25,
"topic": "Hierarchical Classifiers",
"difficulty": "application",
"question": "When building a Decision Tree, if one specific feature is a 'perfect discriminator,' where will the algorithm place it?",
"options": [
"At the root of the tree.",
"In the leaf nodes.",
"As a pruning threshold.",
"It will be discarded to prevent overfitting."
],
"correct_index": 0,
"rationale": "Slide 25 states the algorithm chooses the 'most discriminative attribute' first. A perfect discriminator provides maximum information gain and thus becomes the root.",
"distractor_analysis": {
"0": "Correct; top-down induction prioritizes the most useful feature.",
"1": "Leaves represent classes, not split features.",
"2": "Pruning is a post-construction step.",
"3": "Discriminative features are the goal of tree construction."
}
},
{
"id": 26,
"topic": "Continuous Attribute Splitting",
"difficulty": "analysis",
"question": "Why is splitting a continuous attribute (like Temperature) more 'computationally heavy' than a discrete one (like Color)?",
"options": [
"Continuous values require the calculation of Euclidean distance between all points.",
"Continuous values must be converted to text strings before splitting.",
"The model must evaluate numerous possible mid-point thresholds between sorted values.",
"Continuous attributes cannot be used with the Gini index."
],
"correct_index": 2,
"rationale": "Slide 31/32 explains that finding the optimal 'split value x' for continuous data is heavy because of the large search space of potential thresholds.",
"distractor_analysis": {
"0": "Distance is for k-NN; trees use impurity measures.",
"1": "This would destroy the numerical relationship.",
"2": "Correct; the search for the optimal threshold is exhaustive.",
"3": "They can; one simply uses a threshold (e.g., Temp < 20)."
}
},
{
"id": 27,
"topic": "k-NN vs Decision Trees",
"difficulty": "analysis",
"question": "In which scenario would k-NN likely outperform a Decision Tree?",
"options": [
"The dataset contains 10 million instances and requires millisecond response times.",
"The decision boundary is highly complex/diagonal, but local neighborhoods are consistent.",
"The user requires a set of human-readable rules.",
"The dataset has many irrelevant features (noise)."
],
"correct_index": 1,
"rationale": "k-NN captures local patterns well (Slide 23). Trees are constrained to axis-aligned splits which struggle with diagonal boundaries that distance metrics handle naturally.",
"distractor_analysis": {
"0": "k-NN is too slow for this due to its 'lazy' prediction (Slide 23).",
"1": "Correct; k-NN handles complex non-axis-aligned boundaries better locally.",
"2": "This is the strength of Trees, not k-NN.",
"3": "k-NN is highly susceptible to noise (Slide 23)."
}
},
{
"id": 28,
"topic": "RNN: Dynamics of Sequences",
"difficulty": "recall",
"question": "What does it mean for an RNN to 'capture the dynamics of sequences'?",
"options": [
"It uses a physics engine to calculate the motion of tokens.",
"It can dynamically change its layer count during inference.",
"It models temporal relationships where current state is a function of input and history.",
"It speeds up training using dynamic learning rates."
],
"correct_index": 2,
"rationale": "Slide 4 defines RNNs as capturing sequence dynamics via recurrent connections and hidden representations.",
"distractor_analysis": {
"0": "Nonsensical interpretation.",
"1": "RNN architectures are static once trained.",
"2": "Correct; 'Dynamics' refers to time-based dependencies.",
"3": "This refers to optimizers like Adam, not the architecture."
}
},
{
"id": 29,
"topic": "Euclidean Distance Property",
"difficulty": "application",
"question": "If the Euclidean distance d(x, y) = 0, what does the 'positivity' property conclude?",
"options": [
"x and y are perpendicular.",
"x and y are located at the origin.",
"x and y are identical points.",
"x and y are in different metric spaces."
],
"correct_index": 2,
"rationale": "Slide 18: 'd(xi, xj) = 0 iff xi = xj'. This is the identity of indiscernibles.",
"distractor_analysis": {
"0": "Perpendicularity relates to dot product, not zero distance.",
"1": "They can be anywhere; they just must be at the same location.",
"2": "Correct; distance is zero only for identical points.",
"3": "Distance is only defined within the same space."
}
},
{
"id": 30,
"topic": "Random Forests: Feature Subsets",
"difficulty": "analysis",
"question": "Why do Random Forests use only a random subset of 'm' features at each split?",
"options": [
"To decorrelate the trees, ensuring strong predictors don't dominate every single tree.",
"To reduce the memory required to store the resulting tree.",
"Because Information Gain only works with a small number of variables.",
"To ensure that trees grow as deep as possible."
],
"correct_index": 0,
"rationale": "Slide 33/35: By forcing trees to ignore the 'best' feature occasionally, we allow other patterns to emerge, creating a diverse ensemble that reduces bias.",
"distractor_analysis": {
"0": "Correct; this is the conceptual core of diversity in ensembles.",
"1": "Secondary effect; the goal is model quality.",
"2": "IG works with any feature count.",
"3": "Deep trees usually signify overfitting; diversity is the goal."
}
},
{
"id": 31,
"topic": "Triangle Inequality",
"difficulty": "recall",
"question": "What is the formal definition of the 'Triangle Inequality'?",
"options": [
"Any three points in a space must form a perfect triangle.",
"Distance between two points is always less than or equal to the sum of distances to a third point.",
"The square of the hypotenuse equals the sum of squares of the other sides.",
"Distance to the origin is the average of distances between all points."
],
"correct_index": 1,
"rationale": "Slide 18 defines it as d(xi, xk) <= d(xi, xj) + d(xj, xk).",
"distractor_analysis": {
"0": "Nonsensical definition.",
"1": "Correct definition.",
"2": "This is the Pythagorean theorem, a specific case of Euclidean geometry.",
"3": "Nonsensical definition."
}
},
{
"id": 32,
"topic": "BERT Training",
"difficulty": "recall",
"question": "What is the objective of BERT's 'Masked Language Modeling'?",
"options": [
"To generate new sentences by filling in blanks randomly.",
"To reduce the number of tokens processed by 15%.",
"To force the model to predict a missing word using context from both directions.",
"To hide sensitive information during translation."
],
"correct_index": 2,
"rationale": "Slide 11: BERT masks random words to make the model predict them, enabling bidirectional understanding.",
"distractor_analysis": {
"0": "BERT is an encoder (understanding), not a generator.",
"1": "Masking doesn't reduce total tokens; it changes their values.",
"2": "Correct; this is the 'Bidirectional' part of BERT.",
"3": "Masking is a training objective, not a privacy tool."
}
},
{
"id": 33,
"topic": "GPT vs BERT",
"difficulty": "analysis",
"question": "Why choose a GPT-style model over BERT for a creative writing application?",
"options": [
"GPT is better at understanding nuace.",
"BERT only works with English text.",
"BERT cannot process sentences longer than 512 tokens.",
"GPT is an autoregressive Decoder model optimized for generating the next word."
],
"correct_index": 3,
"rationale": "Slide 11 identifies GPT as 'Generative' and based on a 'Decoder' with a 'next token prediction' objective.",
"distractor_analysis": {
"0": "BERT is often better at 'understanding'; GPT is better at 'generating'.",
"1": "Both are language-agnostic architectures.",
"2": "This is an implementation limit, not the conceptual reason for the task choice.",
"3": "Correct; its objective is specifically designed for generation."
}
},
{
"id": 34,
"topic": "Distance vs Similarity",
"difficulty": "application",
"question": "If the distance d(x, y) approaches infinity, what happens to similarity in the formula 'sim(x, y) := 1 / exp(d(x,y))'?",
"options": [
"Similarity approaches zero.",
"Similarity also approaches infinity.",
"Similarity remains constant at 1.0.",
"Similarity becomes negative."
],
"correct_index": 0,
"rationale": "Slide 19: As distance grows, the denominator exp(d(x,y)) grows exponentially, causing similarity to approach zero.",
"distractor_analysis": {
"0": "Correct; infinite distance means no similarity.",
"1": "Distance and similarity are inversely related.",
"2": "Constant similarity would mean distance has no effect.",
"3": "Similarity is bounded [0, 1]."
}
},
{
"id": 35,
"topic": "Information Gain Calculation",
"difficulty": "analysis",
"question": "A dataset D is split into Partition A (all 'Yes') and Partition B (all 'No'). What is the Information Gain?",
"options": [
"Zero, because both classes exist in the end.",
"Negative, because the split was too simple.",
"It is maximized (equal to Entropy(D)), as resulting partitions have zero entropy.",
"0.5, because the data was split in half."
],
"correct_index": 2,
"rationale": "Slide 29: Gain = Entropy(D) - Weighted Partition Entropy. If partitions are pure, entropy is 0, maximizing gain.",
"distractor_analysis": {
"0": "Purity is the goal; this split achieved total purity.",
"1": "IG is always non-negative.",
"2": "Correct; gain is maximized when remaining impurity is zero.",
"3": "Gain is measured in bits of information, not data volume ratio."
}
},
{
"id": 36,
"topic": "k-NN Decision Regions",
"difficulty": "recall",
"question": "In the provided example, which distance metric results in 'non-convex' decision regions?",
"options": [
"Euclidean Distance",
"Both Euclidean and Manhattan",
"Manhattan Distance",
"Neither; k-NN is always linear"
],
"correct_index": 2,
"rationale": "Slide 21 labels Manhattan distance visualization as 'Non-convex decision regions'.",
"distractor_analysis": {
"0": "Labeled as 'Linear' in the specific slide example.",
"1": "The slide distinguishes them by this property.",
"2": "Correct; direct label from Slide 21.",
"3": "k-NN is non-linear; it can generate complex regions."
}
},
{
"id": 37,
"topic": "Transformer backbone",
"difficulty": "recall",
"question": "Is the Transformer architecture still the backbone of modern AI models?",
"options": [
"No; replaced by more efficient RNNs in 2023.",
"Yes; it remains the backbone of models like GPT-4.",
"No; modern models use completely different Hyper-Networks.",
"Yes; but only for image processing, not text."
],
"correct_index": 1,
"rationale": "Slide 10: 'Still the backbone of modern models!'",
"distractor_analysis": {
"0": "Transformers largely replaced RNNs.",
"1": "Correct; as per the slide.",
"2": "Transformers remain the dominant architecture.",
"3": "It is dominant in both NLP and vision."
}
},
{
"id": 38,
"topic": "RLHF: Scoring Mechanism",
"difficulty": "application",
"question": "If RLHF human graders favor 'politeness' over 'accuracy,' what is the likely model failure mode?",
"options": [
"The model will refuse to answer any questions.",
"The model will be highly polite but may confidently provide incorrect information.",
"The model will become too slow to answer.",
"The model will use jargon to confuse graders."
],
"correct_index": 1,
"rationale": "Slide 16: Models optimize for what is rewarded. If politeness is rewarded over truth, it will prioritize politeness.",
"distractor_analysis": {
"0": "It only refuses if that leads to higher rewards.",
"1": "Correct; it optimizes for the provided scoring signal.",
"2": "Inference speed is not a scoring objective.",
"3": "It won't use jargon unless rewarded for it."
}
},
{
"id": 39,
"topic": "Random Forest: Inference",
"difficulty": "recall",
"question": "How are results of individual trees combined in a Random Forest prediction?",
"options": [
"The model computes a weighted average (or majority vote) from all trees.",
"Only the prediction from the most accurate tree is used.",
"The results are summed and multiplied by feature count.",
"Trees are processed sequentially, each correcting the previous one's error."
],
"correct_index": 0,
"rationale": "Slide 35: 'compute weighted average from all predictions.'",
"distractor_analysis": {
"0": "Correct; the 'Forest' uses a collective vote.",
"1": "Ensembles use the group, not just one expert.",
"2": "Mathematical nonsense.",
"3": "This describes 'Boosting,' not Random Forest (Bagging)."
}
},
{
"id": 40,
"topic": "Transformer: Original Task",
"difficulty": "recall",
"question": "For what original task was the Transformer developed?",
"options": [
"Image Classification",
"Playing Chess",
"Stock Market Prediction",
"Translation Tasks"
],
"correct_index": 3,
"rationale": "Slide 10: 'Originally developed for translation tasks...'",
"distractor_analysis": {
"0": "Adapted for images later.",
"1": "Not the original purpose.",
"2": "Not the original purpose.",
"3": "Correct; referenced in the Vaswani 2017 paper."
}
},
{
"id": 41,
"topic": "Decision Trees: Depth vs Purity",
"difficulty": "analysis",
"question": "If a tree splits until 100% purity, but leaves contain only single instances, what is the problem?",
"options": [
"High Bias",
"Overfitting",
"Vanishing Gradients",
"Underfitting"
],
"correct_index": 1,
"rationale": "Slide 32: 'Danger of overfitting (e.g. high purity from few training samples).'",
"distractor_analysis": {
"0": "High bias means the model is too simple.",
"1": "Correct; the tree is memorizing noise.",
"2": "This is a neural net training issue.",
"3": "Underfitting is when the model is not complex enough."
}
},
{
"id": 42,
"topic": "Mahalanobis Distance",
"difficulty": "recall",
"question": "Which metric accounts for data correlation via a covariance matrix (Σ)?",
"options": [
"Mahalanobis Distance",
"Euclidean Distance",
"Manhattan Distance",
"Maximum Distance"
],
"correct_index": 0,
"rationale": "Slide 19 explicitly includes the covariance matrix in the Mahalanobis definition.",
"distractor_analysis": {
"0": "Correct; it is normalized by data distribution.",
"1": "Assumes spherical distribution.",
"2": "Based on absolute axis differences.",
"3": "This is the L-infinity norm."
}
},
{
"id": 43,
"topic": "RNN: Context Accumulation",
"difficulty": "application",
"question": "In an RNN processing 'A-B-C-D', which token is typically most dominant in the final state?",
"options": [
"Token A",
"Token B",
"Token C",
"Token D"
],
"correct_index": 3,
"rationale": "Due to sequential accumulation and vanishing gradients, the most recent token (D) is usually most influential in the final hidden state.",
"distractor_analysis": {
"0": "Information from A has to survive the most transformations.",
"1": "Intermediate token.",
"2": "Second most recent.",
"3": "Correct; the most recent input before inference."
}
},
{
"id": 44,
"topic": "Attention Mechanism: Parallelism",
"difficulty": "analysis",
"question": "Why are Transformers faster to train than RNNs for long sequences?",
"options": [
"Transformers have fewer parameters.",
"Transformers process sequences in parallel via attention; RNNs are sequential.",
"Transformers use less memory per token.",
"Transformers don't use matrix multiplication."
],
"correct_index": 1,
"rationale": "RNNs must process tokens one-by-one (Slide 4). Attention (Slide 12-14) allows parallel calculation of all token relationships.",
"distractor_analysis": {
"0": "Transformers are generally much larger.",
"1": "Correct; parallelization is the key efficiency gain.",
"2": "Attention has quadratic memory complexity, often using more memory.",
"3": "Matrix multiplication is core to Transformers."
}
},
{
"id": 45,
"topic": "Decision Trees: Categorical Performance",
"difficulty": "recall",
"question": "On which type of domain do Decision Trees typically show 'very good performance'?",
"options": [
"Continuous attributes",
"Discrete attribute domains",
"High-resolution image pixels",
"Audio waveforms"
],
"correct_index": 1,
"rationale": "Slide 32 lists 'Very good performance on discrete attribute domains' as a key remark.",
"distractor_analysis": {
"0": "Performance degrades on continuous domains.",
"1": "Correct.",
"2": "Trees struggle with raw high-D spatial data like pixels.",
"3": "Raw waveforms are too complex for standard trees."
}
},
{
"id": 46,
"topic": "k-NN: Optimal Bayes Rate",
"difficulty": "recall",
"question": "What is the theoretical error upper bound for k-NN relative to the Bayes error rate?",
"options": [
"At most twice the Bayes error rate.",
"Always equal to the Bayes error rate.",
"Exactly 10% higher.",
"Unrelated to the Bayes error rate."
],
"correct_index": 0,
"rationale": "Slide 23 states k-NN is almost Bayes optimal with 'twice Bayes error rate as error upper bound.'",
"distractor_analysis": {
"0": "Correct theoretical result cited in the slides.",
"1": "Only true in the limit of infinite data.",
"2": "Nonsensical.",
"3": "The slide explicitly links them."
}
},
{
"id": 47,
"topic": "Transformer: Attention weights",
"difficulty": "recall",
"question": "Which is NOT one of the three core learned weight matrices in Attention?",
"options": [
"Query weights (WQ)",
"Key weights (WK)",
"Bias weights (WB)",
"Value weights (WV)"
],
"correct_index": 2,
"rationale": "Slide 14 specifically highlights the Query, Key, and Value weights trio.",
"distractor_analysis": {
"0": "Part of the core trio.",
"1": "Part of the core trio.",
"2": "Correct; while biases exist in layers, they aren't part of the core Q/K/V conceptual definition.",
"3": "Part of the core trio."
}
},
{
"id": 48,
"topic": "Decision Tree: Stopping Criteria",
"difficulty": "application",
"question": "What is the primary stopping condition for creating a leaf in the growTree algorithm?",
"options": [
"The tree reaches depth 100.",
"The data subset D is 'homogeneous'.",
"The computer runs out of RAM.",
"The entropy is greater than 1.0."
],
"correct_index": 1,
"rationale": "Slide 25: 'If homogeneous(D)... return class of D as labeled leaf.'",
"distractor_analysis": {
"0": "This is an optional depth limit, not the fundamental logical condition.",
"1": "Correct; it means no further splitting can improve purity.",
"2": "Hardware failure.",
"3": "High entropy requires more splitting, not less."
}
},
{
"id": 49,
"topic": "Random Forest: Interpretability",
"difficulty": "analysis",
"question": "Why is a Random Forest considered a 'Black Box'?",
"options": [
"Weights are hidden in neural layers.",
"It only works on dark-mode datasets.",
"It is hard to interpret the collective logic of hundreds of voting trees.",
"It doesn't output class probabilities."
],
"correct_index": 2,
"rationale": "Slide 35 notes Random Forests are 'Difficult to interpret' because the ensemble complexity hides the simple rule-paths found in single trees.",
"distractor_analysis": {
"0": "Forests are tree-based, not neural.",
"1": "Nonsensical.",
"2": "Correct; ensemble complexity obscures simple explainability.",
"3": "They can output probabilities based on tree vote percentages."
}
},
{
"id": 50,
"topic": "RNN: Many-to-One",
"difficulty": "application",
"question": "Which task is best modeled as 'Many-to-one'?",
"options": [
"Machine Translation",
"Sentiment Classification",
"Named Entity Recognition",
"Image Captioning"
],
"correct_index": 1,
"rationale": "Slide 5 shows 'Sentiment Classification' as Many-to-one (sequence input -> single label output).",
"distractor_analysis": {
"0": "Many-to-many (unequal).",
"1": "Correct; sequence of text -> sentiment score.",
"2": "Many-to-many (equal).",
"3": "One-to-many (image -> sequence)."
}
}
]
  }
}
